{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING NECESSARY LIBRARIES\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "# using tokentextsplitter to better maintain the meaning and context of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "langchain, tiktoken, pandas, chromadb\n",
    "\n",
    "tiktoken - for running some of langchain.text_splitters\n",
    "\n",
    "langchain - library useful for creating embeddings, tokenization, indexing, connecting to vectordbs and other nlp stuff\n",
    "\n",
    "chromadb - vector database for embeddings\n",
    "\n",
    "pandas - useful data scientific library for manipulating tabular data (using dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS COULD BE CHANGED TO IMPORTING VIA LINK\n",
    "\n",
    "df = pd.read_csv(\"./medium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1. Introduction of Word2vec\\n\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\n\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\n\\n2. Gensim Python Library Introduction\\n\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\n\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\n\\nPython >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\n\\n>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\\n\\n>= 1.11.3 SciPy >= 0.18.1\\n\\n>= 0.18.1 Six >= 1.5.0\\n\\n>= 1.5.0 smart_open >= 1.2.1\\n\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\n\\npip install --upgrade gensim\\n\\nOr, alternatively for Conda environments:\\n\\nconda install -c conda-forge gensim\\n\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\n\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\n\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\n\\n>>> df = pd.read_csv(\\'data.csv\\')\\n\\n>>> df.head()\\n\\n3.1 Data Preprocessing:\\n\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\n\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\n\\nTo achieve this, we need to do the following things :\\n\\na. Create a new column for Make Model\\n\\n>>> df[\\'Maker_Model\\']= df[\\'Make\\']+ \" \" + df[\\'Model\\']\\n\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\n\\n# Select features from original dataset to form a new dataframe\\n\\n>>> df1 = df[[\\'Engine Fuel Type\\',\\'Transmission Type\\',\\'Driven_Wheels\\',\\'Market Category\\',\\'Vehicle Size\\', \\'Vehicle Style\\', \\'Maker_Model\\']] # For each row, combine all the columns into one column\\n\\n>>> df2 = df1.apply(lambda x: \\',\\'.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\n\\n>>> df_clean = pd.DataFrame({\\'clean\\': df2}) # Create the list of list format of the custom corpus for gensim modeling\\n\\n>>> sent = [row.split(\\',\\') for row in df_clean[\\'clean\\']] # show the example of list of list format of the custom corpus for gensim modeling\\n\\n>>> sent[:2]\\n\\n[[\\'premium unleaded (required)\\',\\n\\n\\'MANUAL\\',\\n\\n\\'rear wheel drive\\',\\n\\n\\'Factory Tuner\\',\\n\\n\\'Luxury\\',\\n\\n\\'High-Performance\\',\\n\\n\\'Compact\\',\\n\\n\\'Coupe\\',\\n\\n\\'BMW 1 Series M\\'],\\n\\n[\\'premium unleaded (required)\\',\\n\\n\\'MANUAL\\',\\n\\n\\'rear wheel drive\\',\\n\\n\\'Luxury\\',\\n\\n\\'Performance\\',\\n\\n\\'Compact\\',\\n\\n\\'Convertible\\',\\n\\n\\'BMW 1 Series\\']]\\n\\n3.2. Genism word2vec Model Training\\n\\nWe can train the genism word2vec model with our own custom corpus as following:\\n\\n>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\n\\nLet’s try to understand the hyperparameters of this model.\\n\\nsize: The number of dimensions of the embeddings and the default is 100.\\n\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\n\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\\n\\nworkers: The number of partitions during training and the default workers is 3.\\n\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\n\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\n\\n>>> model[\\'Toyota Camry\\'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\n\\n-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\n\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\n\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\n\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\n\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\n\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\n\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\n\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\\n\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\n\\ndtype=float32)\\n\\n4. Compute Similarities\\n\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\n\\n>>> model.similarity(\\'Porsche 718 Cayman\\', \\'Nissan Van\\')\\n\\n0.822824584626184 >>> model.similarity(\\'Porsche 718 Cayman\\', \\'Mercedes-Benz SLK-Class\\')\\n\\n0.961089779453727\\n\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\n\\n>>> model1.most_similar(\\'Mercedes-Benz SLK-Class\\')[:5] [(\\'BMW M4\\', 0.9959905743598938),\\n\\n(\\'Maserati Coupe\\', 0.9949707984924316),\\n\\n(\\'Porsche Cayman\\', 0.9945154190063477),\\n\\n(\\'Mercedes-Benz SLS AMG GT\\', 0.9944609999656677),\\n\\n(\\'Maserati Spyder\\', 0.9942780137062073)]\\n\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\n\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\n\\ndef cosine_distance (model, word,target_list , num) :\\n\\ncosine_dict ={}\\n\\nword_list = []\\n\\na = model[word]\\n\\nfor item in target_list :\\n\\nif item != word :\\n\\nb = model [item]\\n\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\n\\ncosine_dict[item] = cos_sim\\n\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\n\\nfor item in dist_sort:\\n\\nword_list.append((item[0], item[1]))\\n\\nreturn word_list[0:num] # only get the unique Maker_Model\\n\\n>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\n\\n>>> cosine_distance (model,\\'Mercedes-Benz SLK-Class\\',Maker_Model,5) [(\\'Mercedes-Benz CLK-Class\\', 0.99737006),\\n\\n(\\'Aston Martin DB9\\', 0.99593246),\\n\\n(\\'Maserati Spyder\\', 0.99571854),\\n\\n(\\'Ferrari 458 Italia\\', 0.9952333),\\n\\n(\\'Maserati GranTurismo Convertible\\', 0.994994)]\\n\\n5. T-SNE Visualizations\\n\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\n\\ndef display_closestwords_tsnescatterplot(model, word, size):\\n\\n\\n\\narr = np.empty((0,size), dtype=\\'f\\')\\n\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\n\\nfor wrd_score in close_words:\\n\\nwrd_vector = model[wrd_score[0]]\\n\\nword_labels.append(wrd_score[0])\\n\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\n\\n\\n\\ntsne = TSNE(n_components=2, random_state=0)\\n\\nnp.set_printoptions(suppress=True)\\n\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\n\\ny_coords = Y[:, 1]\\n\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\n\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\\'offset points\\')\\n\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\n\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\n\\nplt.show() >>> display_closestwords_tsnescatterplot(model, \\'Porsche 718 Cayman\\', 50)\\n\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\n\\nAbout Me\\n\\nI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.' metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}\n"
     ]
    }
   ],
   "source": [
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "docs = loader.load()\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5566.378864126527"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average article text length\n",
    "article_characters_count = 0\n",
    "for doc in docs:\n",
    "\tarticle_characters_count += len(doc.page_content)\n",
    "\n",
    "average_article_character_quantity = article_characters_count / len(docs)\n",
    "\n",
    "average_article_character_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean word count 901.5384615384615\n",
      "median word count 516.0\n"
     ]
    }
   ],
   "source": [
    "df['word_count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "mean_word_count = df['word_count'].mean()\n",
    "median_word_count = df['word_count'].median()\n",
    "print('mean word count', mean_word_count)\n",
    "print('median word count', median_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing for better intuition for proper chunk_size\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df.index, df['word_count'], alpha=0.3, color='#28a67c')\n",
    "plt.title('Scatter Plot of Article Word Counts')\n",
    "plt.xlabel('Article Index')\n",
    "plt.ylabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk size 225 \n",
      "chunk overlap 22\n",
      "docs length 1391 \n",
      "splitted docs length 9490\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "chunk_size = int(mean_word_count / 4)\n",
    "overlap_percent = 0.1\n",
    "chunk_overlap = int(chunk_size * overlap_percent)\n",
    "text_splitter = TokenTextSplitter(\n",
    "\tchunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "print('chunk size', chunk_size, '\\nchunk overlap', chunk_overlap)\n",
    "\n",
    "docs_splitted = text_splitter.split_documents(docs)\n",
    "print('docs length', len(docs),'\\nsplitted docs length', len(docs_splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1. Introduction of Word2vec\\n\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\n\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\n\\n2. Gensim Python Library Introduction\\n\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\n\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\n\\nPython >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\n\\n>= 2.7 (tested with versions', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='\\n>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\\n\\n>= 1.11.3 SciPy >= 0.18.1\\n\\n>= 0.18.1 Six >= 1.5.0\\n\\n>= 1.5.0 smart_open >= 1.2.1\\n\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\n\\npip install --upgrade gensim\\n\\nOr, alternatively for Conda environments:\\n\\nconda install -c conda-forge gensim\\n\\n3. Implementation of word Embedding', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"\\n\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\n\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\n\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\n\\n>>> df = pd.read_csv('data.csv')\\n\\n>>> df.head()\\n\\n3.1 Data Preprocessing:\\n\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='\\n\\n3.1 Data Preprocessing:\\n\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\n\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\n\\nTo achieve this', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' of that make model.\\n\\nTo achieve this, we need to do the following things :\\n\\na. Create a new column for Make Model\\n\\n>>> df[\\'Maker_Model\\']= df[\\'Make\\']+ \" \" + df[\\'Model\\']\\n\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\n\\n# Select features from original dataset to form a new dataframe\\n\\n>>> df1 = df[[\\'Engine Fuel Type\\',\\'Transmission Type\\',\\'Driven_Wheels\\',\\'Market Category\\',\\'Vehicle Size\\', \\'Vehicle Style\\', \\'Maker', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"icle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column\\n\\n>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\n\\n>>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling\\n\\n>>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling\\n\\n>>> sent[:2]\\n\\n\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"\\n\\n>>> sent[:2]\\n\\n[['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Factory Tuner',\\n\\n'Luxury',\\n\\n'High-Performance',\\n\\n'Compact',\\n\\n'Coupe',\\n\\n'BMW 1 Series M'],\\n\\n['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Luxury',\\n\\n'Performance',\\n\\n'Compact',\\n\\n'Convertible',\\n\\n'BMW 1 Series']]\\n\\n3.2. Genism word2vec Model Training\\n\\n\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='. Genism word2vec Model Training\\n\\nWe can train the genism word2vec model with our own custom corpus as following:\\n\\n>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\n\\nLet’s try to understand the hyperparameters of this model.\\n\\nsize: The number of dimensions of the embeddings and the default is 100.\\n\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\n\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"; words with occurrence less than this count will be ignored. The default for min_count is 5.\\n\\nworkers: The number of partitions during training and the default workers is 3.\\n\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\n\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\n\\n>>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\n\\n-0.03832747, -0\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\n\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\n\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\n\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\n', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='15, -0.08467747,\\n\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\n\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\n\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\n\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='.05617865, 0.00297452,\\n\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\n\\ndtype=float32)\\n\\n4. Compute Similarities\\n\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\n\\n>>> model.similar', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\" and Nissan Van.\\n\\n>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')\\n\\n0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')\\n\\n0.961089779453727\\n\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\n\\n>>> model1.most_similar('Mercedes\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\">>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),\\n\\n('Maserati Coupe', 0.9949707984924316),\\n\\n('Porsche Cayman', 0.9945154190063477),\\n\\n('Mercedes-Benz SLS AMG GT', 0.9944609999656677),\\n\\n('Maserati Spyder', 0.9942780137062073)]\\n\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\n\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\n\\ndef cosine_distance (model, word,target_list , num) :\\n\\ncosine_dict =', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' num) :\\n\\ncosine_dict ={}\\n\\nword_list = []\\n\\na = model[word]\\n\\nfor item in target_list :\\n\\nif item != word :\\n\\nb = model [item]\\n\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\n\\ncosine_dict[item] = cos_sim\\n\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\n\\nfor item in dist_sort:\\n\\nword_list.append((item[0], item[1]))\\n\\nreturn word', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"], item[1]))\\n\\nreturn word_list[0:num] # only get the unique Maker_Model\\n\\n>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\n\\n>>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),\\n\\n('Aston Martin DB9', 0.99593246),\\n\\n('Maserati Spyder', 0.99571854),\\n\\n('Ferrari 458 Italia', 0.9952333),\\n\\n('\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"', 0.9952333),\\n\\n('Maserati GranTurismo Convertible', 0.994994)]\\n\\n5. T-SNE Visualizations\\n\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\n\\ndef display_closestwords_tsnescatterplot(model, word, size):\\n\\n\\n\\n\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"(model, word, size):\\n\\n\\n\\narr = np.empty((0,size), dtype='f')\\n\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\n\\nfor wrd_score in close_words:\\n\\nwrd_vector = model[wrd_score[0]]\\n\\nword_labels.append(wrd_score[0])\\n\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\n\\n\\n\\ntsne = TSNE(n_components\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\"tsne = TSNE(n_components=2, random_state=0)\\n\\nnp.set_printoptions(suppress=True)\\n\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\n\\ny_coords = Y[:, 1]\\n\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\n\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\\n\\nplt.xlim(\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=\" points')\\n\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\n\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\n\\nplt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)\\n\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\n\\nAbout Me\\n\\nI am a master student in Data Science at the University of San Francisco\", metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content=' student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.', metadata={'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model'}),\n",
       " Document(page_content='In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).\\n\\nIn this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.\\n', metadata={'Title': 'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'}),\n",
       " Document(page_content='-known GNN framework, DGL.\\n\\nAside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.', metadata={'Title': 'Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric'}),\n",
       " Document(page_content='Introduction\\n\\nThanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.\\n\\nHowever, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.\\n\\nThe Grammar of Graphics\\n\\nIn case you should', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' Grammar of Graphics\\n\\nIn case you should be unfamiliar with the grammar of graphics, here is a quick overview:\\n\\nMain Components of the Grammar of Graphics\\n\\nAs you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types.\\n\\nThese first', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' the other existing plot types.\\n\\nThese first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.\\n\\nWhile there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above.\\n', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' that symbolizes the same idea described above.\\n\\nplotnine\\n\\nplotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.\\n\\nInstallation\\n\\nBefore getting started, you have to install plotnine. As always, there are two main options for doing', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' As always, there are two main options for doing so: pip and conda.\\n\\nPlotting\\n\\nHaving installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.\\n\\nBuilding a plot using the grammar of graphics\\n\\nAs you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=', we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot:\\n\\nThe code above will yield the following output:\\n\\nWhile this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot.\\n\\nFor instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this:\\n\\nPlotting Multidimensional Data\\n\\nBesides basic plots, you can do almost everything you could otherwise do in gg', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content=' do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot:\\n\\nAdding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like:\\n\\nConclusion\\n\\nAs you can see, plotnine', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content='Conclusion\\n\\nAs you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.', metadata={'Title': 'How to Use ggplot2 in Python'}),\n",
       " Document(page_content='Photo credit to Mika Baumeister from Unsplash\\n\\nWhen I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics.\\n\\nIf you want to work with data frames and run models using pyspark, you can easily refer to', metadata={'Title': 'Databricks: How to Save Data Frames as CSV Files on Your Local Computer'}),\n",
       " Document(page_content=' using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.\\n\\n1. Explore the Databricks File System (DBFS)\\n\\nFrom Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”.\\n\\nDBFS FileStore is where you create folders and', metadata={'Title': 'Databricks: How to Save Data Frames as CSV Files on Your Local Computer'}),\n",
       " Document(page_content='DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.\\n\\n2. Save a data frame into CSV in FileStore\\n\\nSample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”)\\n\\nUsing the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(', metadata={'Title': 'Databricks: How to Save Data Frames as CSV Files on Your Local Computer'}),\n",
       " Document(page_content=' CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.', metadata={'Title': 'Databricks: How to Save Data Frames as CSV Files on Your Local Computer'}),\n",
       " Document(page_content='A Step-by-Step Implementation of Gradient Descent and Backpropagation\\n\\nThe original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in.\\n\\nPhoto from Unsplash\\n\\nNeural network in a nutshell', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content='splash\\n\\nNeural network in a nutshell\\n\\nThe core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=' repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t).\\n\\nThe mathematical intuition\\n\\nPhoto from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\\n\\nFor my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=' the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer.\\n\\nThe model output calculation, in this case, would be:\\n\\nOften the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk.\\n\\nThe complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=' all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction.\\n\\nMore thoughts:\\n\\nNotice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=' an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.”\\n\\nPutting the above process into code:\\n\\nBelow is the complete example:\\n\\nimport numpy as np class NeuralNetwork:\\n\\ndef __init__(self):\\n\\nnp.random.seed(10) # for generating the same results\\n\\nself.wij = np.random.rand(3,4) # input to hidden layer weights\\n\\nself.wjk = np.random.rand(4,1) # hidden layer to output weights\\n\\n\\n\\ndef sigmoid(self,', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content='\\n\\n\\n\\ndef sigmoid(self, x, w):\\n\\nz = np.dot(x, w)\\n\\nreturn 1/(1 + np.exp(-z))\\n\\n\\n\\ndef sigmoid_derivative(self, x, w):\\n\\nreturn self.sigmoid(x, w) * (1 - self.sigmoid(x, w))\\n\\n\\n\\ndef gradient_descent(self, x, y, iterations):\\n\\nfor i in range(iterations):\\n\\nXi = x\\n\\nXj = self.sigmoid(Xi, self.wij)\\n\\nyhat = self.sigmoid(Xj, self.', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content='.sigmoid(Xj, self.wjk)\\n\\n# gradients for hidden to output weights\\n\\ng_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))\\n\\n# gradients for input to hidden weights\\n\\ng_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))\\n\\n#', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=\"Xi, self.wij))\\n\\n# update weights\\n\\nself.wij += g_wij\\n\\nself.wjk += g_wjk\\n\\nprint('The final prediction from neural network are: ')\\n\\nprint(yhat) if __name__ == '__main__':\\n\\nneural_network = NeuralNetwork()\\n\\nprint('Random starting input to hidden weights: ')\\n\\nprint(neural_network.wij)\\n\\nprint('Random starting hidden to output weights: ')\\n\\nprint(neural_network.wjk)\\n\\nX = np.array([[0, 0, 1], [1, 1, 1],\", metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content=', 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\\n\\ny = np.array([[0, 1, 1, 0]]).T\\n\\nneural_network.gradient_descent(X, y, 10000)\\n\\nReferences:', metadata={'Title': 'A Step-by-Step Implementation of Gradient Descent and Backpropagation'}),\n",
       " Document(page_content='Want to be inspired? Come join my Super Quotes newsletter. 😎\\n\\nSQL (Structured Query Language) is a standardised programming language designed for data storage and management. It allows one to create, parse, and manipulate data fast and easy.\\n\\nWith the AI-hype of recent years, technology companies serving all kinds of industries have been forced to become more data driven. When a company that serves thousands of customers is data driven, they’ll need a way to store and frequently access data on the order of millions or even billions of data points.\\n\\nThat’s where SQL comes in.\\n\\nSQL is popular because it’s both fast and easy to understand. It’s designed', metadata={'Title': 'An Easy Introduction to SQL for Data Scientists'}),\n",
       " Document(page_content=' and easy to understand. It’s designed to be read and written in a similar way to the English language. When an SQL query is used to retrieve data, that data is not copied anywhere, but instead accessed directly where it’s stored making the process much faster than other approaches.\\n\\nThis tutorial will teach you the basics of SQL including:\\n\\nCreating database tables\\n\\nPopulating the database tables with real data\\n\\nRetrieving your data for usage in a Data Science or Machine Learning task\\n\\nLet’s jump right into it!\\n\\nInstalling MySQL\\n\\nThe first thing we’ll do is actually install our SQL server! That’ll give us a workbench to start playing around', metadata={'Title': 'An Easy Introduction to SQL for Data Scientists'}),\n",
       " Document(page_content='ll give us a workbench to start playing around with databases and SQL queries.\\n\\nTo install a MySQL server, you can run the following command from your terminal:\\n\\nsudo apt-get install mysql-server\\n\\nNow we’ll start our MySQL server. This is similar to how we start Python in the terminal by just typing out “python”. The only difference here is that it’s convenient to give our server root privileges so we’ll have flexible access to everything.\\n\\nsudo mysql -u root -p\\n\\nGreat! Now our mysql server is running and we can start issuing MySQL commands.\\n\\nA couple of things to keep in mind before we move forward:', metadata={'Title': 'An Easy Introduction to SQL for Data Scientists'}),\n",
       " Document(page_content='Hypothesis testing visualized\\n\\nIn this article, we’ll get an intuitive, visual feel for hypothesis testing. While there are many articles online that explain it in words, there aren’t nearly enough that rely primarily on visuals; which is surprising since the subject lends itself quite well to exposition through pictures and movies.\\n\\nBut before getting too far ahead of ourselves, let’s briefly describe what it even is.\\n\\nWhat is\\n\\nBest to start with an example of a hypothesis test before describing it generally. The first thing we need is a hypothesis. For example, we could hypothesize that the average height of men is greater than the average height of women. In the spirit of ‘proof by', metadata={'Title': 'Hypothesis testing visualized'}),\n",
       " Document(page_content=' women. In the spirit of ‘proof by contradiction’, we first assume that there is no difference between the average heights of the two genders. This becomes our default, or null hypothesis. If we collect data on the heights of the two groups and find that it is extremely unlikely to have observed this data if the null hypotheses were true (for example, “if the null is true, why do I see such a big difference between the average male and female heights in my samples?”), we can reject it and conclude there is indeed a difference.\\n\\nFor a general hypothesis testing problem, we need the following:\\n\\nA metric we care about (average height in the example above). Two (or more) groups', metadata={'Title': 'Hypothesis testing visualized'}),\n",
       " Document(page_content=' the example above). Two (or more) groups which are different from each other in some known way (males and females in the example above). A null hypothesis that the metric is the same across our groups, so any difference we observe in our collected data must merely be statistical noise and an alternate hypothesis which says there is indeed some difference.\\n\\nWe can then proceed to collect data for the two groups, estimate the metric of interest for them and see how compatible our data is with our null and alternate hypothesis. The last part is where the theory of hypothesis testing comes in. We’ll literally see how it works in the proceeding sections.\\n\\nHow to reject\\n\\nNow that we’ve formed our hypothesis and collected our', metadata={'Title': 'Hypothesis testing visualized'}),\n",
       " Document(page_content=' we’ve formed our hypothesis and collected our data, how do we use it to reject our null? The general framework is as follows:', metadata={'Title': 'Hypothesis testing visualized'}),\n",
       " Document(page_content='Latent Factors are “Hidden Factors” unseen in the data set. Let’s use their power. Image URL: https://www.3dmgame.com/games/darknet/tu/\\n\\nLatent Matrix Factorization is an incredibly powerful method to use when creating a Recommender System. Ever since Latent Matrix Factorization was shown to outperform other recommendation methods in the Netflix Recommendation contest, its been a cornerstone in building Recommender Systems. This article will aim to give you some intuition for when to use Latent Matrix Factorization for Recommendation, while also giving some intuition behind why it works. If you’d like to see a full implementation, you can go to my Kaggle kernel', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=', you can go to my Kaggle kernel: Probabilistic Matrix Factorization.\\n\\nBefore starting, let’s first review the problem we’re trying to solve. Latent Matrix Factorization is an algorithm tackling the Recommendation Problem: Given a set of m users and n items, and set of ratings from user for some items, try to recommend the top items for each user. There are many flavors and alternate deviations of this problem, most of which add more dimensions to the problem, like adding tags. What makes Latent Matrix Factorization powerful is that it yields really strong results from the core problem, and can be a good foundation to build from.\\n\\nWhen working with an User-Item matrix of ratings', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content='When working with an User-Item matrix of ratings, and reading an article on matrix factorization, the first place to look is in linear algebra. That intuition is correct, but its not exactly what you’d expect.\\n\\nSparse and Incomplete Matrix Algebra:\\n\\nTraditional Linear Algebra is the bedrock of Machine Learning, and that is because most machine learning applications have something Recommender Systems do not: a data-set without NaNs(incomplete data entries). For example, whenever you’re constructing a model, NaNs, or missing data, are pruned in the data pre-processing step, as most functions cannot work with unfilled values. Functions like Principal Component Analysis are undefined if there are missing', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' like Principal Component Analysis are undefined if there are missing values. However, Recommender Systems cannot work if you get rid of NaNs. Those NaNs exist for a reason: not every user has rated every item, and its a bit nonsensical to expect them to. Working with Sparse data is something that can be very different — and that’s what makes Recommendation an interesting problem.\\n\\nSparsity complicates matters. Singular Value Decomposition, a factorization of a m x n Matrix into its singular and orthogonal values, is undefined if any of the entries in the Matrix are undefined. This means we cannot explicitly factorize the Matrix in such a way where we can find which we can find which diagonal(or', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' can find which we can find which diagonal(or latent) factors carry the most weight in the data set.\\n\\nInstead, we’re going to approximate the best factorization of the Matrix, using a technique called Probabilistic Matrix Factorization. This technique is accredited to Simon Funk who used this technique in his FunkSVD algorithm to get very successful in the Netflix contest. For more reading, check out Simon’s original post.\\n\\nThe Approach:\\n\\nI’ll explain the algorithm, then explain the intuition.\\n\\nWe’ll first initialize two matrices from a Gaussian Distribution(alternatively, randomly initialize them). The first one will be a m x k matrix P while the second will', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' a m x k matrix P while the second will be a k x n matrix Q. When these two matrices multiply with each other, they result in an m x n matrix, which is exactly the size of our Rating matrix in which we are trying to predict. The dimension k is one of our hyper-parameters, which represents the amount of latent factors we’re using to estimate the ratings matrix. Generally, k is between 10–250, but don’t take my word for it — use a line search(or grid search) to find the optimal value for your application.\\n\\nWith our Matrices P, Q, we’ll optimize their values by using Stochastic Gradient Descent. Therefore', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' using Stochastic Gradient Descent. Therefore, you’ll have two more hyper-parameters to optimize, learning rate and epochs. For each Epoch, we’re going to iterate through every known rating in our original m x n matrix.\\n\\nThen, we’ll get a error or residual value e by subtracting the original rating value by the dot product of the original ratings’ user’s row in P and its item’s column in Q.\\n\\nIn normal Stochastic Gradient Descent fashion, we’ll update both of the matrices P and Q simultaneously by adding the current row for P and Q by the learning rate times the product of the', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=\" Q by the learning rate times the product of the error times the other Matrix’s values.\\n\\nHere it is in python. View it fully in my Kaggle Kernel.\\n\\n#randomly initialize user/item factors from a Gaussian\\n\\nP = np.random.normal(0,.1,(train.n_users,self.num_factors))\\n\\nQ = np.random.normal(0,.1,(train.n_items,self.num_factors))\\n\\n#print('fit') for epoch in range(self.num_epochs):\\n\\nfor u,i,r_ui in train.all_ratings():\\n\\nresid\", metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content='.all_ratings():\\n\\nresidual = r_ui - np.dot(P[u],Q[i])\\n\\ntemp = P[u,:] # we want to update them at the same time, so we make a temporary variable.\\n\\nP[u,:] += self.alpha * residual * Q[i]\\n\\nQ[i,:] += self.alpha * residual * temp self.P = P\\n\\nself.Q = Q self.trainset = train\\n\\n\\n\\nNow that we have the algorithm, why does it work and how do we interpret it’s results?\\n\\nLatent factors represent categories that are present in the data. For k=5', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' are present in the data. For k=5 latent factors for a movie data-set, those could represent action, romance, sci-fi, comedy, and horror. With a higher k, you have more specific categories. Whats going is we are trying to predict a user u’s rating of item i. Therefore, we look at P to find a vector representing user u, and their preferences or “affinity” toward all of the latent factors. Then, we look at Q to find a vector representing item i and it’s “affinity” toward all the latent factors. We get the dot product of these two vectors, which will return us a sense of how much the user likes the item', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content=' a sense of how much the user likes the item in context of the latent factors.', metadata={'Title': 'Introduction to Latent Matrix Factorization Recommender Systems'}),\n",
       " Document(page_content='Which 2020 Candidate is the Best at Twitter?\\n\\nThe contest for the 2020 Democratic party nomination will be fought in many arenas. Before the first debates in a month, before the campaign rallies in key states, and even before prime time TV interviews, the fight for the nomination has begun on Twitter. Each of the major Democratic candidates has a signifiant social media following. With these accounts, the candidates have the means to directly communicate to voters, the media, and the world. After all, we’ve seen that carefully crafted tweets can change narratives in the real world.\\n\\nKnowing this, I decided to collect all of the tweets from 11 of the top Democratic candidates for president. Three of these contenders have separate work accounts, so', metadata={'Title': 'Which 2020 Candidate is the Best at Twitter?'}),\n",
       " Document(page_content=' Three of these contenders have separate work accounts, so in total 14 profiles were analyzed. With this data, it’s possible to see which candidates make the best use of this new and powerful platform.\\n\\nTwitter Statistics\\n\\nFollowers\\n\\nThe candidate with the most Twitter followers is definely Bernie Sanders. Between his senate (@SenSanders) and personal (@BernieSanders) accounts, Sanders has over 17 million followers. No doubt some of these overlap, but it goes to show that his 2016 campaign created a massive social media following. Elizabeth Warren’s senate account is a distant third, while Cory Booker, Joe Biden, and Kamala Harris are also followed by multiple millions of people.\\n\\nThe follower count can best be seen', metadata={'Title': 'Which 2020 Candidate is the Best at Twitter?'}),\n",
       " Document(page_content='.\\n\\nThe follower count can best be seen as measure of the potential influence of a candidate online. The actual effectiveness of a large following depends on how good the candidate is at communicating.\\n\\nNumber of Tweets\\n\\nIf a follower count is like potential energy, then the number of tweets issued is analogous to kinetic energy. In this respect, Andrew Yang is the most energetic and also the most prolific of the 2020 candidates. With almost 3000 tweets in 2019, Yang uses social media far more than his peers. He is the one contender who probably leverages this…', metadata={'Title': 'Which 2020 Candidate is the Best at Twitter?'}),\n",
       " Document(page_content='Irreverent Demystifiers\\n\\nWhat if AI model understanding were easy?\\n\\nLet’s talk about the What-If Tool, as in “What if getting a look at your model performance and data during ML/AI development weren’t such a royal pain in the butt?” (Or ignore my chatter and scroll straight to the walkthrough screenshots below!)\\n\\nWhy bother with analytics for AI?\\n\\nBeing able to get a grip on your progress is the key to speedy iteration towards an awesome ML/AI solution, so good tools designed for analysts working in the machine learning space help them help you meet ambitious targets and catch problems like AI bias before it hurts your users.\\n\\nThe What-', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' it hurts your users.\\n\\nThe What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training.\\n\\nAnalytics is not about proving anything, so this tool won’t help with that. Instead, it’ll help you discover the unknown unknowns in your data faster. Learn more about explainable AI (XAI) and its limitations here.\\n\\nAbout the What-If Tool\\n\\nThe What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training. The first version (released in late 2018) was pretty, but you couldn’t use it unless you were all-in on TensorFlow. As someone who appreci', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='in on TensorFlow. As someone who appreciates the expediency of tools like Scikit Learn, I’m delighted that What-If Tool is now geared at all analysts working with models in Python.\\n\\nNo more TensorFlow exclusivity!\\n\\nWe’ve been incorporating feedback from internal and external users to make the tool awesome for data scientists, researchers, and corporate megateams alike. Learn more about our UX journey here. Without further ado, let me make myself a hexpresso and play with the most current version to give you my take on what’s awesome and what’s awful.\\n\\nWhat’s awesome about the What-If Tool?\\n\\nEasy to use and', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='-If Tool?\\n\\nEasy to use and versatile\\n\\nIn the current version of What-If Tool, we expanded access to the magic beyond TensorFlow afficionados. Yes, that’s right — no more TF-exclusivity! This is model understanding and quick data exploration for feature selection/preprocessing insights even if you’re allergic to TensorFlow. Want to compare models made in Scikit Learn or PyTorch? Step right up! Does it work with standard Jupyter notebooks? You bet! It also works with Colaboratory because we know you prefer to choose your weapon. The tool is designed to reduce the amount of code you need to write to get your eyes on your data', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' need to write to get your eyes on your data, so it’s built for ease of use.\\n\\nIn this screenshot, we’re using the tool to compare two classifiers (deep neural network on the x-axis, linear model on the y-axis) trained on the UCI Census Income Dataset to predict whether someone will earn more than $50,000 a year. Numbers closer to 1 indicate that a model is giving a stronger YES vote. The scatterplot shows the votes of one model versus the other. See the notebook here and play with it yourself if you’re feeling curious — no install required.\\n\\nAs expected, there’s a positive correlation but the models don’', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='s a positive correlation but the models don’t give identical results. (Working as intended! They’re different models, after all.) If I’m curious about how the model votes are related to, say marital status, it’s very easy to find out — simply select that feature from the dropdown menu.\\n\\nVoilà! Most of our dataset shows civil marriages and we see an interesting preponderance of other statuses where the models disagree with one another or both vote a strong no. Remember, this is analytics, so don’t jump to conclusions beyond our current dataset!\\n\\nThe What-If Tool is not going to give you every slice of every view of every way that you', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' every slice of every view of every way that you might want to explore your data. But it’s great at what it’s designed for: a first start with low effort. It also works on a subsample, which means you get a quick look quickly without having to pay the memory cost to ingest and process all your data if you don’t want to. Huzzah for speed!\\n\\nIt’s great at what it’s designed for: a first look with low effort.\\n\\nFighting AI bias\\n\\nThe What-If Tool is also your secret weapon for fighting AI bias. To understand why, check out my discussion of AI bias here. Its bias-catching features are not an', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' here. Its bias-catching features are not an accident — a large fraction of the project’s core team hails from Google Brain’s PAIR initiative aimed at human-centered research and design to make AI partnerships productive, enjoyable, and fair.\\n\\nIn the fairness tab, we can play with all kinds of uncomfortable questions. For example, we can find out where we’d have to set our classification thresholds (the ones you’d naively want to put at 50%) for males vs females in our test set to achieve demographic parity between them. Uh-oh.\\n\\nSmarter ML/AI iteration\\n\\nThe What-If Tool incorporates the Facets tool, which tackles the data analytics piece without', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='ets tool, which tackles the data analytics piece without the model understanding component.\\n\\nIn the features tab, I can get a quick look at histograms to show me how my features are distributed. Oh my goodness, capital loss is a super imbalanced feature with only ~4% nonzero values. I’m already itching to try dropping it and rerunning both models. If you’ve been around the block a few times (or studied the math) you’ll know that putting something like that in a linear model is bad news indeed. I see similar trouble with capital gains. (If you insist on using ’em, how about doing some light feature engineering to combine them? Minuses are awesome.) Ah,', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' combine them? Minuses are awesome.) Ah, and here’s a question for the more advanced analysts among you: can you see why optimizing for accuracy should make us very nervous?\\n\\nWhat-If puts both together to help you iterate smartly. Think of it like this: to figure out what to do next in the kitchen, you want a handy way to compare the tastiness of several potential recipes (with model understanding) and also getting a handle on what’s in your grocery bags (with data analytics) so you don’t accidentally use rotten tomatoes. Facets gave you eyes on your ingredients, while the What-If Tool goes a step further to deliver that plus the recipe comparison. If you�', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' deliver that plus the recipe comparison. If you’ve been cooking blindly, you’ll love this tool for iterative model development and training.\\n\\nExploring counterfactuals\\n\\nNever underestimate the power of being able to ask your own what-if questions, like “What if we raise this person’s work hours and change their gender? How does the model react?” The What-If Tool is purpose-built to give you more of a grip on guided what-if/counterfactual questions. The tool makes it easy to see how the prediction changes if you vary a variable (finally!) over its domain and shows you whether there’s some value where the prediction behaves in a', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='�s some value where the prediction behaves in a suspicious way and letting you see exactly where the classification flips from, say, NO to YES. Try playing with the counterfactual options to find a datapoint’s most similar counterpart in a different predicted class. It’s a great way to see the effects of subtle differences on your model’s output.\\n\\nBack to our first tab. That red point I’ve selected is one where the models are having an argument: neural network says nah, but linear model says a gentle yes to high income. What-If… I want to do a quick deep dive into that point off the diagonal? I simply click on it and there’s the', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' simply click on it and there’s the info. Turns out the linear model is right, this is a high income earner. Moreover, it’s a married woman who works 10 hours per week. I love how quickly I could see that.\\n\\nWhat’s this “visualize” thing on the left? Let’s see what happens if we try toggling the “counterfactual” setting.\\n\\nAha! Here’s the nearest buddy where neural network changes its mind and correctly predicts a large salary. And it is a buddy indeed: this is a male executive who works 45 hours a week. What-If… we do a deep dive and see which', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='If… we do a deep dive and see which of these differences the models are most sensitive to?\\n\\nLooking at the partial dependence plots, we can see that the neural network (blue) seems to expect pay to go up with hours worked, while the linear model (orange) slopes down. Curious. The statistician in me is shouting at all of us not to get excited — they’re probably both wrong in their own way, so we shouldn’t learn anything grand about the universe, but seeing how models react to inputs is very valuable for picking approaches to try next. Our mystery candidate’s lower hours worked look more compelling to the linear model (yeah, quiet down friends, obviously the economist in me is just', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' down friends, obviously the economist in me is just as suspicious as the statistician). I bet we also want to take a quick look at other features here — how about gender…?\\n\\nInterestingly, the linear model (orange) is not getting itself too excited about gender, but the neural network (blue) seems more reactive to it. How about our mystery woman’s question-mark of an occupation? Could that be contributing to her lower score by the neural network?\\n\\nWhoa, while the linear model (orange) is stoic again, the neural network (blue) gives execs a pretty big prediction boost relative to those with missing occupation information. Now isn’t the time to say that snarky thing', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='t the time to say that snarky thing about linear models versus neural networks, is it? Well, maybe I’ll restrain myself… the whole point of the tool is to give you eyes on your data so you can iterate wisely, not let biases take you by surprise, and create a more awesome model faster. We’re not done yet! (But I sure have a few ideas I’m inspired to try next.)\\n\\nLearn more about our two model types here.\\n\\nWhat’s annoying about the What-If Tool?\\n\\nWork in progress\\n\\nThe tool isn’t perfect yet. For example, you’ll occasionally stumble onto something guaranteed to earn a scowl', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' occasionally stumble onto something guaranteed to earn a scowl from Tufte fans — for example, the screenshot below had me ranting in a meeting recently. (If you can’t see why, it’s a good opportunity for a little data viz lesson: Why are the text labels “Young” and “Not Young” the only visual cues? Why not shape? Because we’re working on making it better in this way and in others too, but perfection takes time. As part of the collaboration, I rant on your behalf to help these issues should dissipate rapidly.)\\n\\nAlso… how about them axis labels?\\n\\nUnguided exploration\\n\\nThe tool will go where your curiosity', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' exploration\\n\\nThe tool will go where your curiosity takes it, but what do you do if you’re not feeling creative? Perhaps you wish the tool were more prescriptive, guiding your eye towards what’s important? Your feedback is on our radar and we’re working on it, but for those who think something beautiful might be lost if your exploration gets hemmed in, never fear! We believe in options and understand that not everyone wants the prescriptive side of things, just as not everyone wants to play video games with a fixed storyline as opposed to an open world.\\n\\nLimited customization\\n\\nYou want every customization under the sun, which is such a data-sciency thing to say. I�', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' data-sciency thing to say. I’ve said things like that too — I remember the first question I asked in a mandatory SAS training for stats PhD students: “How do I write these functions myself so they do exactly what I want?”.\\n\\nSo when you ask the same thing about the What-If Tool, I’ll tell you what my profs told me that day: that’s what raw Python and R are for! (Or, heaven help us, C/C++ if you’re going that far down into the weeds.) Visualization tools like the What-If Tool aren’t replacements, they’re accelerators. They give you a first look', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='re accelerators. They give you a first look with minimal effort so you know where to dig, but once you’ve picked your spot, you’re probably going to want to write your own code to dig exactly the way you like to. If you’re an expert analyst with your own awesome way of doing things, our goal is to help you narrow your search so there’s less code to write later, not to replace your entire approach.\\n\\nTensorFlow-ish terminology\\n\\nAnother thing that irritates me (and the rest of statistician-kind, I’m sure) is the terminology compromises we had to make for the sake of our TensorFlow user group, keeping some of', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' our TensorFlow user group, keeping some of the TensorFlow legacy lingo that makes traditional data scientists want to punch something. Yeah, that “inference” isn’t inference. TensorFlow is a hilarious bucket of words appropriated and promptly misused — fellow nerds, don’t even get me started on its use of “experiment”, “validation”, “estimator”, or the batch vs minibatch thing… Just let this be a lesson about thinking carefully about what you’re calling things when it’s just you and your buddies bouncing some ideas around in a garage. What if the project is a success and everyone will have to', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' the project is a success and everyone will have to live with your choices? Sigh.\\n\\nVerdict\\n\\nAll in all, these grumbles are on the petty side. Overall, I really like the What-If Tool and I hope you will too.\\n\\nSee it in action!\\n\\nWhile the What-If Tool is not designed for novices (you need to know your way around the basics and it’s best if this isn’t your first rodeo with Python or notebooks), it’s an awesome accelerant for the practicing analyst and ML engineer.\\n\\nIf you’re eager to see the What-If Tool in action, you don’t have to install anything', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=', you don’t have to install anything — just go here. We’ve got dazzling demos and docs aplenty. If you want to start using it for realsies, you don’t even need to install TensorFlow. Simply pip install witwidget.\\n\\nIf you’re a fan of Google Cloud Platform, you might be excited by a new integration that just got announced. Now you can connect your AI Platform model to the What-If Tool with just one method call! Check out how here.\\n\\nThanks for reading! How about an AI course?\\n\\nIf you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content=' course designed to be fun for beginners and experts alike, here’s one I made for your amusement:\\n\\nEnjoy the entire course playlist here: bit.ly/machinefriend\\n\\nLiked the author? Connect with Cassie Kozyrkov\\n\\nLet’s be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.', metadata={'Title': 'What if AI model understanding were easy?'}),\n",
       " Document(page_content='What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur\\n\\nPhoto by Georgie Cobbs on Unsplash\\n\\nQuick Bio\\n\\nBefore his many data scientist stints in companies scattered throughout Germany, Abhishek Thakur earned his bachelor’s in electrical engineering at NIT Surat and his master’s in computer science at the University of Bonn. Currently, he holds the title of Chief Data Scientist at Norway’s boost.ai, a “software company that specializes in conversational artificial intelligence (AI).” But I’m most impressed by Abhishek’s Kaggle clout.\\n\\nYou can visit', metadata={'Title': 'What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur'}),\n",
       " Document(page_content=' Kaggle clout.\\n\\nYou can visit his Kaggle profile here. Here’s a snapshot of his accolades:\\n\\nCompetitions Grandmaster (17 gold medals and an all-time high rank of #3 in the world)\\n\\nKernels Expert (he’s well within the top 1% of Kagglers)\\n\\nDiscussion Grandmaster (65 gold medals and an all-time high rank of #2 in the world)\\n\\nI want to take a look at Abhishek’s tutorial, Approaching (Almost) Any NLP Problem on Kaggle. I’ve selected this kernel of Abhishek’s because I myself have been trying to', metadata={'Title': 'What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur'}),\n",
       " Document(page_content='’s because I myself have been trying to learn more about natural language processing, and how could I resist learning with Kaggle’s Halloween-themed Spooky Authors dataset?\\n\\nAbhishek’s Approach to NLP\\n\\nI would highly encourage you to read this article alongside the kernel. And if you really want a firmer grasp of NLP or data science in general, be sure that you understand every line of Abhishek’s code by writing it yourself as you go through his kernel.\\n\\nJust so we don’t forget — our task is to identify the author (EAP — Edgar Allen Poe; HPL — H.P. Lovecraft; MWS — Mary', metadata={'Title': 'What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur'}),\n",
       " Document(page_content=' H.P. Lovecraft; MWS — Mary Wollstonecraft Shelley) of each sentence in the test set.\\n\\n1. Exploring the Data and Understanding the Problem\\n\\nAfter importing the necessary Python modules and the data, Abhishek calls the head() method on the data to see what the first five rows look like. Since Abhishek is a pro and this is an NLP problem, the exploratory data analysis (you’ll most often see this referred to as EDA) is shallow compared to problems involving numerical data. Data science newcomers might benefit from more thorough EDA. A…', metadata={'Title': 'What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur'}),\n",
       " Document(page_content='Making a DotA2 Bot Using ML\\n\\nDesigning a resource-efficient machine-learning algorithm Musashi Schroeder · Follow 18 min read · May 30, 2019 -- 1 Listen Share\\n\\nThe bot roster\\n\\nProblem\\n\\nIn December of 2018, the creators of AI Sports gave a presentation and introduced the DotA2 AI Competition to the school. DotA (Defense of the Ancients) is a game played by two teams, each consisting of five players who can choose from over one hundred different heroes. The goal of the game is to destroy the opponents base, while defending your own. Each hero has access to at least four unique abilities, and are able to purchase items that also have different abilities. Items are purchased with gold earned', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' have different abilities. Items are purchased with gold earned from destroying opponent structures, or defeating players or creeps, NPC (non-player character) units that spawn to help defend and attack bases.\\n\\nThe complexity of the game comes from not only the roster of characters, but the ever changing state of the map. Full information games such as chess or go leave no information withheld from players, allowing them to see every possible action on the board at any given time. The DotA map includes a “fog of war” which hides any part of the map not seen by a player or their teammates. Each hero’s abilities also have “cooldowns” — after a player uses an ability, they cannot use it', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' a player uses an ability, they cannot use it again for a set amount of time — and use mana as a resource. While a player has access to this information about their allies, they do not have this information about the opponent, and must take it into consideration when engaging in fights.\\n\\nThe rules for the competition were to program a full team of five bots to play in Captain’s Mode. Captain’s Mode sets one member of each team as a captain, giving them the ability to choose the heroes for the rest of the team, and also “banning”, or selecting heroes that the opponent’s team cannot use. In order to avoid having all of our characters banned, we needed to program', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' all of our characters banned, we needed to program at least sixteen of them. Our limitation set by Maurice,42 Silicon Valley staff, was that we could not use the built in “desires,” a system to provide default behaviors for the bot to execute, provided by Valve. Instead of using the default bot behaviors, we were tasked with writing the code from the bottom up. The API for DotA2 is written in Lua and allows players to create their own bots. The competition was originally designed to use a C++ API written by\\n\\nOverview of DotA battlefield with labels, image from https://dota2.gamepedia.com/Map\\n\\nthe AI Sports creators, but due to “complications,', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' creators, but due to “complications,” our team used Lua instead.\\n\\nHow Was it Solved\\n\\nLearning Lua and the API\\n\\nIn order to create the bot, we first read through the API and looked for other examples that users had created. The DotA API was made available in early 2016, though hasn’t received any meaningful updates since approximately October of 2017. The first resource we used was a guide on getting started, written by RuoyuSon. RuoyuSon explained where to find other resources and how to start games, as well as useful console commands for the testing process. Valve also provides small examples of bot scripts in the games directory that can be used to potentially get started.', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' directory that can be used to potentially get started. With the API and other examples, we naively believed we could create a bot and have a crude, working version of the code within a week.\\n\\nThe first challenge came in selecting the heroes we wanted to use and starting the game. What we didn’t know at the time was that if the code for hero selection has an error, the entire game will crash without displaying anything. The example provided by Valve can be used to quickly create hero selection code for All Pick Mode, but is unusable for Captain’s Mode. In order to select heroes, we read through other examples of the code. Although our current iteration of the bot allows for human players to play against and', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' the bot allows for human players to play against and alongside it, the original version was only meant to play against another bot in Captain’s Mode. Finally getting a simple version of the hero selection working took a little over one week, but since then has been modified to support All Pick Mode and human players.\\n\\nAfter getting the game to start, we began experimenting with making heroes walk to locations on the map. We quickly learned not knowing the Lua language made writing and understanding other examples of code difficult. While we were able to make bots walk to certain locations or purchase items, we frequently made syntax errors and finding bugs in code took considerable time. After a frustrating two weeks, we took time to learn the language before engaging with the API', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' time to learn the language before engaging with the API again.\\n\\nWhile the tournament drew near, we were still figuring out Lua and fighting to understand the API. Our heroes moved to the correct locations and they were able to fight enemy minions and opponents, albeit poorly, but they would never retreat, resulting in death after death. Even against the easiest default bot difficulty, Passive, we were unable to win. We implemented a crude retreat function — simply telling the bots to run back to their base if they took too much damage — that helped, but left a lot to be desired. We were able to consistently win against the Passive bot, but usually ended the game with close to 100 deaths per game on our side, and we were lucky to see', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' on our side, and we were lucky to see two deaths on the opponents.\\n\\nThe next step, now that the groundwork had been laid for bot behaviors, was to begin to individualize each bot so that they could use their abilities. Each bot uses their skills based on conditions, allowing them to fight the enemy. At this point our lack of DotA experience began to show — although the bots were able to use skills, they didn’t use them optimally simply because we didn’t know what optimal was. We frequently asked people with more experience for tips and slowly made the bot stronger. Finally, we were able to defeat the Passive bot with a positive score. We attempted to beat the Easy difficulty, but struggled.', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' attempted to beat the Easy difficulty, but struggled. The difference between the two was significant and we needed to implement more behaviors in order to win.\\n\\nState Machine\\n\\nUp to this point, all code had been written as predefined actions for each bot to execute. The complexity of DotA gradually made it more and more difficult to separate what actions to take and when. Should we fight in this instance, or run away? When should we focus on hitting creeps? While we were able to defeat the Passive difficulty consistently, we realized that Easy would be a significant hurdle. We began to discuss possible options, and landed on the State Machine.\\n\\nExample of State Machine\\n\\nWhen modifying bot behaviors, it became impossible to cleanly separate', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' bot behaviors, it became impossible to cleanly separate when it would perform actions. They were so closely intertwined that adjusting one would affect the performance of the other, and none of the behaviors worked particularly well. We were also unable to include more behaviors neatly without disrupting other parts of the code. By creating the State Machine, we were able to separate each behavior, using weighted averages to decide which would be the most optimal in any instance of the game. The code for each bot is run every frame, allowing for constant calculations and giving each behavior a value as a weight. Assuming we programmed the bot well, it could now decide for itself what it should do based on the game state.\\n\\nAt this point we were able to separate each behavior into', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' this point we were able to separate each behavior into its own distinct code, broken down into components and conditions. Components are pieces of information that are always necessary to calculate a behavior, while conditions would add or subtract from the weight only under specific circumstances. Separating the code allowed us to make each behavior perform better — previously, each behavior was reliant on another, but by using the State Machine, we would only execute the parts of the code we needed to and only when we needed to.\\n\\nWhile some of us set up the State Machine, we also continued to improve the non-State Machine version, to the point that we were able to defeat the Easy difficulty. We were once again seeing 100 deaths on the scoreboard, but would get', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' seeing 100 deaths on the scoreboard, but would get more kills on our side and eke out wins. The code from the non-State Machine version easily slotted into our new bot, allowing us to continue working without any significant delays.\\n\\nOne of the benefits of the State Machine was the modularity of the system. Prior to this, the bot’s generic behaviors were made up of two files that had necessary comments written throughout in order to understand which part of the code was being looked at — the new version had separate files for each weight and the behaviors were separated so they did not interact with one another. The modularity allowed multiple people to work on different parts of the project without affecting what someone else might be working on,', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' without affecting what someone else might be working on, improving clarity, simplicity, and the team’s workflow.\\n\\nWe were also preparing for our first bot vs bot match against another team in the competition, but the State Machine was untested and not ready to implement. This gave us one last chance to see how the previous version held up. Before we started our scrimmage, we decided to test and make sure both teams’ code ran properly. When both bots had a random mix of the opponent’s heroes and their own, the teams realized we had made an error in the picking phase. Both teams were able to fix the issue, but it was another instance of fighting with the API, something that would persist throughout the entire', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' the API, something that would persist throughout the entire process. At this time, we were also notified by Maurice that the tournament would postponed for a month, giving us a chance to continue to improve our bots.\\n\\nDuring testing against Valve’s proprietary bots, we would frequently have to restart games because of the compatibility issues with their bot and Captain’s Mode. We decided to make our own picking mode for two teams in order to speed up the process, and cut down on the unnecessary restarts. We gave our opponent’s bots a random team of five and used this team for much of our testing. What we didn’t know at the time was that this would come back to bite us later.\\n\\n', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' would come back to bite us later.\\n\\nOur team continued to work with the State Machine, adding more behaviors that we were unable to implement before. As the behaviors increased, we also started to see improvements in our matches against Valve’s bot. After defeating Easy, within 24 hours we were able to beat Medium, and the next day we beat Hard and Unfair back to back. We were ecstatic, not expecting to beat Unfair much later down the line, but as we decided to watch the opponent’s bots closer, our jaws dropped. Two of our opponent’s bots didn’t buy items, and one didn’t use any abilities. Although we were able to win, still a feat in', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' we were able to win, still a feat in and of itself, it wasn’t a real victory against the Unfair bot.\\n\\nWhat we didn’t know was that Valve only implemented specific skill usage and item purchasing on 46 of the bots. We changed the opponent’s lineup to five of those bots, and while we could put up a good fight against Hard and win about forty percent of the time, we rarely won against Unfair. We began to have more discussions about what we could do to increase our win rate, resulting in our first roster change. After looking at the heroes we had implemented, at the time only five, we decided to switch out heroes that would hopefully fit our overall game plan better.', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' that would hopefully fit our overall game plan better. Immediately we saw an increase and, while we had become attached to the heroes we chose to use, we began to consider swapping heroes as an option as we continued to program.\\n\\nData Gathering\\n\\nWe continued to implement more behaviors into the State Machine, and added more features and, as we did, saw a slow but steady increase in performance in our matches. In order to see how well we did when including something new, we had to watch a full game to observe the specific behavior and to see whether or not we won the match. All of the bot’s weights were hand-tuned, and any tweaks we made might not be visible within a single game. Even sped', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' not be visible within a single game. Even sped up, a game would take between ten and fifteen minutes. In order to gather any meaningful data, we could spend over hours just watching. To speed up this process and make sure that any change we added was meaningful, using Python, the Go programming language , and Docker, we began to create a way of gathering data over hundreds of games.\\n\\nMaurice gave us access to fifteen computers which we could use to run games on and gather data. At this point, we had researched a “headless” mode for DotA; we were able to run games graphicless which would speed up the games themselves, and allow us to run multiple instances of the game without using the', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' to run multiple instances of the game without using the GPU. Using Docker, we set up a client to server connection that allowed us to use virtual machines on fourteen of those computers. We calculated that we could run up to four games per computer optimally, so ran four virtual machines at six times speed. Altogether, we were able to run games approximately 300 times faster than with originally.\\n\\nEach game could range between fifteen and eighty minutes. Docker Swarm distributed the total number of games requested evenly to all of our worker computers. If we were running less than 56 games this solution would be fine, but anything more would be suboptimal. We initially attempted to deploy using Docker Swarm, but it made more sense for us to create our own', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' it made more sense for us to create our own solution. It would need to be customizable, work well on a distributed network, and have support for simple concurrency. We decided to use Go because it filled our criteria and was easy to build and deploy. Finally, Python was used to graph and illustrate our data results as histograms and line graphs.\\n\\nData showing wins and losses over time\\n\\nUsing this setup, we were able to run 500 games over an hour, giving us meaningful data. While it was still necessary for us to watch games to observe and confirm behaviors worked properly, we could now test them and gather data in order to confirm whether or not a change was beneficial or detrimental to the bot.\\n\\nAs we went', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' detrimental to the bot.\\n\\nAs we went into the final weeks, we played with the idea of incorporating a genetic algorithm. The State Machine weights were all hand tuned and based on our observations. Specifically our Farm, Hunt, and Retreat weight were so closely tied together that by changing the values of one, we would see dramatic differences in the way they played and their win rates would generally decrease. We knew they were at a good point, but were sure they weren’t optimal, especially considering different characters played differently and using the same weights made them all play more or less the same. Using a genetic algorithm would use machine learning to tune each weight, giving us the most ideal numbers to defeat the default bots, and hopefully our opponents', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' to defeat the default bots, and hopefully our opponents in the tournament. An ambitious goal was to create different genes for each character, thereby giving them each their own unique play style, but we knew that without more time or more computing power, we would have to make do with the hand-tuned weights we had.\\n\\nA week before the competition, we strayed away from adding major features, only including small changes that our data decisively proved would increase the win rate. By the end, with the State Machine we were able to achieve a consistent win rate above 98% against the Valve bots. Ready for the competition, Maurice messaged us, informing us that once again the competition had been extended for another month.\\n\\nGenetic Al', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' extended for another month.\\n\\nGenetic Algorithm\\n\\nWith the month long extension to the tournament, we began to discuss how we could create a genetic algorithm. In the end, we decided to use Go once again because our data gathering programs had already been written in it, therefore making it easier to tie the programs together.\\n\\nGenetic algorithm flowchart, from arwn\\n\\nIn order to get the genetic algorithm to work, we needed to run multiple iterations of our bot. From those iterations, we would grab the top five heroes genes and “breed” them by shuffling, averaging, and splicing them together. The next generation would be made up of slightly modified versions (using a 10% mutation probability', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' slightly modified versions (using a 10% mutation probability to choose which genes to change, and 10% mutation rate to change each gene by the respective amount) which we would then gather data on, repeating the process until the beginning of the competition. Our plan was to replace the current hand-tuned genes with our new machine learned ones.\\n\\nOur first step was making sure we could run the genetic algorithm using Go and Docker, and modify the Lua script at the same time. Each bot’s gene was a Lua file containing the values we wanted to mutate using the genetic algorithm. We used Go to read in the gene file, mutated the values, and output the new gene using a gene template. The new genes were then used', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' a gene template. The new genes were then used for the subsequent iterations.\\n\\nHaving successfully created a way to read and write to our new gene files, instead of making one generic genetic algorithm as we had originally planned, we created genes for each hero we were using. In order to make it work, each file had to include the name of the hero we were writing to. Unfortunately we could only train five heroes at a time, so we opted to train our starting lineup and use our hand tuned genes for the rest of the heroes we had implemented.\\n\\nFinishing the genetic algorithm ended up taking longer than planned. We hoped to have it running and training within a week, but needed a few more days to iron out bugs. We', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' a few more days to iron out bugs. We had each made separate parts of the genetic algorithm, and piecing each together took some time.\\n\\nFinally, the genetic algorithm worked, but as we began to run the first generations, we ran into multiple issues. At this point, we had continued to have some issues with our Docker containers not running games, but had chosen to ignore it for the time being because while it had been slower in collecting data, it wasn’t a significant time difference. If one computer malfunctioned and dropped off the network the server would hang, waiting for data to come in from the downed machine. When we decided to use the genetic algorithm, we needed it to run non-stop and continue working through', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' it to run non-stop and continue working through each generation. If a worker failed to respond, the server could never move onto the next generation because it was waiting for the remaining games to come in. It made little sense for us to monitor the computers in shifts all day, so we added in a way of timing out if we did not get a response from the container after a period of time.\\n\\nIn the end, after about four days of starting and stopping the genetic algorithm, we finally had it working. While running the genetic algorithm and confirming that it worked, we decided to change our team lineup in favor of one we thought could raise our win rate. When we began running the genetic algorithm and set up the genes we wanted to', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' genetic algorithm and set up the genes we wanted to manipulate, as a team, we went through them and adjusted them to numbers we believed made sense for the genetic algorithm to start on. At that time, we decided to manipulate approximately 25 components and conditions, the “genes,” from our Farm, Hunt, and Retreat weights. This change combined with a new hero selection we used for the opposing team dropped our win rate from 98% to 80%. While the genetic algorithm was slowly raising the win rate, we spoke as a team and decided that if we could boost it by switching or adding heroes early on, it could be worth testing. After the switch, the initial 80% rose closer to 90%.\\n\\nWhile we observed', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' rose closer to 90%.\\n\\nWhile we observed the bot, we knew that time was beginning to run out and it wasn’t growing fast enough. Although it was a risky decision that could result in a potentially drastic decrease in win rate, we decided to adjust the rate of change from 10% mutation probability and 10% mutation rate to 15% and 25% respectively. We calculated that in the most ideal situation, in order to get rid of a gene that was not useful, it would take at least thirty generations, or at least one week. We wanted to reduce that number and figured if we doubled it, we would see a higher rate of change, for better or for worse. After days of observing the results, our risk paid', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' After days of observing the results, our risk paid off and the bot saw a faster and more consistent increase in win rate.\\n\\nFitness growth over time\\n\\nOnce we were sure of the outcome, we began to add in more genes to manipulate from other State Machine weights. Another problem we ran into throughout the project that we had been unable to solve was how to play early on in the game, and how to play near the end. In DotA, the play styles between the two are drastically different. The behaviors that are important early on are less important as the game goes on for longer, and vice versa. Our strategy up to this point had been to trade a slightly weaker start to the game for a more powerful finish. We', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' to the game for a more powerful finish. We had tried to tweak the weights multiple times, but even if they played better at the beginning, the manipulated weights would fail in the end dropping the overall win rate. Now that we had our working genetic algorithm, we added in various multipliers to health for it to adjust, but also decided to add in multipliers based on how powerful the hero is. Heroes go from level 1 to 25 and get stronger as they gain levels. By hand we were never able to manipulate the weights in an effective way that would allow for early and late game adjustments. With the genetic algorithm, we could now leave it up to the computer to decide when to play differently.\\n\\nAfter one more hero change, we', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content='.\\n\\nAfter one more hero change, we settled on our final roster and continued to let the genetic algorithm work. A few days before the beginning of the tournament, we saw that the bot had finally reached a 99% win rate for one generation, but this dropped the next generation to 96%. While our rapid manipulation of genes had created a powerful bot, once it got closer and closer to it’s theoretical peak, the 25% mutation rate would change to much at once and dropped the win rate. We decided that in order to preserve our win rate, we would need to slow down the mutation. The mutation probability was dropped to 7% and the mutation rate was dropped to 15%.\\n\\nAs we changed our genetic algorithm once again', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content='\\n\\nAs we changed our genetic algorithm once again, we decided to take another risk. Up to this point we had been taking the top five genes from each hero as parents, breeding them and using the offspring for the next generation. While this had worked for us, classically it was not how we should have been using a genetic algorithm. In a genetic algorithm, all genes should have a chance of being picked, but we were actively selecting which to use. The importance of using the lower win rate bots is diversity. While in the generation it may not have performed as well, in a future generation its genes may be an important part towards increasing the win rate. In order to make sure those lower win rate genes had a chance at being selected', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' lower win rate genes had a chance at being selected, we mapped all of the genes, allowing a higher probability of being picked to the higher win rates, while still giving the lower win rates a chance, albeit smaller.\\n\\nWe also discussed a change in strategy. Taking the genes from each individual bot was necessary, but we considered the importance of taking all of the genes from one “team” of bots. A bot at first glance may have looked like it had less potential, but as part of a team it’s genes could have been an important key to victory. We thought about the benefits of switching from the individual bots to only breeding teams, but couldn’t justify losing out on more powerful heroes genes. As', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' justify losing out on more powerful heroes genes. As we came to the conclusion that we should continue using the same strategy of selecting the individual bots, a thought popped into our heads. What if we do both? Taking the individual genes was undeniably important, but by breeding bots from the same team with the stronger individual bots, we believed we could unlock the potential of both worlds.\\n\\nConclusion\\n\\nThe genetic algorithm seems to have improved the bot, although its play style is considerably different from the original non-genetic version. While we were much more aggressive earlier, we now play more conservatively and aim to win mostly by destroying structures and winning the occasional team fights. The older bot would group together more often as a team, forcing opponent bots', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' together more often as a team, forcing opponent bots to react and resulting in more fights. If we continued to work on the project, I believe the next step would be having the bot begin to fight itself and the older, non-genetic version of the bot.\\n\\nThrough this project, I’ve been able to learn multiple programming languages, as well as familiarize myself with Docker, and the importance of documentation when working as a team. The reason I decided to work on this project was less an interest in DotA2, but more trying to understand machine learning. I’d heard the term multiple times, had read about it, but didn’t have a real understanding of what it entailed and how to', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content=' real understanding of what it entailed and how to actually program it. Participating on this project gave me a unique opportunity to work with machine learning, and has increased my understanding of programming as a whole.', metadata={'Title': 'Making a DotA2 Bot Using ML'}),\n",
       " Document(page_content='This is a tutorial on building a Chrome Extension that leverages Serverless architecture. Specifically — we will use Google Cloud Functions in the Back-End of our Chrome Extension to do some fancy Python magic for us.\\n\\nThe Extension we will build is the SummarLight Chrome Extension:\\n\\nThe SummarLight Extension takes the text of he current web page you are on (presumably a cool blog on medium such as mine) and highlights the most important parts of that page/article.\\n\\nIn order to do this, we will setup a UI (a button in this case) which will send the text on our current web page to our Back-End. The ‘Back-End’ in this case will be a Google', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='End’ in this case will be a Google Cloud Function which will analyze that text and return its Extractive Summary (the most important sentences of that text).\\n\\nArchitecture\\n\\nA Simple & Flexible Architecture\\n\\nAs we can see, the architecture is very straightforward and flexible. You can setup a simple UI like an App or, in this case, a Chrome Extension, and pass any complex work to your serverless functions. You can easily change your logic in the function and re-deploy it to try alternative methods. And finally, you can scale it for as many API calls as needed.\\n\\nThis is not an article on the benefits of serverless so I will not go into details about the advantages of using', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=' will not go into details about the advantages of using it over traditional servers. But usually, a serverless solution can be much cheaper and scalable (but not always…depends on your use case).\\n\\nThe Chrome Extension\\n\\nHere is a good guide on the setup of a Chrome Extension:\\n\\nAnd all the code for the SummarLight Extension can be found here:\\n\\nThe main.py file in the root directory is where we define our Google Cloud Function. The extension_bundle folder has all the files that go into creating the Chrome Extension.\\n\\nGoogle Cloud Function\\n\\nI chose Google instead of AWS Lamba because I had some free credits (thanks Google!) but you can easily do it with AWS as well', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=\" but you can easily do it with AWS as well. It was a huge plus for me that they had just released Google Cloud Functions for Python as I do most of my data crunching in that beautiful language.\\n\\nYou can learn more about deploying Google Cloud Functions here:\\n\\nI highly recommend using the gcloud sdk and starting with there hello_world example. You can edit the function in the main.py file they provide for your needs. Here is my function:\\n\\nimport sys\\n\\nfrom flask import escape\\n\\nfrom gensim.summarization import summarize\\n\\nimport requests\\n\\nimport json\\n\\n\\n\\n\\n\\ndef read_article(file_json):\\n\\narticle = ''\\n\\nfiledata\", metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=\"\\n\\narticle = ''\\n\\nfiledata = json.dumps(file_json)\\n\\nif len(filedata) < 100000:\\n\\narticle = filedata\\n\\nreturn article\\n\\n\\n\\ndef generate_summary(request):\\n\\n\\n\\nrequest_json = request.get_json(silent=True)\\n\\nsentences = read_article(request_json)\\n\\n\\n\\nsummary = summarize(sentences, ratio=0.3)\\n\\nsummary_list = summary.split('.')\\n\\nfor i, v in enumerate(summary_list):\\n\\nsummary_list[i] = v.strip() + '.'\\n\\nsummary_list.remove('\", metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=\" '.'\\n\\nsummary_list.remove('.')\\n\\n\\n\\nreturn json.dumps(summary_list)\\n\\nPretty straight forward. I receive some text via the read_article() function and then, using the awesome Gensim library, I return a Summary of that text. The Gensim Summary function works by ranking all the sentences in order of importance. In this case, I have chosen to return the top 30% of the most important sentences. This will highlight the top one third of the article/blog.\\n\\nAlternative Approaches: I tried different methods for summarization including using Glove Word Embeddings but the results were not that much better compared to Gensim (especially considering the increased processing\", metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=' to Gensim (especially considering the increased processing compute/time because of loading in those massive embeddings). There is still a lot of room for improvement here though. This is an active area of research and better text summarization approaches are being developed:\\n\\nOnce we are good with the function we can deploy it and it will be available at an HTTP endpoint which we can call from our App/Extension.\\n\\nExtension Bundle\\n\\nNow for the Front-End. To start, we need a popup.html file. This will deal with the UI part. It will create a menu with a button.\\n\\n<!DOCTYPE html>\\n\\n<html>\\n\\n<head>\\n\\n<', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='html>\\n\\n<head>\\n\\n<link rel=\"stylesheet\" href=\"styles.css\">\\n\\n</head>\\n\\n<body>\\n\\n<ul>\\n\\n<li>\\n\\n<a><button id=\"clickme\" class=\"dropbtn\">Highlight Summary</button></a>\\n\\n<script type=\"text/javascript\" src=\"popup.js\" charset=\"utf-8\"></script>\\n\\n</li>\\n\\n</ul>\\n\\n</body>\\n\\n</html>\\n\\nAs we can see, the ‘Highlight Summary’ button has an onClick event that triggers the popup.js file. This in turn will call the summarize function:', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='. This in turn will call the summarize function:\\n\\nfunction summarize() {\\n\\nchrome.tabs.executeScript(null, { file: \"jquery-2.2.js\" }, function() {\\n\\nchrome.tabs.executeScript(null, { file: \"content.js\" });\\n\\n});\\n\\n}\\n\\ndocument.getElementById(\\'clickme\\').addEventListener(\\'click\\', summarize);\\n\\nThe summarize function calls the content.js script (yeah yeah I know we could have avoided this extra step…).\\n\\nalert(\"Generating summary highlights. This may take up to 30 seconds depending on length of article.\");\\n\\n\\n\\nfunction unicodeToChar(text)', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=\"\\n\\nfunction unicodeToChar(text) {\\n\\nreturn text.replace(/\\\\\\\\u[\\\\dA-F]{4}/gi,\\n\\nfunction (match) {\\n\\nreturn String.fromCharCode(parseInt(match.replace(/\\\\\\\\u/g, ''), 16));\\n\\n});\\n\\n}\\n\\n\\n\\n// capture all text\\n\\nvar textToSend = document.body.innerText;\\n\\n\\n\\n// summarize and send back\\n\\nconst api_url = 'YOUR_GOOGLE_CLOUD_FUNCTION_URL';\\n\\n\\n\\nfetch(api_url, {\\n\\nmethod: 'POST',\\n\\nbody:\", metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='\\nmethod: \\'POST\\',\\n\\nbody: JSON.stringify(textToSend),\\n\\nheaders:{\\n\\n\\'Content-Type\\': \\'application/json\\'\\n\\n} })\\n\\n.then(data => { return data.json() })\\n\\n.then(res => {\\n\\n$.each(res, function( index, value ) {\\n\\nvalue = unicodeToChar(value).replace(/\\\\\\n\\n/g, \\'\\');\\n\\ndocument.body.innerHTML = document.body.innerHTML.split(value).join(\\'<span style=\"background-color: #fff799;\">\\' + value + \\'</span>\\');\\n\\n});\\n\\n})\\n\\n.', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=\"');\\n\\n});\\n\\n})\\n\\n.catch(error => console.error('Error:', error));\\n\\nHere is where we parse the html of the page we are currently on (document.body.innerText) and, after some pre-processing with the unicodeToChar function, we send it to our Google Cloud Function via the Fetch API. You can add your own HTTP endpoint url in the api_url variable for this.\\n\\nAgain, leveraging Fetch, we return a Promise, which is the summary generated from our serverless function. Once we resolve this, we can parse the loop through the html content of our page and highlight the sentences from our summary.\\n\\nSince — it can take a\", metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=' summary.\\n\\nSince — it can take a little while for all this processing to be done, we add an alert at the top of the page which will indicate this (“Generating summary highlights. This may take up to 30 seconds depending on length of article.\").\\n\\nFinally — we need to create a manifest.json file that is needed to publish the Extension:\\n\\n{\\n\\n\"manifest_version\": 2,\\n\\n\"name\": \"SummarLight\",\\n\\n\"version\": \"0.7.5\",\\n\\n\"permissions\": [\"activeTab\", \"YOUR_GOOGLE_CLOUD_FUNCTION_URL\"],\\n\\n\"description\": \"Highlights the', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='\"],\\n\\n\"description\": \"Highlights the most important parts of posts/stories/articles!\",\\n\\n\"icons\": {\"16\": \"icon16.png\",\\n\\n\"48\": \"icon48.png\",\\n\\n\"128\": \"icon128.png\" },\\n\\n\"browser_action\": {\\n\\n\"default_icon\": \"tab-icon.png\",\\n\\n\"default_title\": \"Highlight the important stuff\",\\n\\n\"default_popup\": \"popup.html\"\\n\\n}\\n\\n}\\n\\nNotice the permissions tab. We have to add our Google Cloud Function URL here to make sure we do not get a CORS error when we call our function via the Fetch', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=' error when we call our function via the Fetch API. We also fill out details like the name/description and icons to be displayed for our Chrome Extension on the Google Store.\\n\\nAnd that’s it! We have created a Chrome Extension that leverages a serverless backbone aka Google Cloud Function. The end effect is something like this:\\n\\nA demo of SummarLight\\n\\nIt’s a simple but effective way to build really cool Apps/Extensions. Think of some of the stuff you have done in Python. Now you can just hook up your scripts to a button in an Extension/App and make a nice product out of it. All without worrying about any servers or configurations.\\n\\nHere is the', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content=' any servers or configurations.\\n\\nHere is the Github Repo: https://github.com/btahir/summarlight\\n\\nAnd you can use the Extension yourself. It is live on the Google Store here:\\n\\nPlease share your ideas for Extensions (or Apps) leveraging Google Cloud Functions in the comments. :)\\n\\nCheers.', metadata={'Title': 'Building A ‘Serverless’ Chrome Extension'}),\n",
       " Document(page_content='Member-only story How to Teach Code\\n\\nLearning to code is really hard and teaching code is even harder. There is no bootcamp in ‘How to Teach Code’. Most teachers do not have any formal training in education and come from a software background.\\n\\nI’ve had the pleasure of working with several bootcamps in different ways (as a student, teacher or experience partner). I’ve also:\\n\\nRan a code Meetup club teaching code\\n\\nDelivered a Hackathon series\\n\\nHelped out with various learning code events.\\n\\nThe Problems\\n\\nIt’s amazing how little regulation there is in teaching code. The result is students can receive wildly different experiences.', metadata={'Title': 'How to Teach Code'}),\n",
       " Document(page_content=' The result is students can receive wildly different experiences. I’ve seen some of the best education on the planet being given away for free. Yet I’ve also seen other bootcamps delivering their students 3 months of confusion and charged £9,000 for the pleasure.\\n\\nI’ve seen bootcamps struggle along until they inevitably fail whilst others just continue to grow and grow. Taking students money to deliver them a poor education is wrong but I also don’t think it was anyone’s intention to just steal from people.\\n\\nWith potential huge swathes of current jobs being taken over by software, there are more students than ever looking to transfer into a technical role.\\n\\nStories like the', metadata={'Title': 'How to Teach Code'}),\n",
       " Document(page_content=' a technical role.\\n\\nStories like the Mined Minds fiasco are just the surface of a very big problem. Rather than just talking about how big the problem is it makes sense to do something to fix it.\\n\\nSolutions\\n\\nBelow is what I’ve learnt myself about the difficulties in teaching code. How to teach code it in a way that gives the best experience for students. And how to grow a sustainable business by delivering a great experience.\\n\\n(Note — as a student on a bootcamp it is worth reading this. If your course if making some of these mistakes you might be able to help save your education by encouraging some better practices.)\\n\\nPart 1 — Why Learning…', metadata={'Title': 'How to Teach Code'}),\n",
       " Document(page_content='Reinventing Personalization For Customer Experience\\n\\n“Remember that a person’s name is, to that person, the sweetest and most important sound in any language.” — Dale Carnegie, How to Win Friends and Influence People\\n\\nWhen it comes to building good relationships with customers, learning their names is an essential step for businesses at any level. Consumers expect to be treated as individuals when it comes to the brands they do business with. Remembering a person’s name and using it whenever appropriate is key to winning that person over to your way of thinking. This fact is backed by science which says that hearing one’s own name has a powerful impact on the listener’s brain. Hence,', metadata={'Title': 'Reinventing Personalization For Customer Experience'}),\n",
       " Document(page_content=' on the listener’s brain. Hence, it is only logical to remember and use not only the customers’ names, but also their likes and dislikes to make them feel valued — in other words, providing consumers with a personalized customer experience can change them into brand loyalists in no time.\\n\\nPersonalizing the customer experience across all touch points\\n\\nThe age of the customer marked the end of the one-size-fits-all messaging era. Today, a single message can’t get the job done unless it’s perfectly tailored to relate with every customer. This is the age where consumers are more empowered than ever before and in control of their relationship with a brand. These consumers continually demand personalization throughout the', metadata={'Title': 'Reinventing Personalization For Customer Experience'}),\n",
       " Document(page_content=' brand. These consumers continually demand personalization throughout the buying journey.\\n\\nLiving in a noisy world of instant gratification, how can a business make an impression in an already overcrowded field? It’s simple actually — by making use of something known as the “cocktail party” effect. Here’s how cocktail party effect works: when you’re at a cocktail party with dozens of people chattering around you, you’ll find that you can easily blur out those conversations. To you, they’re just background noise. But, as soon as someone says your name or something that is of particular interest to you, your ears perk up and tune into that specific conversation. This information will rise above', metadata={'Title': 'Reinventing Personalization For Customer Experience'}),\n",
       " Document(page_content=' into that specific conversation. This information will rise above the noise because it is important to you.\\n\\nSimilarly, adding a personal touch to the customer experience, for instance, using dynamic recipient name tags in emails can enable businesses to get their voice heard, allowing them to stand out in an overcrowded market.\\n\\nPersonalization — A winning strategy', metadata={'Title': 'Reinventing Personalization For Customer Experience'}),\n",
       " Document(page_content='In the machine learning and deep learning paradigm, model “parameters” and “hyperparameters” are two frequently used terms where “parameters” define configuration variables that are internal to the model and whose values can be estimated from the training data and “hyperparameters” define configuration variables that are external to the model and whose values cannot be estimated from the training data ( What is the Difference Between a Parameter and a Hyperparameter? ). Thus, the hyperparameter values need to be manually assigned by the practitioner.\\n\\nEvery machine learning and deep learning model that we make has a different set of hyperparameter values that need to be fine-tuned to be able to obtain a satisfactory', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='-tuned to be able to obtain a satisfactory result. Compared to machine learning models, deep learning models tend to have a larger number of hyperparameters that need optimizing in order to get the desired predictions due to its architectural complexity over typical machine learning models.\\n\\nRepeatedly experimenting with different value combinations manually to derive the optimal hyperparameter values for each of these hyperparameters can be a very time consuming and tedious task that requires good intuition, a lot of experience, and a deep understanding of the model. Moreover, some hyperparameter values may require continuous values, which will have an undefined number of possibilities, and even if the hyperparameters require a discrete value, the number of possibilities is enormous, thus manually performing this task', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' of possibilities is enormous, thus manually performing this task is rather difficult. Having said all that, hyperparameter optimization might seem like a daunting task but thanks to several libraries that are readily available in the cyberspace, this task has become more straightforward. These libraries aid in implementing different hyperparameter optimization algorithms with less effort. A few such libraries are Scikit-Optimize, Scikit-Learn, and Hyperopt.\\n\\nThere are several hyperparameter optimization algorithms that have been employed frequently throughout the years, they are Grid Search, Random Search, and automated hyperparameter optimization methods. Grid Search and Random Search both set up a grid of hyperparameters but in Grid Search every single value combination will be exhaustively explored to find', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' single value combination will be exhaustively explored to find the hyperparameter value combination that gives the best accuracy values making this method very inefficient. On the other hand, Random Search will repeatedly select random combinations from the grid until the specified number of iterations is met and is proven to yield better results compared to the Grid Search. However, even though it manages to give a good hyperparameter combination we cannot be certain that it is, in fact, the best combination. Automated hyperparameter optimization uses different techniques like Bayesian Optimization that carries out a guided search for the best hyperparameters ( Hyperparameter Tuning using Grid and Random Search). Research has shown that Bayesian optimization can yield better hyperparameter combinations than Random Search ( Bay', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' better hyperparameter combinations than Random Search ( Bayesian Optimization for Hyperparameter Tuning).\\n\\nIn this article, we will be providing a step-by-step guide into performing a hyperparameter optimization task on a deep learning model by employing Bayesian Optimization that uses the Gaussian Process. We used the gp_minimize package provided by the Scikit-Optimize (skopt) library to perform this task. We will be performing the hyperparameter optimization on a simple stock closing price forecasting model developed using TensorFlow.\\n\\nScikit-Optimize (skopt)\\n\\nScikit-Optimize is a library that is relatively easy to use than other hyperparameter optimization libraries and also', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' use than other hyperparameter optimization libraries and also has better community support and documentation. This library implements several methods for sequential model-based optimization by reducing expensive and noisy black-box functions. For more information you can refer neptune.ai’s article where they have done a comprehensive analysis on the capabilities and usage of skopt.\\n\\nBayesian Optimization using the Gaussian Process\\n\\nBayesian optimization is one of the many functions that skopt offers. Bayesian optimization finds a posterior distribution as the function to be optimized during the parameter optimization, then uses an acquisition function (eg. Expected Improvement-EI, another function etc) to sample from that posterior to find the next set of parameters to be explored. Since', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' the next set of parameters to be explored. Since Bayesian optimization decides the next point based on more systematic approach considering the available data it is expected to yield achieve better configurations faster compared to the exhaustive parameter optimization techniques such as Grid Search and Random Search. You can read more about the Bayesian Optimizer in skopt from here.\\n\\nCode Alert!\\n\\nSo, enough with the theory, let’s get down to business!\\n\\nThis example code is done using python and TensorFlow. Furthermore, the goal of this hyperparameter optimization task is to obtain the set of hyperparameter values that can give the lowest possible Root Mean Square Error (RMSE) for our deep learning model. We hope this will be very', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' deep learning model. We hope this will be very straight forward for any first-timer.\\n\\nFirst, let us install Scikit-Optimize. You can install it using pip by executing this command.\\n\\npip install scikit-optimize\\n\\nPlease note that you will have to make some adjustments to your existing deep learning model code in order to make it work with the optimization.\\n\\nFirst, let’s do some necessary imports.\\n\\nimport skopt\\n\\nfrom skopt import gp_minimize\\n\\nfrom skopt.space import Real, Integer\\n\\nfrom skopt.utils import use_named_args\\n\\nimport tensorflow as tf\\n\\nimport numpy as np\\n', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' as tf\\n\\nimport numpy as np\\n\\nimport pandas as pd\\n\\nfrom math import sqrt\\n\\nimport atexit\\n\\nfrom time import time, strftime, localtime\\n\\nfrom datetime import timedelta\\n\\nfrom sklearn.metrics import mean_squared_error\\n\\nfrom skopt.plots import plot_convergence\\n\\nWe will now set the TensorFlow and Numpy seed as we want to get reproducible results.\\n\\nrandomState = 46\\n\\nnp.random.seed(randomState)\\n\\ntf.set_random_seed(randomState)\\n\\nShown below are some essential python global variables that we have declared. Among the variables', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\" global variables that we have declared. Among the variables, we have also declared the hyperparameters that we are hoping to optimize (the 2nd set of variables).\\n\\ninput_size=1\\n\\nfeatures = 2\\n\\ncolumn_min_max = [[0, 2000],[0,500000000]]\\n\\ncolumns = ['Close', 'Volume']\\n\\n\\n\\nnum_steps = None\\n\\nlstm_size = None\\n\\nbatch_size = None\\n\\ninit_learning_rate = None\\n\\nlearning_rate_decay = None\\n\\ninit_epoch = None\\n\\nmax_epoch = None\\n\\ndropout_rate = None\\n\\nThe “input_size�\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\" None\\n\\nThe “input_size” depicts a part of the shape of the prediction. The “features” depict the number of features in the data set and the “columns” list has the header names of the two features. The “column_min_max” variable contains the upper and lower scaling bounds of both the features (this was taken by examining validation and training splits).\\n\\nAfter declaring all these variables it’s finally time to declare the search space for each of the hyperparameters we are hoping to optimize.\\n\\nlstm_num_steps = Integer(low=2, high=14, name='lstm_num_steps')\\n\\n\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\"lstm_num_steps')\\n\\nsize = Integer(low=8, high=200, name='size')\\n\\nlstm_learning_rate_decay = Real(low=0.7, high=0.99, prior='uniform', name='lstm_learning_rate_decay')\\n\\nlstm_max_epoch = Integer(low=20, high=200, name='lstm_max_epoch')\\n\\nlstm_init_epoch = Integer(low=2, high=50, name='lstm_init_epoch')\\n\\nlstm_batch_size = Integer(low=5\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\"_batch_size = Integer(low=5, high=100, name='lstm_batch_size')\\n\\nlstm_dropout_rate = Real(low=0.1, high=0.9, prior='uniform', name='lstm_dropout_rate')\\n\\nlstm_init_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform', name='lstm_init_learning_rate')\\n\\nIf you look closely you will be able to see that we have declared the ‘lstm_init_learning_rate’ prior to log-uniform without\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='rate’ prior to log-uniform without just putting uniform. What this does is that, if you had put prior as uniform, the optimizer will have to search from 1e-4 (0.0001 ) to 1e-1 (0.1) in a uniform distribution. But when declared as log-uniform, the optimizer will search between -4 and -1, thus making the process much more efficient. This has been advised when assigning the search space for learning rate by the skopt library.\\n\\nThere are several data types using which you can define the search space. Those are Categorical, Real and Integer. When defining a search space that involves floating point values you should go for “Real', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' floating point values you should go for “Real” and if it involves integers, go for “Integer”. If your search space involves categorical values like different activation functions, then you should go for the “Categorical” type.\\n\\nWe are now going to put down the parameters that we are going to optimize in the ‘dimensions’ list. This list will be passed to the ‘gp_minimize’ function later on. You can see that we have also declared the ‘default_parameters’. These are the default parameter values we have given to each hyperparameter. Remember to type in the default values in the same order as you listed the hyperparam', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' in the same order as you listed the hyperparameters in the ‘dimensions’ list.\\n\\ndimensions = [lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,\\n\\nlstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate]\\n\\n\\n\\ndefault_parameters = [2,128,3,30,0.99,64,0.2,0.001]\\n\\nThe most important thing to remember is that the hyperparameters in the “default_parameters”', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' in the “default_parameters” list will be the starting point of your optimization task. The Bayesian Optimizer will use the default parameters that you have declared in the first iteration and depending on the result, the acquisition function will determine which point it wants to explore next.\\n\\nIt can be said that if you have run the model several times previously and found a decent set of hyperparameter values, you can put them as the default hyperparameter values and start your exploration from there. What this may do is that it will help the algorithm find the lowest RMSE value faster (fewer iterations). However, do keep in mind that this might not always be true. Also, remember to assign a value that is within', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' Also, remember to assign a value that is within the search space that you have defined when assigning the default values.\\n\\nWhat we have done up to now is setting up all the initial work for the hyperparameter optimization task. We will now focus on the implementation of our deep learning model. We will not be discussing the data pre-processing of the model development process as this article only focuses on the hyperparameter optimization task. We will include the GitHub link of the complete implementation at the end of this article.\\n\\nHowever, to give you a little bit more context, we divided our data set into three splits for training, validation, and testing. The training set was used to train the model and the validation set was used to', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' train the model and the validation set was used to do the hyperparameter optimization task. As mentioned before, we are using the Root Mean Square Error (RMSE) to evaluate the model and perform the optimization (minimize RMSE).\\n\\nThe accuracy assessed using the validation split cannot be used to evaluate the model since the selected hyperparameters minimizing the RMSE with validation split can be overfitted to the validation set during the hyperparameter optimization process. Therefore, it is standard procedure to use a test split that has not used at any point in the pipeline to measure the accuracy of the final model.\\n\\nShown below is the implementation of our deep learning model:\\n\\ndef setupRNN(inputs, model_drop', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' setupRNN(inputs, model_dropout_rate):\\n\\n\\n\\ncell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True, activation=tf.nn.tanh,use_peepholes=True)\\n\\n\\n\\nval1, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\\n\\n\\n\\nval = tf.transpose(val1, [1, 0, 2])\\n\\n\\n\\nlast = tf.gather(val, int(val.get_shape()[0]) -1, name=\"last_lstm_output\")', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' name=\"last_lstm_output\")\\n\\n\\n\\ndropout = tf.layers.dropout(last, rate=model_dropout_rate, training=True,seed=46)\\n\\n\\n\\nweight = tf.Variable(tf.truncated_normal([lstm_size, input_size]))\\n\\nbias = tf.Variable(tf.constant(0.1, shape=[input_size]))\\n\\n\\n\\nprediction = tf.matmul(dropout, weight) +bias\\n\\n\\n\\nreturn prediction\\n\\nThe “setupRNN” function contains our deep learning model. Still, you may not want to understand those details, as', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' you may not want to understand those details, as Bayesian optimization considers that function as a black-box that takes certain hyperparameters as the inputs and then outputs the prediction. So if you are not interested in understanding what we have inside that function, you may skip the next paragraph.\\n\\nOur deep learning model contains an LSTM layer, a dropout layer and an output layer. The necessary information required for the model to work needs to be sent to this function (in our case, it was the input and the dropout rate). You can then proceed with implementing your deep learning model inside this function. In our case, we used an LSTM layer to identify the temporal dependencies of our stock data-set.\\n\\n', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' dependencies of our stock data-set.\\n\\nWe then fed the last output of the LSTM to the dropout layer for regularization purposes and obtained the prediction through the output layer. Finally, remember to return this prediction (in a classification task this can be your logit) to the function that will be passed to the Bayesian Optimization ( “setupRNN” will be called by this function).\\n\\nIf you are performing a hyperparameter optimization for a machine learning algorithm (using a library like Scikit-Learn) you will not need a separate function to implement your model as the model itself is already given by the library and you will only be writing code to train and obtain predictions. Therefore, this code', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' to train and obtain predictions. Therefore, this code can go inside the function that will be returned to the Bayesian Optimization.\\n\\nWe have now come to the most important section of the hyperparameter optimization task, the ‘fitness’ function.\\n\\n@use_named_args(dimensions=dimensions)\\n\\ndef fitness(lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,\\n\\nlstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate):\\n\\n\\n\\nglobal iteration, num_', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='rate):\\n\\n\\n\\nglobal iteration, num_steps, lstm_size, init_epoch, max_epoch, learning_rate_decay, dropout_rate, init_learning_rate, batch_size\\n\\n\\n\\nnum_steps = np.int32(lstm_num_steps)\\n\\nlstm_size = np.int32(size)\\n\\nbatch_size = np.int32(lstm_batch_size)\\n\\nlearning_rate_decay = np.float32(lstm_learning_rate_decay)\\n\\ninit_epoch = np.int32(lstm_init_epoch)\\n', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='lstm_init_epoch)\\n\\nmax_epoch = np.int32(lstm_max_epoch)\\n\\ndropout_rate = np.float32(lstm_dropout_rate)\\n\\ninit_learning_rate = np.float32(lstm_init_learning_rate)\\n\\n\\n\\ntf.reset_default_graph()\\n\\ntf.set_random_seed(randomState)\\n\\nsess = tf.Session()\\n\\n\\n\\ntrain_X, train_y, val_X, val_y, nonescaled_val_y = pre_process()\\n\\n\\n\\ninputs = tf.placeholder(tf', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='\\ninputs = tf.placeholder(tf.float32, [None, num_steps, features], name=\"inputs\")\\n\\ntargets = tf.placeholder(tf.float32, [None, input_size], name=\"targets\")\\n\\nmodel_learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\\n\\nmodel_dropout_rate = tf.placeholder_with_default(0.0, shape=())\\n\\nglobal_step = tf.Variable(0, trainable=False)\\n\\n\\n\\nprediction = setupRNN(inputs,model_dropout_rate)\\n\\n\\n\\nmodel_', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\"dropout_rate)\\n\\n\\n\\nmodel_learning_rate = tf.train.exponential_decay(learning_rate=model_learning_rate, global_step=global_step, decay_rate=learning_rate_decay,\\n\\ndecay_steps=init_epoch, staircase=False)\\n\\n\\n\\nwith tf.name_scope('loss'):\\n\\nmodel_loss = tf.losses.mean_squared_error(targets, prediction)\\n\\n\\n\\nwith tf.name_scope('adam_optimizer'):\\n\\ntrain_step = tf.train.AdamOptimizer(model_learning_rate).minimize(model_loss\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='learning_rate).minimize(model_loss,global_step=global_step)\\n\\n\\n\\nsess.run(tf.global_variables_initializer())\\n\\n\\n\\nfor epoch_step in range(max_epoch):\\n\\n\\n\\nfor batch_X, batch_y in generate_batches(train_X, train_y, batch_size):\\n\\ntrain_data_feed = {\\n\\ninputs: batch_X,\\n\\ntargets: batch_y,\\n\\nmodel_learning_rate: init_learning_rate,\\n\\nmodel_dropout_rate: dropout_rate\\n\\n}\\n\\nsess.run(train_step,', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='\\nsess.run(train_step, train_data_feed)\\n\\n\\n\\nval_data_feed = {\\n\\ninputs: val_X,\\n\\n}\\n\\nvali_pred = sess.run(prediction, val_data_feed)\\n\\n\\n\\nvali_pred_vals = rescle(vali_pred)\\n\\n\\n\\nvali_pred_vals = np.array(vali_pred_vals)\\n\\n\\n\\nvali_pred_vals = vali_pred_vals.flatten()\\n\\n\\n\\nvali_pred_vals = vali_pred_vals.tolist()\\n\\n\\n\\nvali_nonescaled_', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='\\n\\nvali_nonescaled_y = nonescaled_val_y.flatten()\\n\\n\\n\\nvali_nonescaled_y = vali_nonescaled_y.tolist()\\n\\n\\n\\nval_error = sqrt(mean_squared_error(vali_nonescaled_y, vali_pred_vals))\\n\\n\\n\\nreturn val_error\\n\\nAs shown above, we are passing the hyperparameter values to a function named “fitness.” The “fitness” function will be passed to the Bayesian hyperparameter optimization process ( gp_minimize). Note that in the first iteration, the', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='imize). Note that in the first iteration, the values passed to this function will be the default values that you defined and from there onward Bayesian Optimization will choose the hyperparameter values on its own. We then assign the chosen values to the python global variables we declared at the beginning so that we will be able to use the latest chosen hyperparameter values outside the fitness function.\\n\\nWe then come to a rather critical point in our optimization task. If you have used TensorFlow prior to this article, you would know that TensorFlow operates by creating a computational graph for any kind of deep learning model that you make.\\n\\nDuring the hyperparameter optimization process, in each iteration, we will be resetting the existing', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' each iteration, we will be resetting the existing graph and constructing a new one. This process is done to minimize the memory taken for the graph and prevent the graphs from stacking on top of each other. Immediately after resetting the graph you will have to set the TensorFlow random seed in order to obtain reproducible results. After the above process, we can finally declare the TensorFlow session.\\n\\nAfter this point, you can start adding code responsible for training and validating your deep learning model as you normally would. This section is not really related to the optimization process but the code after this point will start utilizing the hyperparameter values chosen by the Bayesian Optimization.\\n\\nThe main point to remember here is to return', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='\\nThe main point to remember here is to return the final metric value (in this case the RMSE value) obtained for the validation split. This value will be returned to the Bayesian Optimization process and will be used when deciding the next set of hyperparameters that it wants to explore.\\n\\nNote: if you are dealing with a classification problem you would want to put your accuracy as a negative value (eg. -96) because, even though the higher the accuracy the better the model, the Bayesian function will keep trying to reduce the value as it is designed to find the hyperparameter values for the lowest value that is returned to it.\\n\\nLet us now put down the execution point for this whole process, the', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=\" down the execution point for this whole process, the “main” function. Inside the main function, we have declared the “gp_minimize” function. We are then passing several essential parameters to this function.\\n\\nif __name__ == '__main__':\\n\\n\\n\\nstart = time()\\n\\n\\n\\nsearch_result = gp_minimize(func=fitness,\\n\\ndimensions=dimensions,\\n\\nacq_func='EI', # Expected Improvement.\\n\\nn_calls=11,\\n\\nx0=default_parameters,\\n\\nrandom_state=46)\\n\\n\\n\\nprint(search_result.x)\\n\\nprint(search_\", metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='result.x)\\n\\nprint(search_result.fun)\\n\\nplot = plot_convergence(search_result,yscale=\"log\")\\n\\n\\n\\natexit.register(endlog)\\n\\nlogger(\"Start Program\")\\n\\nThe “func” parameter is the function you would want to finally model using the Bayesian Optimizer. The “dimensions” parameter is the set of hyperparameters that you are hoping to optimize and the “acq_func” stands for the acquisition function and is the function that helps to decide the next set of hyperparameter values that should be used. There are 4 types of acquisition functions supported by gp_minimize. They are', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' functions supported by gp_minimize. They are:\\n\\nLCB: lower confidence bound\\n\\nEI: expected improvement\\n\\nPI: probability of improvement\\n\\ngp_hedge: probabilistically choose one of the above three acquisition functions at every iteration\\n\\nThe above information was extracted from the documentation. Each of these has its own advantages but if you are a beginner to Bayesian Optimization, try using “EI” or “gp_hedge” as “EI” is the most widely used acquisition function and “gp_hedge” will choose one of the above-stated acquisition functions probabilistically thus, you wouldn’t have to worry too much about', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' wouldn’t have to worry too much about that.\\n\\nKeep in mind that when using different acquisition functions there might be other parameters that you might want to change that affects your chosen acquisition function. Please refer the parameter list in the documentation for this.\\n\\nBack to explaining the rest of the parameters, the “n_calls” parameter is the number of times you would want to run the fitness function. The optimization task will start by using the hyperparameter values defined by “x0”, the default hyperparameter values. Finally, we are setting the random state of the hyperparameter optimizer as we need reproducible results.\\n\\nNow when you run the gp_optimize function the', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' when you run the gp_optimize function the flow of events will be:\\n\\nThe fitness function will use with the parameters passed to x0. The LSTM will be trained with the specified epochs and the validation input will be run to get the RMSE value for its predictions. Then depending on that value, the Bayesian optimizer will decide what the next set of hyperparameter values it wants to explore with the help of the acquisition function.\\n\\nIn the 2nd iteration, the fitness function will run with the hyperparameter values that the Bayesian optimization has derived and the same process will repeat until it has iterated “n_call” times. When the complete process comes to an end, the', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=' When the complete process comes to an end, the Scikit-Optimize object will get assigned to the “search _result” variable.\\n\\nWe can use this object to retrieve useful information as stated in the documentation.\\n\\nx [list]: location of the minimum.\\n\\nfun [float]: function value at the minimum.\\n\\nmodels: surrogate models used for each iteration.\\n\\nx_iters [list of lists]: location of function evaluation for each iteration.\\n\\nfunc_vals [array]: function value for each iteration.\\n\\nspace [Space]: the optimization space.\\n\\nspecs [dict]`: the call specifications.\\n\\nrng [RandomState instance]: State of the random', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='ng [RandomState instance]: State of the random state at the end of minimization.\\n\\nThe “search_result.x” gives us optimal hyperparameter values and using “search_result.fun” we can obtain the RMSE value of the validation set corresponding to the obtained hyperparameter values (The lowest RMSE value obtained for the validation set).\\n\\nShown below are the optimal hyperparameter values that we obtained for our model and the lowest RMSE value of the validation set. If you are finding it hard to figure out the order in which the hyperparameter values are being listed when using “search_result.x”, it is in the same order as you specified', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=', it is in the same order as you specified your hyperparameters in the “dimensions” list.\\n\\nHyperparameter Values:\\n\\nlstm_num_steps: 6\\n\\nlstm_size: 171\\n\\nlstm_init_epoch: 3\\n\\nlstm_max_epoch: 58\\n\\nlstm_learning_rate_decay: 0.7518394019565194\\n\\nlstm_batch_size: 24\\n\\nlstm_dropout_rate: 0.21830825193089087\\n\\nlstm_init_learning_rate: 0.00064013635678135', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content=': 0.0006401363567813549\\n\\nLowest RMSE: 2.73755355221523\\n\\nConvergence Graph\\n\\nThe hyperparameters that produced the lowest point of the Bayesian Optimization in this graph is what we get as the optimal set of hyperparameter values.\\n\\nThe graph shows a comparison of the lowest RMSE values recorded for each iteration (50 iterations) in Bayesian Optimization and Random Search. We can see that the Bayesian Optimization has been able to converge rather better than the Random Search. However, in the beginning, we can see that Random search has found a better minimum faster than the Bayesian Optimizer. This can be due to random sampling being', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='izer. This can be due to random sampling being the nature of Random Search.\\n\\nWe have finally come to the end of this article, so to conclude, we hope this article made your deep learning model building task easier by showing you a better way of finding the optimal set of hyperparameters. Here’s to no more stressing over hyperparameter optimization. Happy coding, fellow geeks!\\n\\nUseful Materials:', metadata={'Title': 'How to Automate Hyperparameter Optimization'}),\n",
       " Document(page_content='Photo by William Iven on Unsplash\\n\\nI recently spent four days at a research lab with a group of data scientists and a few behavior scientists diving into a large, messy data set to see if we might find insights related to group dynamics. It was framed as an initial dive to look around, get an initial assessment of potential for useful research insights, and then build a proposal for two to three months of work based on what was found that week.\\n\\nI attended theoretically in the role of a behavioral scientist, but since I’m also obsessed with creative problem solving processes, or design thinking (and I’m opinionated as hell), the following are some reflections on the process we used and ideas for what’', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' the process we used and ideas for what’s next.\\n\\nA lot of process methodologies occupy the space in the overlap between software development, data analytics, and creative problem solving disciplines: UX, design thinking, Think Wrong, Google Design Sprint, Agile, Kanban, emerging data analysis methods for group sprints, etc. It’s great, because if you’re curious you can acquire a pretty big toolset to bring together groups of people to crash and get creative on all different kinds of problems for widely varying lengths in time — a two hour meeting to an 18 month project. The challenge, or mastery of practice as a facilitator, is to learn not just the buzz words of the methodologies or the', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' just the buzz words of the methodologies or the exercises themselves, but the underlying science behind group dynamics, creativity, psychology, and neuroscience.\\n\\nDuring this workshop, we had a two distinct types of professionals in the room — social science and data science, each with complementary skillsets within those groupings. They were all brand new to the data set, and it was pretty messy when we first looked at it. The direction from the sprint’s sponsor was extremely broad — what general insights about the group dynamics of these co-workers can be identified in a three day sprint.\\n\\nUnfortunately, we didn’t clearly identify the direction for what kind of insights would be most useful until the afternoon of the second day. At that', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' until the afternoon of the second day. At that point, the social science group separated to brainstorm a range of different questions that could be asked about group dynamics, and which theories and research questions were emerging as especially intriguing for the field. Besides ensuring the whole team had a clear idea of the direction for the workshop and what question themes regarding group dynamics would be most useful (ie. performance of small work teams, changes in group composition over time, looking for patterns in the kinds of people who grouped together, etc) we also needed design constraints, but didn’t identify them. Intro design thinking uses a generic Venn diagram to illustrate the criteria a design must successfully meet in order to be green lit for funding/ effort to develop a high-', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' lit for funding/ effort to develop a high-fidelity solution. Companies develop their own specific criteria that usually fall somewhere into these buckets. Without identifying our own, the team relied on the expertise and gut instinct of the people gathered in the room. Not bad — everyone was incredibly smart and knew their field well; but not great if you want to maximize resources toward understanding the most important/ impactful questions.\\n\\nvia Legacy Innova\\n\\nOur facilitator took the route of using UX and design thinking exercises — personas and needs statements — over the first two days. They were meant to get the group to identify specific questions that could then be voted on by the whole team. On the third day, small groups broke off, each taking', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' third day, small groups broke off, each taking one or two questions that might be asked of the data. Normally personas and needs statements are an incredibly reliable method; they work in all kinds of situations. But during this sprint many people found them to be frustrating when they had been told to look for insights about group dynamics, not individual personas.\\n\\nThe first challenge with persona methods is that they are designed to be used with deliberative modes of thinking, or System 2 (from Thinking Fast and Slow) that take a diverse, seemingly disconnected array of information — most often qualitative ethnography — and synthesize it to come to some detailed statement of need. With no prior set of information about the people in the data set, we were', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' about the people in the data set, we were left looking at spreadsheets of raw data and making up cartoon characters as personas. The whole team went along gamely and tried to make use of it, but the time could have been better spent.\\n\\nThe second challenge regarding the selection of persona methods is that they are meant to be used during design processes where the outcome is some product or service serving archetypal individuals. There is a relationship between problem for the persona, and solution — solving a need of the persona. For this workshop, our team was not meant to produce a product or service, the insights were the deliverables. Rather than personas, we should have been exploring and mapping the space of the complex concept of group', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' and mapping the space of the complex concept of group dynamics.\\n\\nThe somewhat awkward use of some design thinking/ UX methods and rejection of others for this workshop was totally understandable. As design sprints have become a popular, justifiably so, tool for businesses to bring multi-disciplinary teams together and develop novel concepts and products, the broad space of design and software development methodologies have edged into unfamiliar territory — data analytics. While I feel experienced to comment on design methodologies, data science is new territory for me. From the literature review I’ve done, and the limited commercial experience with analytics teams, it seems that they are themselves wrestling with what methodologies serve group data sprints best. Analysis has traditionally been done solo, or', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' best. Analysis has traditionally been done solo, or sitting side by side, not requiring a more formal group process. Three forces are pushing data scientists to develop group analytics methodologies: the data science field has bifurcated with an ever increasing number of specializations and tools, the sheer amount of data has exponentially increased, and data science is spreading to disciplines that had previously done their analysis with almost ubiquitous qualitative methods and tools — so multi-disciplinary teams were becoming the norm. In the last five to seven years, there have been several academic articles and blogs describing group data analysis processes.\\n\\nMy PhD research involves developing a co-design process to analyze a complex concept — wellbeing, using 3D data visualization software. I’ve', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' 3D data visualization software. I’ve been thinking about how we bring groups together to investigate and communicate about complex concepts, the very early work of group and individual need finding in order to eventually do policy, service, and product design. Trying to grasp complex systems and complex concepts is a totally different beast than even synthesizing nuanced psychological and social needs from personas. Oral cultures have very different approaches to thinking about and discussing complexity, and I believe our western dominated, literate, tech cultures have a lot to learn from them about how to analyze complexity.\\n\\nI would like to offer these ideas for multi-disciplinary groups working to identify insights in large, messy data sets. I’m curious what others are doing, and would', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content='�m curious what others are doing, and would love some feedback.\\n\\nFirst, here is the framework I use to organize design sprints organized around addressing particular needs of particular archetypal personas. Four quadrants correspond to the four phases usually associated with design thinking: interviewing and observing users, synthesis of needs, ideation, and prototyping and testing. I find that it helps me as a facilitator to understand the modes of thinking I’m trying to induce my participants to exercise for different methods. It’s also helpful to pair the mode of thinking with a clear delineation between problem and solution. Trying to brainstorm about both the problem and solution at the same time is a recipe for a chaotic descent of the group', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' is a recipe for a chaotic descent of the group way, way off track. This 2x2 framework is most useful when designing exercises for workshops that revolve around understanding a user’s need and developing some thing to solve it — a software product interface, a household object, a social service.\\n\\nComplexity, ie group dynamics, requires a different framework for thinking about which methods to select and which modes of thinking to use during a sprint. In our sprint, the work revolved around getting to an insight, refining a raw data set and teasing apart a complex topic. Using a 2x2 matrix again, complex concept and raw data set replace problem and solution on one of the axes. For the second axis, we might', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' the axes. For the second axis, we might use different flare and focus modes than ones that serve thinking about defined needs of individual archetypes, two examples would be inductive/ deductive and declarative/ modal. Dr. Mihnea Moldoveanu of Rottman Business School, led a decade long research effort to understand the modes of thinking, or adaptive intelligence, that transcends discipline boundaries. His admonishment is essentially to consciously select the mode of thinking, or pattern of thinking modes, best fit for the problem and group of people at hand.\\n\\nA hypothetical set of methods for another data sprint might go: 1) map the scales, layers, and theories related to the complex concept in question, 2) down', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' to the complex concept in question, 2) down select to the most important set of themes or meta-questions to ask of the data set, 3) exploratory analysis of the raw data set, 4) develop a schema and begin cleaning the data and organizing it into an easy to access data frame, 5) break up into small groups to each tackle a theme or meta-question and use alternating flare and focus modes of thinking like inductive/ deductive and declarative/ modal to iteratively analyze the data, develop hypothetical insights, and refine them with more data analysis.\\n\\nThis space of methodologies — especially sprint methodologies — to ask complex questions of big data sets using multidisciplinary teams is really exciting. So', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content=' using multidisciplinary teams is really exciting. So many of the most interesting questions facing our policy-makers, scientists, and researchers sit on huge piles of data at the intersection of fields that traditionally haven’t had to find a common language to work together. Facilitators, data scientists, and design practitioners might develop new theories on how best best to organize teams to more quickly and efficiently divine insights from large, chaotic data sets. Inevitably, someone will try to come along and commoditize it with pretty graphic icons and simplified descriptions. But right now it’s a fun, collaborative space of community exploration as we figure out how to bring more disciplines and professions together with data scientists and raw data.', metadata={'Title': 'Ideas: Design Methodologies for Data Sprints'}),\n",
       " Document(page_content='RoboSomm\\n\\nWine Embeddings and a Wine Recommender\\n\\nOne of the cornerstones of previous chapters of the RoboSomm series has been to extract descriptors from professional wine reviews, and to convert these into quantitative features. In this article, we will explore a way of extracting features from wine reviews that combines the best of the existing RoboSomm series and academic literature on this topic. We will then use these features to produce a simple wine recommendation engine.\\n\\nThe Jupyter Notebook with all relevant code can be found in this Github repository. Our dataset consists of roughly 180,000 professional wine reviews, scraped from www.winemag.com. These reviews span roughly 20 years, dozens of', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='. These reviews span roughly 20 years, dozens of countries and hundreds of grape varieties.\\n\\nWine Embeddings\\n\\nIn the following section, we walk through the five steps required to create our ‘wine embeddings’: a 300-dimensional vector for each wine, summarizing its sensory profile. On the way, we will explain successful approaches others have taken in similar projects. Before we proceed, let’s pick a wine to join us on this journey:\\n\\nPoint & Line 2016 John Sebastiano Vineyard Reserve Pinot Noir\\n\\nReview: Dried red flowers and sagebrush combine for an elegant aromatic entry to this bottling by two business partners who have worked in Santa Barbara’s restaurant scene', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' have worked in Santa Barbara’s restaurant scene for many years. Tarragon and intriguing peppercorn flavors decorate the tangy cranberry palate, which is lightly bodied but very well structured.\\n\\nExcellent! Time to get stuck in.\\n\\nStep 1: Normalize words in wine review (remove stopwords, punctuation, stemming)\\n\\nThe first step is to normalize our text. We want to remove stopwords and any punctuation from our raw text. In addition, we will use a stemmer (Snowball Stemmer in Sci-Kit Learn) to reduce inflected words to their stem. The Pinot review becomes the following:\\n\\ndri red flower sagebrush combin eleg aromat', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='dri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa barbara restaur scene mani year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur\\n\\nStep 2: Enhance the set of normalized words with phrases (bi-grams and tri-grams)\\n\\nNext, we want to account for the possibility that some of the terms we want to extract from the wine descriptions are actually combinations of words or phrases. Here, we can use the gensim package Phrases to produce a set of bi- and tri-grams for the full corpus. Running our normalized wine review through the phraser consolidates', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' our normalized wine review through the phraser consolidates terms such as ‘light’ and ‘bodi’ which are frequently found next to each other to ‘light_bodi’:\\n\\ndri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur\\n\\nStep 3: Use the RoboSomm wine wheels to standardize the wine descriptors in each review\\n\\nWine reviewers are often creative in their use of language, and sometimes use different words to describe things that are seemingly the same.', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' words to describe things that are seemingly the same. After all, are ‘wet slate’, ‘wet stone’ and ‘wet cement’ aromas not really manifestations of the same sensory experience? In addition, wine tasting has specific jargon. Terms such as ‘baked’, ‘hot’ or ‘polished’ have a specific meaning in the world of wine tasting.\\n\\nTo standardize wine jargon and creative descriptors, researchers such as Bernard Chen have developed the Computational Wine Wheel. The Computational Wine Wheel categorizes and maps various wine terms that appear in wine reviews to create a consolidated set of descriptors. This great work, together with the contributions of', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='. This great work, together with the contributions of others (e.g. Wine Folly and UC Davis) has been used to generate the RoboSomm wine wheels. These wine wheels were created by looking at a list of the most frequently occurring descriptors in the corpus after going through steps 1 and 2 outlined above. This list was then reviewed manually, and mapped onto a set of standardized descriptors. In total, this resulted in a mapping for over 1,000 ‘raw’ descriptors.\\n\\nThe first of the RoboSomm wine wheels is an aroma wheel, that categorizes a variety of aromatic descriptors:\\n\\nWine Aroma Wheel\\n\\nThe second wine wheel is a non-aroma wheel,', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' wine wheel is a non-aroma wheel, that accounts for other characteristics, such as body, sweetness and acid levels. These descriptors are not typically included in tasting wheels, but are prominent parts of a tasting experience:\\n\\nWine Non-Aroma Wheel\\n\\nWe can choose to standardize wine terms at any of the three levels of the wheel, or use the raw descriptor itself (no standardization). For now, we will map the descriptors to the outside layer of the wheel. For the Pinot Noir review we started processing, we obtain the following:\\n\\ndry red flower sagebrush combin elegant aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarr', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='barbara restaur scene mani_year tarragon intrigu pepper flavor decor tangy cranberry palat light_bodied veri well structur\\n\\nNote that all the descriptors that have been mapped are highlighted in bold. The other terms are either non-informative or ambiguous in the context of this analysis.\\n\\nStep 4: Retrieve the Word2Vec word embedding for each mapped term in the review\\n\\nNext, we need to consider how we will quantify our set of mapped descriptors. A common approach to doing this (and one that was used in previous chapters of the RoboSomm series!) is to represent the absence/presence of each descriptor in the corpus with a 0 or a 1.', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' in the corpus with a 0 or a 1. However, this approach does not take into account semantic (dis)similarities between terms. Tarragon, for instance, is more similar to sagebrush than it is to cranberry. To account for this, we can create word embeddings: vector representations of words and phrases. Researchers such as Els Lefever and her co-authors have taken a similar approach to quantifying wine reviews in their work.\\n\\nFor the purpose of this project, we will use a technique called Word2Vec to generate a 300-dimensional embedding for every mapped term. Since wine jargon is so specific, we have to train our Word2Vec model on a representative corpus. Fortunately', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='2Vec model on a representative corpus. Fortunately, our set of 180,000 wine reviews is exactly that! Having previously mapped our descriptors using our wine wheels, we have already somewhat standardized the wine terms in our corpus. This was done to eliminate unnecessary semantic nuance (e.g. consolidate ‘wet stone’, ‘wet slate’ and ‘wet cement’ to ‘wet rock’), hopefully enhancing the quality of our Word2Vec model.\\n\\nOur trained Word2Vec model consists of a 300-dimensional embedding for every term in our corpus. However, we can recall from the previous step in this analysis that we only really care about the terms that are', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' that we only really care about the terms that are relevant descriptors of a wine’s sensory experience.\\n\\nFor our Pinot Noir, these were:\\n\\ndry, flower, sagebrush, elegant, tarragon, pepper, tangy, cranberry, light_bodied\\n\\nIn the adjacent image, we can see the word embedding for each of these mapped descriptors.\\n\\nStep 5: Weight each word embedding in the wine review with a TF-IDF weighting, and sum the word embeddings together\\n\\nNow that we have a word embedding for each mapped descriptor, we need to think about how we can combine these into a single vector. Looking at our Pinot Noir example,', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' vector. Looking at our Pinot Noir example, ‘dry’ is a fairly common descriptor across all wine reviews. We want to weight that less than a rarer, more distinctive descriptor such as ‘sagebrush’. In addition, we want to take into consideration the total number of descriptors per review. If there are 20 descriptors in one review and five in another, each individual descriptor in the former review probably contributes less to the overall profile of the wine than in the latter. Term Frequency-Inverse Document Frequency (TF-IDF) takes both of these factors into consideration. TF-IDF looks at how many mapped descriptors are contained within a single review (TF), as well as at how often', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content=' review (TF), as well as at how often each mapped descriptor appears in the 180,000 wine reviews (IDF).\\n\\nMultiplying each mapped descriptor vector by its TF-IDF weighting gives us our set of weighted mapped descriptor vectors. We can then sum these to obtain a single wine embedding for each wine review. For our Pinot Noir, this looks something like:', metadata={'Title': 'Wine Embeddings and a Wine Recommender'}),\n",
       " Document(page_content='20 must-know Data Science Interview Questions\\n\\nA non-exhaustive(duh) list of some of the good data science questions I have come across. I hope this list is of use to someone wanting to brush up some basic concepts. Kudos to the authors of all the amazing posts mentioned here.\\n\\nQ. Define mean, mode, median. Explain these concepts to a layman. When is either preferred over the other. Give practical examples.', metadata={'Title': '20 must-know Data Science Interview Questions'}),\n",
       " Document(page_content='Learning rate scheduler\\n\\nESPNetv2 introduces a variant of the cosine learning rate, wherein the learning rate is decayed linearly until cycle length and then restarted. At each epoch t, the learning rate ηₜ is computed as:\\n\\nFigure 1: Cyclic learning rate policy with linear learning rate decay and warm restarts\\n\\nF aster training\\n\\nWith the learning rate scheduler described above, we train ShuffleNetv2 for a total of 120 epochs with a batch size of 512 across four TitanX GPUs using SGD with momentum at two different FLOP settings: (1) 41 MFLOPs and (2) 146 MFLOPs. For the first 60 epoch', metadata={'Title': 'Faster Training for Efficient CNNs'}),\n",
       " Document(page_content=' MFLOPs. For the first 60 epochs, we set ηₘᵢₙ, ηₘₐₓ, and T to 0.1, 0.5, and 5, respectively. For the remaining 60 epochs, we set ηₘₐₓ=0.1, T=60, and ηₘᵢₙ=ηₘₐₓ/T. The figure below visualizes the learning policy for 120 epochs.\\n\\nFigure 2: Cyclic learning rate scheduler with warm restarts for faster training proposed in the ESPNetv2 paper.\\n\\n', metadata={'Title': 'Faster Training for Efficient CNNs'}),\n",
       " Document(page_content=' in the ESPNetv2 paper.\\n\\nResults on the ImageNet dataset\\n\\nResults are given in Figure 3 and Table 1. We can clearly see that the learning rate scheduler (discussed above) enables faster training while delivering similar performance as the linear learning rate scheduler in ShuffleNet paper.', metadata={'Title': 'Faster Training for Efficient CNNs'}),\n",
       " Document(page_content='Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.\\n\\nOpinion spamming is a situation that is aggravating, for instance, CBS News reports that 52% of product reviews posted in Walmart.com are “inauthentic or unreliable”, while at least 30% of reviews posted at Amazon are fake. The problem of identifying opinion spamming remains an open topic, despite the fact that several researchers have addressed it already.\\n\\nWhat makes businesses to incur in product deceptive reviews? The main driver is getting ahead of the competition for positioning their product or service to influence the public and organizations to make a purchase, therefore increasing their sales. The fraud takes place by', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' therefore increasing their sales. The fraud takes place by posting fake negative reviews and giving unjust ratings to products from the competition.\\n\\nIt is known that “Amazon Mechanical Turk”, an internet crowdsourcing marketplace that allows requesters (businesses or individuals) to coordinate human labor to carry out a task, was employed to crowdsource fake reviews for a hotel chain. Given that this problem has grown to alarming rates, Yelp.com, a business directory service that publishes crowd-sourced reviews about businesses, launched a sting operation in order to unmask those businesses who buy fake reviews.\\n\\nI will discuss the method that Mukherjee et al. present in their paper for detecting spam in product reviews. They called their model “', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' in product reviews. They called their model “Author Spamicity Model” (ASM). It is based on unsupervised learning which models spamicity as latent, shortly meaning that the model variables are “hidden”. It is also a Bayesian inference framework. The aim of the model is to cluster the categorization of this latent population distributions into spammers and not spammers.\\n\\nPlease note that when I refer to products, I will be including also services.\\n\\nHow can we identify that a review may be fake? In order to develop their model, the authors define nine variables as observed features, the first five they categorize them as author features that have values in the interval [0, 1]', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' have values in the interval [0, 1] (denoting a Beta distribution), where a value close to 0 or 1 denotes non-spamming or spamming, respectively. In the other hand, variables 5 to 9, represent review features, and they assume a binary value, 0 for non-spamming, and 1 for spamming (denoting a Bernoulli distribution):\\n\\nContent Similarity (CS). Spammers are prone to copy reviews for comparable products. Cosine similarity is used to capture content similarity in these reviews. Maximum Number of Reviews (MNR). The unusual behavior of posting several reviews in one day by the same author, can be a sign of spamming. Reviewing Burstiness (BST', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' spamming. Reviewing Burstiness (BST). Refers to the frequency (short) to which an author posts a review. This author is usually a new member of the site. Meeting this condition may signify a tendency to incur in deceptive reviews. Ratio of First Reviewers (RFR). This metric quantifies the fact that early reviews have an impact on sales of newly launched products. Duplicate/Near Duplicate Reviews (DUP). Identical or quasi identical reviews may indicate a spamming behavior. This feature is similar to CS, but in this case pertains to the review features. Extreme Rating (EXT). In order to deliver the most or the least benefit to a review, spammers usually mark the product with either one or', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' spammers usually mark the product with either one or five stars. Rating Deviation (DEV). Spammers will try to divert the average sentiment on reviews, by placing theirs. These types of reviews are identified when this quantified deviation exceeds a threshold. Early Time Frame (ETF). This feature captures how early the review was made. The rationale is, spammers are most likely to review earlier, close to the launch of the product to achieve the greatest impact. Rating Abuse (RA). Refers to the action of star-qualifying the same product multiple times.\\n\\nHow does ASM work? In order to illustrate the model, I have simplified its functioning in the following schema (See Figures 1-A and 1-B), for a', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' 1-A and 1-B), for a mathematical representation please refer to the paper.\\n\\nFigure 1-A\\n\\nFigure 1-B\\n\\nASM commences by taking in all the reviews by all the authors, where these reviews are organized by the features that we have discussed. Each sphere represents an observable variable (i.e. feature). Once the features are collected (See Figure 1-A node A) they are processed by the model and learn the “latent behavior distributions for spam and not spam” (Murkherjee et al.). Therefore, ASM solves a clustering problem (K = 2).\\n\\nThe spamicity is modeled as latent as ASM functions in the Bayesian context.', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' as ASM functions in the Bayesian context. It is a generative process because it emits the nine features with their probability of spamming.\\n\\nIn order to perform an inference, the model uses “Collapsed Gibbs Sampling” (CGS) that represents a technique for approximating the posterior probability distribution. CGS belongs to the family of algorithms of Markov Chain Montecarlo.\\n\\nOnce the ranking functions have been inferred, they are processed using the Learning to Rank supervised technique, that basically takes the rankings obtained by ASM and generates a single aggregated ranking function (see Figure 1-A node C).\\n\\nIn my opinion, this paper presents a technique that can improve significantly the detection of opinion spammers', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=' that can improve significantly the detection of opinion spammers in product reviews. It is innovative because presents an unsupervised method for detecting fake reviews. The authors claim having achieved a superior level of accuracy compared to strong competitors. I believe that opinion spamming will start to decrease as more businesses providing this type of information start to implement ML techniques like ASM, meanwhile consumers must be skeptical and get informed using sites that filter fake reviews.\\n\\nA Mukherjee, A Kumar, B Liu, J Wang, M Hsu… — Proceedings of the 19th …, 2013 — dl.acm.org\\n\\nP Resnik, E Hardisty — 2010\\n\\nGEP Box, GC Tiao — 2011 — books.google', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content=', GC Tiao — 2011 — books.google.com\\n\\nby JA López — \\u200e2017\\n\\n— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -\\n\\nDear reader, I am interested to know from you:\\n\\nWhen you buy online, do you feel influenced to decide by the local review that is presented? Or in addition to, you look for one or more external reviews? What is the products review site that you trust more, if you use one? What do you think may be a solution to this problem that is growing at alarming rates?\\n\\nThanks for participating, you can leave a comment to', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content='Thanks for participating, you can leave a comment to respond.', metadata={'Title': 'Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.'}),\n",
       " Document(page_content='Edgar Yau: It’s been about three, four years since I’ve actually been a part of these communities. Now I’m more of an observer, but I’ve stayed really up-to-date with the controversies and the outrage that goes on in them. In middle school I played video games and it’s the way I connected with my friends. It’s an interesting subculture for a boy to get all caught up in. The gamerspeak is very distinctive, games are very irreverent, they just kind of say whatever and if you get offended, you’re offended and everyone makes fun of you for it.\\n\\nIt’s not a first person', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='\\n\\nIt’s not a first person shooter thing, it’s just a gamer thing. I wasn’t primarily playing first person shooters at that time, I was playing a lot of Minecraft with my friends and going on all these random servers. Even then it was like, who can build the biggest swastika? It was this whole idea of, I’m not getting offended, so if other people get offended they’re stupid, they’re vulnerable, they’re idiotic. What right do they have to control the way that I speak? The concept of freedom of speech is huge in these communities.\\n\\nI started researching this because I was playing Call of Duty with my friend one', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' I was playing Call of Duty with my friend one day online, and within two minutes I heard a prepubescent voice say the n-word. I was catapulted into my adolescent years.\\n\\nAndrew Thompson: Angela Nagle talked about this in Kill All Normies where she discusses the idea of extending cultural capital into subcultural capital. The distinction between groups in this case is based on one’s knowledge of these very esoteric references and memes and language, like gamerspeak, and I would say also on your ability to withstand insults. If you bristle, you’re sort of being filtered out of the subculture.\\n\\nEY: The instant some feminist critique of games comes out, it feels like the enemy outside is', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' comes out, it feels like the enemy outside is coming into my space and telling me that I shouldn’t like something. There are only two options: you either double down on your opinions, or you kind of acknowledge that maybe they’re right. I don’t want to say that I was too young-I was around 16-but I was probably just too certain of what I thought and how I felt that I didn’t consider just for a second that those criticisms might not be totally baseless. The interesting thing is that I’m not from here, I grew up in Hong Kong and I only came to the States for college, but I was still extremely involved in this stuff.\\n\\nAT:', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' extremely involved in this stuff.\\n\\nAT: I really think of people in these subcultures expressing this fear about threats against “the West”. To hear that you are actually having these experiences in Hong Kong I wouldn’t have expected.\\n\\nEY: I went to an international school so their first language was English. It was a very Western atmosphere. Out of all my friends, I was the deepest into this stuff. I was bringing a lot of these ideas to them, especially my guy friends, and they would latch onto it. Then as I’d talk about it around my female friends they’d freak out and I’d feel vindicated. Because like, Oh, I triggered someone', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='icated. Because like, Oh, I triggered someone. I have the power to deliver this uncomfortable truth them and make them emotionally react.\\n\\nI can’t speak to how similar or different it was from an American high school experience, but it was certainly a very Western experience. It’s embarrassing to admit, but a lot of the guys would just say the n-word and we would get reprimanded for it. All of these moral norms in the US are the same over there. Obviously we’re consuming American pop culture, we’re watching American TV shows, we’re listening to American music. There was no connection of that word for me to race, it was just the idea that I get', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' race, it was just the idea that I get to use it and I can just kind of say that and trigger a reaction in people. And that’s kind of a global feeling.\\n\\nAT: I was talking to somebody recently who told me he was really into this stuff for a time during his adolescence. This is somebody who if you met him you would not guess in a million years that any of this materiel ever appealed to any part of him, and it did. I’m almost 33 now-this subculture as I witnessed it in my own adolescence was more or less the same. The platforms were different, I don’t think 4chan existed when I was in high school, but the texture of', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' I was in high school, but the texture of it was all pretty much identical in the sense of using racial slurs, maximizing offense, and being jarring in the worst possible way. I didn’t really participate in that, but my teenage rebellion certainly had a digital form. This was when we had AOL Instant Messenger. I would just find people and troll them. I was just like a little shitposter. The line dividing my own form of trolling and more outwardly hateful activity is razor thin. Between you and talking to this other person and thinking about me, and thinking about other people I know from high school, I’ve realized that this transgressive online behavior is not this fringe practice that a few troubled youth drift into,', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' fringe practice that a few troubled youth drift into, this is pretty pervasive among males of a certain age to the extent that I would almost say you can assume that if you see a young male who isn’t exceedingly attractive, he is probably involved in this in some way, or is adjacent to it. It’s just the culture they default to. What the media has done is framed it as this marginal subculture when in fact, it’s actually this mainstream culture that just hasn’t been identified as such until very recently.\\n\\nEY: I think that’s spot on and that’s something that I’ve been looking into a lot, how it’s actually the norm.', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=', how it’s actually the norm. Often the media doesn’t treat the internet with the respect that it needs to. There’s no discussion of it with an actual understanding that these are real people and this is a serious phenomenon. I remember on CNN they were like, “Who is this hacker 4chan?” It reminds me of the kinds of things that I was thinking, because a lot of this is kind of a rejection of the mainstream media, mainstream ideals, partially to be edgy, but also partially because you don’t feel like it reflects or represents you.\\n\\nAT: There’s just more people engaging on /pol today than there were just a few years ago.', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' today than there were just a few years ago. So now it’s even more mainstreamed than it was and has developed a critical mass.\\n\\nEY: There’s almost like no meaningful, actual alternative to these monolithic places of discussion. If you go on Reddit, for example, there is still a voting system where the most popular ideas go to the top and the most controversial ideas [within a given subreddit] go away. And so if you want to express controversial ideas that don’t necessarily line up exactly with the way everyone around you is supposed to think, there’s no way to do that.\\n\\nThe way that 4chan is laid out, it feels like you’re forced to', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=', it feels like you’re forced to engage with as many ideas as are thrown at you. There are conservative Reddits, but then those don’t really get as far as 4chan. And you don’t really get exposed to as many ideas that have the potential for that thrill of revelation. When you’re first starting out engaging in political ideas, you get those feelings of revelation over and over again. And it’s an interesting feeling, it feels like you’re learning. And so once you find something like Reddit, once you get a good grasp on say, if you go on r/gaming, like if you’re on the default gaming subreddit, it’s like', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' the default gaming subreddit, it’s like okay, I get what these people are saying now. This is boring to me. What’s next? Oh, let’s find more and more specific, niche things. A platform like Reddit, is, I think, eventually not enough for someone who’s looking for that kind of discussion. There’s almost no alternative but 4chan to go to consistently have those provocative feelings. You don’t really see that on Reddit once you go through it.\\n\\nI think that that is what that’s indicative of. Not that I’m saying that people are using 4chan as a substitute for Reddit. I just think that that is a', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' for Reddit. I just think that that is a general trend, where 4chan is interesting because it’s constantly presenting you with new ideas.\\n\\nAT: You bring up a good point which is that it’s popular because it’s not one of these big platforms. It isn’t owned. I don’t even know who runs it. Who maintains the 4chan servers? I don’t know who the people are behind it. I think that there’s an appeal to that.\\n\\nI’ve also been considering what Reddit is. Is it good, is it bad, what’s good, what’s bad, and so on. Reddit has basically', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='s bad, and so on. Reddit has basically subsumed all message boards at this point. You don’t go to message boards anymore. You go on different kinds of social media, and Reddit is the closest form of social media we have to a message board. But it doesn’t mimic message boards a lot. 4chan feels more like an old forum to me, it’s more Web 1.0 than something like Reddit is. I think that there’s no other places off these platforms to go, really. What are the other non-alt-right message boards that aren’t owned by venture capitalists?\\n\\nWherever you spend your time, the more time you spend there, the more', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=', the more time you spend there, the more you just absorb the ideas you’re surrounded by. I find myself absorbing ideas even when I’m approaching them from a distance. Even when researching things, I can feel them trying to work on me in someway.\\n\\nEY: I use the word “Chad” in conversation sometimes. The language seeps into your brain, and you start to think of things in that same framework. It’s really ridiculous.\\n\\nAT: The YouTuber ContraPoints did that incels video, and she actually said that just in the course of researching her video, she started using the words like Chad and Stacy. I think the reason she did that', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' and Stacy. I think the reason she did that, and the reason you do that, and the reason I do it, is because the word Chad is funny and is a great word for a man of bulletproof fuckability.\\n\\nIt’s also indicative of more pernicious and more destructive ideas that can seep in just by sheer proximity. The way that there’s research indicating that if you force yourself to smile, you feel happier. Who knows, it’s pop psychology bullshit, I don’t know if it’s real or not. But I do think it’s real in the sort of digital-political realm, where just by touching these ideas, you are susceptible to them', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' by touching these ideas, you are susceptible to them in some way.\\n\\nThis is data from r/Braincels and r/TheRedPill. These are the most popular subreddits for both people. The one that really stuck out to me in the r/Braincels results was the subreddit NEET, which stands for “Not in Employment, Education or Training”, and I have kind of a long thought I want to launch into for a minute.\\n\\nThis overlap between the sexual and economic outcasts is I think brought up really well in Nagle’s book. She’s talking about this guy Roger Devlin in this passage:\\n\\nHis essay “Sexual Utopia and Power�', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='His essay “Sexual Utopia and Power” argues against “today’s sexual dystopia with its loose morals and confused sexual roles. It explores “ female hypergamy”, mating up, narcissism, infidelity, deceptiveness, and masochism.” It also argues that “the breakdown of monogamy results in promiscuity for the few, loneliness for the majority.” On this last point, I think he’s getting to the central issue driving this kind of reactionary sexual politics, perhaps even the central personal motivation behind the entire turn to the far right among young men. The sexual revolution that started the decline of lifelong marriage has produced great freedom from the shackles of', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' marriage has produced great freedom from the shackles of loveless marriage and selfless duty to the family for both men and women, but this ever-extended adolescence has also brought with it the rise of adult childlessness and a steep sexual hierarchy. Sexual patterns that have emerged as a result of the decline of monogamy have seen a greater level of sexual toys for an elite of men and a growing celibacy among the large male population at the bottom of the pecking order. Their own anxiety and anger about their lower ranking status in this hierarchy is precisely what it’s produced that are hard line rhetoric about asserting hierarchy in the world politically when it comes to women and non-whites. The pain of relentless rejection has festered', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='ites. The pain of relentless rejection has festered in these forums and allowed them to be the masters of the cruel natural hierarchies that bring them so much humiliation.\\n\\nFrom Dataclysm by Christian Rudder\\n\\nI thought of two things when I read this passage. The first was a graph from this book Dataclysm by Christian Rudder, who founded OkCupid. Rudder ultimately published this book that drew on a lot of different datasets but also used the OkCupid dataset. This is one of the analyses that was published in Dataclysm. OkCupid doesn’t work like this anymore as I understand, but for a long time when you came across another user, you would rate them from one to five stars. It', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' would rate them from one to five stars. It’s probably good we don’t do that anymore. But that was how the platform operated. And I remember that if you were in a certain echelon of people, if you were in the top 10 percent or whatever-and I wasn’t-you would get this email that congratulated you on being the creme de la creme of attractiveness.\\n\\nAccording to the data on OkCupid, women find men less attractive than men find women. And I think you can synthesize what Nagle is saying with this graph and just see this swelling discontent on incels.co and Braincels and TheRedPill as essentially the casualties of this new', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='RedPill as essentially the casualties of this new sexual system we’ve been developing since the 1970s where relations are reduced to this sexual marketplace.\\n\\nThe second thing I thought of was basically the entire worldview of Michel Houellebecq, whose ideas I see throughout Kill All Normies. His first big book has written a lot about how the sexual revolution, despite being associated with the left, was really the beginning of a cultural neoliberalism. Just as institutions under neoliberalism have dissolved and been taken over by market forces, our social relations are unmediated by anything other than sheer desire and you end up with nothing but a bazaar of flesh. One result of that is that you end up with a sexual underclass, the sexual', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' end up with a sexual underclass, the sexual counterpart to the NEET subreddit. Just as people are cast away from the job market through automation and are no longer able to adapt to the world necessary to survive economically, it’s more difficult for people to survive in this new sexual reality, especially one that provides a method of meeting people on digital platforms that optimize for quick, weak interactions that give little opportunity for anything other than physicality to dominate the selection process.\\n\\nI think the bearing that reality has on people differs on geography. I don’t know how much Tinder is used in Laramie, Wyoming, where I used to live, but I can tell you that it is the dominant method by which people meet each', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' it is the dominant method by which people meet each other in places like New York City. And I have watched people move here and thrive or die in that system. I have seen people go in completely opposite directions. I can think of two people off the top of my head where this happened. One of them was a very attractive girl who moved to Brooklyn from Philadelphia, and I think Philadelphia is less dictated by this ruthless superficiality than New York, which makes a lot of sense when you think about New York as a neoliberal center, both economically and in this cultural sense we’re talking about. She moved here and became this #bestself member of this culture’s sexual elite who saw her own sexual activity as basically the extent of', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' saw her own sexual activity as basically the extent of her politics.\\n\\nThe other friend also moved here from Philadelphia and I guess he split with his girlfriend or something, and I found these Red Pill leanings creeping into his personality. He believed women’s brains were hackable. He would call women ugly to me. It was really weird. And I would tell him not to do it and he would laugh and find it funny. To him, that was his own brand of 4chan offensiveness, he was upsetting these feminist sensibilities of the left. And what was so fucking bizarre was that he was kind of a leftist. He worked as an environmental organizer for years and lived in Brooklyn and had a bookshelf of Chuck Kl', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' Brooklyn and had a bookshelf of Chuck Klosterman and whatever you would find on a Brooklyn bookshelf. But I think the longer he went without sex, the more hateful he became. He actually referred to himself as celibate. He would say, “Well I’m celibate now.” He never said incel. I had never heard that term actually until 2018, but he was the first person I ever knew who referred to himself as celibate.\\n\\nMy friendships with both these people reached their conclusions for these very reasons, which sucks because in New York City friendships don’t abound. But it was bizarre to see both of them sort of convert to this worldview.\\n\\n', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' them sort of convert to this worldview.\\n\\nEY: It seems like that worldview is being forced upon her, not her reinforcing the assumptions of that ideology.\\n\\nAT: Absolutely, and that that’s my whole point. Even if incels engage in toxic behavior, they exist in a macro historical moment in which that behavior is cultivated. That absolutely goes for her, and it goes from my other friend as well. It goes for all of us, not to completely dismiss any kind of agency. And I think that this is an environment that I’m always relieved when people acknowledge, whether it be Houellebecq or Nagle.\\n\\nEY: But then I think that things like the Amazon documentary The Red', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' I think that things like the Amazon documentary The Red Pill and places like Rolling Stone and all of them almost never investigate this, the moment that we find ourselves in that is creating this kind of hyper-sexual frustration.\\n\\nAT: And I will say as well, so not only have things been replaced by this sexual marketplace but we are daily inundated with images establishing what sort of conventional existence looks like. There’s a fascinating paper written by Jonah Peretti, who founded Buzzfeed, wrote when he was at MIT. It’s one of the most interesting papers I think I’ve ever read. His whole idea was that in this current stage of capitalism, we cycle through identities more quickly.\\n\\nLacan had', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' through identities more quickly.\\n\\nLacan had this idea in his psychoanalysis of the Mirror Stage where the child develops a sense of itself when it looks at the mirror and it recognizes itself. But this phenomenon is extended past the mirror and into images. So we don’t just identify with the reflection of ourselves in the mirror. We then identify with the image on the movie screen or the TV screen or phone screen or whatever sort of picture is looking back at us. And that sort of becomes our sense of self as well.\\n\\nBecause we are so inundated with these images and these narratives, we find ways to reconcile our own sense of identity with this identity that is projected back at us from the screen. You can think of', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' at us from the screen. You can think of a child after they watch The Avengers, they go and they play Avengers with their friends and they think of themselves as Captain America and Black Widow and whatever. That’s kind of a very rudimentary, basic example of this idea of the child taking on the identify of the screen.\\n\\nBut we do this all the time as adults. So extrapolate that behavior to an adult looking at an image in a Chanel advertisement or watching The Social Network, and they see in the mirror a model or a machinating startup CEO. The adult takes on the identity of what is projected at them by the image and then reconciles their own sense of self with what they see projected at them.\\n', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' self with what they see projected at them.\\n\\nWith people like incels you have something of an identity crisis. Because of this new sexual marketplace, they are no longer able to close the gap between their own identities and the identities projected back at them in the image. And because of the endless white noise of media, they are drowning in reminders of their very inability to close that gap.\\n\\nSomething the activist-turned-red-piller/black-piller I mentioned observed that I think is very true is that we don’t really have an outlet for men who don’t have sex or can’t have sex for whatever reason. There’s no monastery for them to join. We have', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='s no monastery for them to join. We have created this system where you are either getting laid or you’re not. Those are your two options. And if you can’t reconcile your identity with that of the image, which is almost universally an image of someone having lots of sex, we don’t really give you any other options. There’s no image of, like, a monk.\\n\\nI sent you that Oneohtrix Point Never video, I don’t know if you watched it or not.\\n\\nEY: Yes, I watched it. I watched the shit out of it.\\n\\nAT: To me it captures everything about what we’re saying in the span', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' about what we’re saying in the span of four minutes, for the most part. Mostly what it captures is that alienation and ostracization and totally unfulfillable desire among these people. That there’s just no way to satiate these urges, and the urges become ever more extreme the more time you spend feeding them and dwelling within them. Nagle’s quote about the unsatiated desire being the fundament of everything is captured in that OPN video better than anything I’ve seen.\\n\\nEY: Yeah, I think so. And something interesting that stood out to me about that video is that there are no “real women” in that video, it’s all furries', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' that video, it’s all furries or hentai. It makes me wonder whether the phenomenon of incels loving hentai is almost self-fulfilling-they pine after these girls and they have these waifus and they love these things just because part of them knows that this isn’t an obtainable, real thing. It’s another form of comfort and safety for them. Like, “I can fall in love with a fictional character, and some part of me knows that will never happen, so committing to that is safe.”\\n\\nAT: I also think that it’s a sign of just how alien an actual woman is to this culture. It is so', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' actual woman is to this culture. It is so distant and abstract that it becomes unreal. In a way, the furries and the hentai girls are both literal depictions of what these people end up desiring, but also these non literal representations of what women are to them.\\n\\nI looked up “red pill” and I looked up “black pill” on /pol. People say “red pill” more, but it’s interesting to see the climb of “black pill” relative to “red pill” and I’m wondering in your research if you’ve gotten the sense that the use of term is becoming more pervasive. Do you think that it indicates', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' becoming more pervasive. Do you think that it indicates a deeping nihilism among these people, or is this just kind of a coincidence?\\n\\nEY: To me it just feels like “red pill” is becoming a normie concept now. It’s mainstream. So they’re kind of getting backed into a corner and I don’t know where you can go past the black pill. There’s no darker black pill.\\n\\nAT: The jet black pill.\\n\\nEY: Seeing this makes me feel like, Okay there is this kind of … “ I’m trying to reconcile this rejection of the mainstream with all the other things you’re talking to me about.', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' things you’re talking to me about. Like because it is … because the mainstream idea is the sexual marketplace, right? And then there’s the belief that the sexual marketplace can’t benefit you. And so I feel like those ideas kind of intertwined like the wires get crossed or something. Where once an idea starts to become part of a more mainstream cultural consciousness it becomes almost, yuck, you’re deterred from it.\\n\\nIt reminds that I was talking to this guy who went viral talking about his alt-right journey and one word that he kept using I actually had never used before was “boomer”. He was saying like, “Oh they’re just a bunch', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' “Oh they’re just a bunch of Boomers.” “That’s just Boomer talk.” And so that really had me thinking about the way all of this stuff is situated in their minds. It all can come back to that enemy outside thing. And so once the Red Pill ideas are hitting mainstream and if … let’s say Chads start building in TheRedPill, which they can do, the Red Pill isn’t an exclusively incel thing, then where do you go. You have to start moving into this space where it’s like, Okay this ideology is now not somewhere that I feel safe. I have to take it further. And that just gets', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' have to take it further. And that just gets encouraged by everyone else who feels the same way and feels like you have to be edgy. You have to be on the edge, you have to be on the fringe.\\n\\nAT: I feel like that pertains to this other result here. I looked at all three-word combinations used in any /pol post with the words “red pill” and the one that stuck out to me was any terms that include the word “ultimate”. This idea of the ultimate red pill, and how commonly that factors into these discussions, this idea of like taking the final red pill. Nick Land’s decision to title his manifesto the “Dark Enlightenment” bears', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' his manifesto the “Dark Enlightenment” bears on this as well, I think, which implies a forbidden knowledge.\\n\\nIt makes me think of something Mark Fisher said in this essay he wrote about Joy Division. “The depressive is always confident of one thing: that he is without illusions.” There’s this idea that the more nihilistic the idea is, the truer is must be. And I wonder if that becomes its own reinforcement to people who traffic in the language of the red pill and the black pill and the dark enlightenment. That because it’s so nihilistic, it is this form of realism.\\n\\nEY: It reminds me of how incels will go and they’ll', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content=' incels will go and they’ll post pictures of themselves to be rated on, fully expecting and knowing for a fact that all they’re going to get is criticism and reaffirmation that their worldview, that they’re never going to be able to fuck, is true. It’s almost comforting in the most depressing sense: Everything I’m thinking is right, I am worthless, I am terrible, any hope is illusion.', metadata={'Title': 'Objects of Desire: A conversation with Edgar Yau'}),\n",
       " Document(page_content='What is a single view of the customer?\\n\\nA single customer view is a centralized location that consolidates all the knowable data and information of your customers within your organization and enables you to view, find, and understand every aspect of your customers. Having a single view of customers within your environment helps you understand what and how your customers purchase. These intelligence servers as fuel to your sales and marketing team’s formulation and development of strategy.\\n\\nInsight into the difficulties?\\n\\nExtraction, consolidation, and integration are what you need on the face of it. The implementation and the solution of it are not straightforward at all, though. Customer interaction with your services or/and products can be long, stretching across multiple online and', metadata={'Title': 'How data management practice enables the successful implementation of a single customer view?'}),\n",
       " Document(page_content=' products can be long, stretching across multiple online and offline channels and touchpoints. Managing, governing, consolidating, and transforming valuable data and information with a tremendous variety across different channels and touchpoints is very much challenging.\\n\\nData management practice helps address the challenges\\n\\nReviewing the end-to-end process and mechanics of building a single customer view reveals what elements of data management practice help solve the challenges.\\n\\nStep 1 Extraction: extraction of all the knowable data of your customers from different systems by similar column names, by documentation from business analysts, and by knowledge from system analysts. But\\n\\nA. what knowable customer data do you have in your organization?\\n\\nB. how do you know which system(', metadata={'Title': 'How data management practice enables the successful implementation of a single customer view?'}),\n",
       " Document(page_content='\\nB. how do you know which system(s) to extract the customer data that you search for?\\n\\nElements: Data Dictionary, Enterprise Data Flow, Data Lineage\\n\\nStep 2 Data Cleansing: removal of all the noises of your customer data by rules provided by IT. But', metadata={'Title': 'How data management practice enables the successful implementation of a single customer view?'}),\n",
       " Document(page_content='Cython will give your Python code super-car speed\\n\\nWant to be inspired? Come join my Super Quotes newsletter. 😎\\n\\nPython is a community favourite programming language! It’s by far one of the easiest to use as code is written in an intuitive, human-readable way.\\n\\nYet you’ll often hear the same complaint about Python over and over again, especially from the C code gurus out there: Python is slow.\\n\\nAnd they’re not wrong.\\n\\nRelative to many other programming languages, Python is slow. Benchmark game has some solid benchmarks for comparing the speed of various programming languages on different tasks.\\n\\nThere’s a couple of different ways', metadata={'Title': 'Use Cython to get more than 30X speedup on your Python code'}),\n",
       " Document(page_content='\\nThere’s a couple of different ways to speed things up that I’ve written about before:\\n\\n(1) Use multi-processing libraries to use all the CPU cores\\n\\n(2) If you’re using Numpy, Pandas, or Scikit-Learn, use Rapids to accelerate the processing on GPU.\\n\\nThose are great if what you’re doing can, in fact, be parallelized, such as data pre-processing or matrix operations.\\n\\nBut what if your code is pure Python? What if you have a big for-loop that you just have to use and can’t put into a matrix because the data has to be processed in sequence? Is there a', metadata={'Title': 'Use Cython to get more than 30X speedup on your Python code'}),\n",
       " Document(page_content=' has to be processed in sequence? Is there a way to speedup Python itself?\\n\\nThat’s where Cython comes in to speed up our raw Python code.\\n\\nWhat is Cython?\\n\\nAt its core, Cython is an intermediate step between Python and C/C++. It allows you to write pure Python code with some minor modifications, which is then translated directly into C code.\\n\\nThe only adjustment you make to your Python code is adding type information to every variable. Normally, we might declare a variable in Python like this:\\n\\nx = 0.5\\n\\nWith Cython, we’re going to add a type to that variable:\\n\\ncdef float x = 0', metadata={'Title': 'Use Cython to get more than 30X speedup on your Python code'}),\n",
       " Document(page_content=' variable:\\n\\ncdef float x = 0.5\\n\\nThis tells Cython that our variable is floating point, just like we would do in C. With pure Python, the variable’s type is determined on the fly. The explicit declaration of the type in Cython is what makes the conversion to C possible, since explicit type declarations are required+.', metadata={'Title': 'Use Cython to get more than 30X speedup on your Python code'}),\n",
       " Document(page_content='Caveat: As of this writing, I’ve used the following database-like systems in a production environment: MySQL, PostgreSQL, Hive, MapReduce on Hadoop, AWS Redshift, GCP BigQuery, in various mixes of on-prem/hybrid/cloud setups. My optimization knowledge largely stems from those. I’ll stick to strategies/thinking process here, but there are definitely features and quirks in other popular databases that I’m not familiar with, especially from SQL Server and Oracle.\\n\\nThis article is about speed, common strategies for making things go FASTER while avoiding specific implementation details. I’m trying to express the thought process of optimization, not the specific mechanics', metadata={'Title': 'Learning SQL 201: Optimizing Queries, Regardless of Platform'}),\n",
       " Document(page_content=' the thought process of optimization, not the specific mechanics. Before I knew it, it’s turned into a monster of an article. There’s a lot to cover!\\n\\nIntro and Background\\n\\nOptimizing queries is a hard topic to write about because it involves specifics. Specifics about database engines, software, and sometimes even hardware and network architecture. I’ve been asked to write about this topic multiple times, and I’ve always resisted because I couldn’t see a way to write a generally useful article for something that very quickly gets into the weeds.\\n\\nThere are entire books written about how to optimize different database systems, which includes queries, but also details in the tuning of the', metadata={'Title': 'Learning SQL 201: Optimizing Queries, Regardless of Platform'}),\n",
       " Document(page_content=' queries, but also details in the tuning of the systems themselves. They’re always written about a specific platform, not in general. It’s for good reason — every platform is different and the tuning parameters you need depend on your workload and setup (write heavy vs read heavy, SSDs vs Spinning disk, etc).\\n\\nBut on the way home during a nasty heatwave, I had an sudden flash of insight as to what threads tie optimization together. So I’m giving this crazy article a try. I’m going to avoid too many specifics and focus on the core thinking process that goes into identifying the things that will make your queries go faster. There will be forays into specifics only for illustrative', metadata={'Title': 'Learning SQL 201: Optimizing Queries, Regardless of Platform'}),\n",
       " Document(page_content=' will be forays into specifics only for illustrative purposes, and no real code examples. Also for brevity, I can’t be super thorough, but I’ll link to examples and further reading as I go.', metadata={'Title': 'Learning SQL 201: Optimizing Queries, Regardless of Platform'}),\n",
       " Document(page_content='Predicting vs. Explaining\\n\\nA directed acyclic graph depicting the causal pathways to foetal alcohol spectrum disorders\\n\\nThe Cultural War in Cognitive Science\\n\\nI recently stumbled across this really juicy debate on natural language processing that took place a few years ago between the old guard of the field, Noam Chomsky, who’s considered as “the father of modern linguistics,” and the new guard, Peter Norvig, Director of Research at Google. When commenting on where the field was heading, Chomsky said the following:\\n\\n“Suppose that somebody says he wants to eliminate the physics department and do it the right way. The “right” way is to take endless numbers', metadata={'Title': 'Predicting vs. Explaining'}),\n",
       " Document(page_content='�right” way is to take endless numbers of videotapes of what’s happening outside the video, and feed them into the biggest and fastest computer, gigabytes of data, and do complex statistical analysis — you know, Bayesian this and that — and you’ll get some kind of prediction about what’s gonna happen outside the window next. In fact, you get a much better prediction than the physics department will ever give. Well, if success is defined as getting a fair approximation to a mass of chaotic unanalyzed data, then it’s way better to do it this way than to do it the way the physicists do, you know, no thought experiments about frictionless planes and so on and so', metadata={'Title': 'Predicting vs. Explaining'}),\n",
       " Document(page_content=' experiments about frictionless planes and so on and so forth. But you won’t get the kind of understanding that the sciences have always been aimed at — what you’ll get at is an approximation to what’s happening.”\\n\\nChomsky reinforced that sentiment repeatedly elsewhere: that the current definition of success in natural language processing — namely predictive accuracy — is not science. Throwing “some immense corpus of text” into a “complicated machine” is merely “approximating unanalyzed data,” or “butterfly collecting,” that would not lead to “real understanding” of the language. He argues that the main goal of science is to �', metadata={'Title': 'Predicting vs. Explaining'}),\n",
       " Document(page_content=' argues that the main goal of science is to “discover explanatory principles” of how a system actually works, and the “right approach” to achieve that goal is to “let the theory guide the data”: study the system’s basic nature by abstracting away “irrelevant intrusions” through carefully designed experiments — the same way modern science has been conducted since Galileo. In his own succinct words: “Just trying to deal with the unanalyzed chaotic data is unlikely to get you anywhere, just like as it wouldn’t have gotten Galileo…', metadata={'Title': 'Predicting vs. Explaining'}),\n",
       " Document(page_content='After several failed ML projects due to unexpected ML degradation, I wanted to share my experience in ML models degradation. Indeed, there is a lot of hype around model creation and development phase, as opposed to model maintenance.\\n\\nAssuming that a Machine Learning solution will work perfectly without maintenance once in production is a faulty assumption and represents the most common mistake of companies taking their first artificial intelligence (AI) products to market.\\n\\nThe moment you put a model in production, it starts degrading.\\n\\nWhy Do ML Models Degrade With Time?\\n\\nAs you may already know, data is the most crucial component of a successful ML system. Having a relevant data set that provides you with accurate predictions is a great start, but how long will that', metadata={'Title': 'Why Machine Learning Models Degrade In Production'}),\n",
       " Document(page_content=' is a great start, but how long will that data continue to provide accurate predictions?\\n\\nIn all ML projects, it is key to predict how your data is going to change over time. In some projects, we underestimated this step and it became hard to deliver high accuracy. In my opinion, as soon as you feel confident with your project after the PoC stage, a plan should be put in place for keeping your models updated.\\n\\nIndeed, your model’s accuracy will be at its best until you start using it. This phenomenon is called concept drift, and while it’s been heavily studied in academia for the past two decades, it’s still often ignored in industry best practices.\\n\\nConcept drift', metadata={'Title': 'Why Machine Learning Models Degrade In Production'}),\n",
       " Document(page_content=' in industry best practices.\\n\\nConcept drift: means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.\\n\\nThe key is that, in contrast to a calculator, your ML system does interact with the real world. If you’re using ML to predict demand and pricing for your store, you’d better consider this week’s weather, the calendar and what your competitors are doing.', metadata={'Title': 'Why Machine Learning Models Degrade In Production'}),\n",
       " Document(page_content='Recurrent Neural Network\\n\\nThe Recurrent neural networks are a class of artificial neural networks where the connection between nodes form a directed graph along a temporal sequence. Unlike the feed-forward neural networks, the recurrent neural networks use their internal state memory for processing sequences. This dynamic behavior of the Recurrent neural networks allows them to be very useful and applicable to audio analysis, handwritten recognition, and several such applications.\\n\\nSimple RNN implementation in Keras.\\n\\nMathematically the simple RNN can be formulated as follows:\\n\\nWhere x(t) and y(t) are the input and output vectors, Wᵢₕ, Wₕₕ, and Wₕₒ are the', metadata={'Title': 'Implementation of RNN, LSTM, and GRU'}),\n",
       " Document(page_content=' and Wₕₒ are the weight matrices and fₕ and fₒ are the hidden and output unit activation functions.\\n\\nThe implementation of RNN with 2 Simple RNN layers each with 32 RNN cells followed by time distribute dense layers for 10 class classification can be illustrated as follows:', metadata={'Title': 'Implementation of RNN, LSTM, and GRU'}),\n",
       " Document(page_content='Need for dimensionality reduction\\n\\nIn machine learning projects we often run into curse of dimensionality problem where the number of records of data are not a substantial factor of the number of features. This often leads to a problems since it means training a lot of parameters using a scarce data set, which can easily lead to overfitting and poor generalization. High dimensionality also means very large training times. So, dimensionality reduction techniques are commonly used to address these issues. It is often true that despite residing in high dimensional space, feature space has a low dimensional structure.\\n\\nTwo very common ways of reducing the dimensionality of the feature space are PCA and auto-encoders. I will only provide brief introduction to these, for a', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' will only provide brief introduction to these, for a more theoretically oriented comparison read this post.\\n\\nPCA\\n\\nPCA essentially learns a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. By restricting the dimensionality to a certain number of components that account for most of the variance of the data set, we can achieve dimensionality reduction.\\n\\nAutoencoders\\n\\nAutoencoders are neural networks that can be used to reduce the data into a low dimensional latent space by stacking multiple non-linear transformations(layers). They have a encoder-decoder architecture. The encoder maps the input to latent space and decoder reconstructs the input. They are', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' and decoder reconstructs the input. They are trained using back propagation for accurate reconstruction of the input. In the latent space has lower dimensions than the input, autoencoders can be used for dimensionality reduction. By intuition, these low dimensional latent variables should encode most important features of the input since they are capable of reconstructing it.\\n\\nComparison\\n\\nPCA is essentially a linear transformation but Auto-encoders are capable of modelling complex non linear functions. PCA features are totally linearly uncorrelated with each other since features are projections onto the orthogonal basis. But autoencoded features might have correlations since they are just trained for accurate reconstruction. PCA is faster and computationally cheaper than autoencod', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' is faster and computationally cheaper than autoencoders. A single layered autoencoder with a linear activation function is very similar to PCA. Autoencoder is prone to overfitting due to high number of parameters. (though regularization and careful design can avoid this)\\n\\nWhen to use which?\\n\\nApart from the consideration about computational resources, the choice of technique depends on the properties of feature space itself. If the features have non-linear relationship with each other than autoencoder will be able to compress the information better into low dimensional latent space leveraging its capability to model complex non-linear functions. What does it mean for the features to have non-linear relationships? Let us do a couple of simple experiments', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' relationships? Let us do a couple of simple experiments to answer these questions and shed some light on comparative usefulness of both techniques.\\n\\nExperiments 2D\\n\\nHere we construct two dimensional feature spaces (x and y being two features) with linear and non-linear relationship between them (with some added noise). We will compare the capability of autoenocoders and PCA to accurately reconstruct the input after projecting it into latent space. PCA is a linear transformation with a well defined inverse transform and decoder output from autoencoder gives us the reconstructed input. We use 1 dimensional latent space for both PCA and autoencoders.\\n\\nIt is evident if there is a non linear relationship (or curvature) in', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' a non linear relationship (or curvature) in the feature space, autoencoded latent space can be used for more accurate reconstruction. Where as PCA only retains the projection onto the first principal component and any information perpendicular to it is lost. Let us look at the reconstruction cost as measured by mean squared error (MSE) in the table below.\\n\\nExperiments 3D\\n\\nConducting similar experiments in 3D. We create two three dimensional feature spaces. One is a 2D plane existing in 3D space and the other is a curved surface in 3D space.\\n\\nWe can see that in case of a plane there is a clearly two dimensional structure to the data and PCA with two components can account for 100', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content=' and PCA with two components can account for 100% of the variance of the data and can thus achieve perfect reconstruction. In case of a curved surface two dimensional PCA is not able to account for all the variance and thus loses information. The projection to the plain that covers the most of variance is retained and other information is lost, thus reconstruction is not that accurate. On the other hand autoencoder is able to reconstruct both plane and surface accurately using two dimensional latent space. So 2D latent space is able to encode more information in case of autoencoder because it is capable of non-linear modelling. Reconstruction cost is provided in the table below.\\n\\nRandom Data Experiment\\n\\nHere we create a random data without any coll', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content='\\nHere we create a random data without any collinearity. All features are independently sampled from a uniform distribution and have no relationship with each other. We use two dimensional latent space fro both PCA and Autoencoder.\\n\\nWe see that PCA is able to retain the projection onto the plane with maximum variance, and loses a lot of information because the random data did not have a underlying 2 dimensional structure. Autoencoder also does poorly since there was no underlying relationship between features.\\n\\nConclusion\\n\\nFor dimensionality reduction to be effective, there needs to be underlying low dimensional structure in the feature space. I.e the features should have some relationship with each other.\\n\\nIf there is non-linearity', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content='.\\n\\nIf there is non-linearity or curvature in low dim structure than autoencoders can encode more information using less dimensions. So they are a better dimensionality reduction technique in these scenarios.\\n\\nAll code for the experiments can be found here:', metadata={'Title': 'Autoencoders vs PCA: when to use ?'}),\n",
       " Document(page_content='An Introduction to Recurrent Neural Networks for Beginners\\n\\nA simple walkthrough of what RNNs are, how they work, and how to build one from scratch in Python. Victor Zhou · Follow Published in Towards Data Science · 10 min read · Jul 25, 2019 -- 2 Share\\n\\nRecurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They’re often used in Natural Language Processing (NLP) tasks because of their effectiveness in handling text. In this post, we’ll explore what RNNs are, understand how they work, and build a real one from scratch (using only numpy) in Python.\\n\\nThis post assumes a basic knowledge of neural', metadata={'Title': 'An Introduction to Recurrent Neural Networks for Beginners'}),\n",
       " Document(page_content='\\n\\nThis post assumes a basic knowledge of neural networks. My introduction to Neural Networks covers everything you’ll need to know, so I’d recommend reading that first.\\n\\nLet’s get into it!\\n\\n1. The Why\\n\\nOne issue with vanilla neural nets (and also CNNs) is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs. Here are a few examples of what RNNs can look like:\\n\\nInputs are red, the RNN itself is green, and outputs are blue. Source: Andrej K', metadata={'Title': 'An Introduction to Recurrent Neural Networks for Beginners'}),\n",
       " Document(page_content=' and outputs are blue. Source: Andrej Karpathy\\n\\nThis ability to process sequences makes RNNs very useful. For example:\\n\\nMachine Translation (e.g. Google Translate) is done with “many to many” RNNs. The original text sequence is fed into an RNN, which then produces translated text as output.\\n\\n(e.g. Google Translate) is done with “many to many” RNNs. The original text sequence is fed into an RNN, which then produces translated text as output. Sentiment Analysis (e.g. Is this a positive or negative review?) is often done with “many to one” RNNs.', metadata={'Title': 'An Introduction to Recurrent Neural Networks for Beginners'}),\n",
       " Document(page_content='�many to one” RNNs. The text to be analyzed is fed into an RNN, which then produces a single output classification (e.g. This is a positive review).\\n\\nLater in this post, we’ll build a “many to one” RNN from scratch to perform basic Sentiment Analysis.\\n\\nNote: I recommend reading the rest of this post on victorzhou.com — much of the math formatting looks better there.\\n\\n2. The How\\n\\nLet’s consider a “many to many” RNN with inputs x_0\\u200b, x_1\\u200b, … x_n\\u200b that wants to produce outputs y_0\\u200b, y', metadata={'Title': 'An Introduction to Recurrent Neural Networks for Beginners'}),\n",
       " Document(page_content=' wants to produce outputs y_0\\u200b, y_1\\u200b, … y_n\\u200b. These x_i\\u200b and y_i\\u200b are vectors and can have arbitrary dimensions.\\n\\nRNNs work by iteratively updating a hidden state h, which is a vector that can also have arbitrary dimension. At any given step t,', metadata={'Title': 'An Introduction to Recurrent Neural Networks for Beginners'}),\n",
       " Document(page_content='Often in data-constrained scenarios, scene comprehension has to occur with few time series observations - whether that’s audio, visual, or even radar. We do this using a surprisingly underrated technique called wavelet scattering.\\n\\nWavelet scattering (or scatter transform) generates a representation that’s invariant to data rotation/translation and stable to deformations of your data. Uninformative variations in your data are discarded — e.g. an audio sample time-shifted by various amounts. Information for downstream tasks like classification is preserved. Wavelet scattering requires no training and works great with low data.\\n\\nIts main computation is convolution, making it fast and applicable to images and 1D signals. We focus', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' applicable to images and 1D signals. We focus on signals in this article. We will retrace findings by the signal processing community and relate it to modern machine learning concepts. I show that, yes, we can do great without learning, using 20 samples. Recreate experiments and illustrations in this article with the colab notebook in this link.\\n\\nWavelets\\n\\nA wavelet can be convolved with the signal in the same sense that filters can. I think of convolution as the continuous analog to inner products, where large activation (commonly said in ML) or wavelet coefficient is caused by similarity between the continuous objects. By convolving elements from a dictionary to the signal under inspection, we capture local, spatial dependencies.\\n', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' inspection, we capture local, spatial dependencies.\\n\\nConvolution is a pivotal computation in the emergence of deep learning — it is extremely fast. The wavelet scattering implementation used by this article calls a deep learning backend solely for the efficient convolution! Kymatio is a great Python package built by passionate researchers that implement wavelet scattering, leveraging the PyTorch framework.\\n\\nReal and imaginary components of the Morlet Wavelet from M. Adamczyk et al., Automatic Sleep Spindle Detection and Genetic Influence Estimation Using Continuous Wavelet Transform (2015)\\n\\nThe basic building block of wavelet scattering is the Morlet wavelet. It is a Gaussian windowed sinusoid with deep connections to mammal hearing and vision', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content='usoid with deep connections to mammal hearing and vision. By convolving wavelets ψᵥ indexed by different frequency locations v, the wavelet transform of x is the set of scatter coefficients\\n\\n{ x ∗ ψᵥ }ᵥ\\n\\nWhen the wavelet’s sine component has room to dilate (sine wave ‘slowing’ its oscillation), it decomposes the signal at decorrelated scales. That’s good for revealing the signal’s frequency structure, but doing so over the course of a longer time range. The consequence is that a wider Gaussian window trades temporal resolution for increased frequency resolution ( itself a consequence of Heisenberg’s Uncertainty', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' of Heisenberg’s Uncertainty Principle). In practice, the width of the Gaussian window that tapers the sine wave is an important parameter [M. Cohen 2018].\\n\\nWavelet Scattering\\n\\nThe historic context of wavelet scattering starts with Fourier transform, the canonical signal processing technique. The shortcoming of Fourier representation includes its instability to signal deformations at high frequency. For a signal x perturbed slightly by a high frequency deformation into x̃, their spectrogram representations look different ( large ‖ FFT(x) -FFT(x̃) ‖ ) even if they remain similar signals to the human eye. This instability is due to sine wave’s', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' instability is due to sine wave’s inability to localize frequency information, since sine itself has non-localized support.\\n\\nNovember, Golden Gardens credit: u/purebredcrab at reddit.com/r/analog\\n\\nWavelet transform fixes this by decomposing the signal with a family of wavelets, with various dilation, where every wavelet has localized support (flattening out eventually like the Morlet wavelet). The resulting wavelet representation localizes high frequency components of the signal. Yet because the wavelet operator commutes with translations, the resulting representation becomes translation covariant — shifting a signal also shifts its wavelet coefficients. This makes comparison between translated signals difficult, and translation invariance', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' comparison between translated signals difficult, and translation invariance is key to tasks like classification. How do we achieve a signal representation Φ(x) that is translation invariant, stable under deformations, and offers good structural information at all frequencies?\\n\\nWavelet scattering builds a signal representation Φ(x) with a redundant dictionary of Morlet wavelets. While the space of signals X can be really high dimensional, the transform forms a kernel metric over the space of signals, inducing a lower dimensional manifold. Watch Stéphane Mallat discuss the manifold interpretation with visualization.\\n\\nReaders have probably trained a neural convolution network to encode an image into a latent manifold Z, whose code/latent representation is used for classification or', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' code/latent representation is used for classification or structure discovery — and that’s what is happening in analogy. Wavelet scattering encodes the dataset X where uninformative variability in X: translation, rotation, and scaling — the action of groups — are discarded in the process.\\n\\nThe key benefits of transforming a signal by Φ [J. Bruna and S. Mallat, 2013] is that\\n\\nΦ is invariant to signal translation.\\n\\nDenote by xₜ a signal identical to x, except translated in time, then Φ(x) = Φ(xₜ).\\n\\nΦ is stable under signal deformations.\\n\\nI.e. Φ', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content='ations.\\n\\nI.e. Φ is Lipschitz continuous to deformations — difference between scatter representations of a signal with its deformed version is linear. A deformation can be some local displacement/distortion (or a ridiculous amount of distortion, as a later example shows). For Lipschitz constant C>0 and a displacement field τ(u) causing deformations to create x̃,\\n\\n‖Φ( x ) - Φ( x̃ )‖ ≤ C‖x‖ supᵤ|∇τ(u)|\\n\\nWhere ‖x‖= ∫ ‖x(u)‖²du and supᵤ|∇', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content='�²du and supᵤ|∇τ(u)| is the global deformation amplitude.\\n\\nΦ does not require learning.\\n\\nThe priors introduced by wavelet scattering are nice enough that its performance often makes learning redundant; plus it comes with interpretable features and outputs. In data-constrained scenarios, if comparable data is publicly available, a nice plan is to pipe your small dataset through a pretrained model. But in the difficult situation that your dataset is small and unique, consider wavelet scattering as an initialization for ConvNets and other models. I suspect the future of ‘data constrained learning’ will be in synergizing predefined filters alongside learned filters.\\n\\nFigure below illustrates stability under', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content=' learned filters.\\n\\nFigure below illustrates stability under deformations. Left we applied scatter transform to the voice of a speaker saying ‘zero’. The scatter representation consists of the coefficients derived from averaging/low pass filter, order 1 wavelets, and order 2 wavelets. Right After applying a displacement field that has mostly masked the structure of the original signal with a sine wave, Φ( x̃ ) is barely affected; the deformation’s effect has been linearized by Φ’s transformation.', metadata={'Title': 'A ConvNet that works well with 20 samples: Wavelet Scattering'}),\n",
       " Document(page_content='Time Series must be handled with care by data scientists. This kind of data contains intrinsic information about temporal dependency. it’s our work to extract these golden resources, where it is possible and useful, in order to help our model to perform the best.\\n\\nWith Time Series I see confusion when we face a problem of dimensionality reduction or clustering. We are used to think about these tasks in more classical domains, while they remain a tabù when we deal with Time Series.\\n\\nIn this post, I try to clarify these topics developing an interesting solution where I work with multidimensional Series coming from different individuals. Our purpose is to cluster them in an unsupervised way making use of deep learning, being wary of', metadata={'Title': 'Time Series Clustering and Dimensionality Reduction'}),\n",
       " Document(page_content=' way making use of deep learning, being wary of correlations, and pointing a useful technique that every data scientist must know!\\n\\nTHE DATASET\\n\\nI got the data from UCI Machine Learning repository; I selected the Public Dataset of Accelerometer Data for Human Motion Primitives Detection. These data are a public collection of labeled accelerometer data recordings to be used for the creation and validation of acceleration models of human motion primitives.\\n\\nDifferent types of activities are tracked, i.e. drinking, eating, climbing and so on. For a particular activity of a specific individual measured, we have 3 different sensor series at disposal: X-axis (pointing toward the hand), Y-axis (pointing toward the', metadata={'Title': 'Time Series Clustering and Dimensionality Reduction'}),\n",
       " Document(page_content=' hand), Y-axis (pointing toward the left), Z-axis (perpendicular to the plane of the hand).\\n\\nI figure myself in this situation because it allows to carry out our initial problems of clustering (multiple individuals) and dimensionality reduction (multiple series for every individual) all in one single case.\\n\\nBelow I plot 2 examples of data at our disposal coming from a male and female individuals. In total, we have 20 individuals with the same measurement length.\\n\\nDIMENSIONALITY REDUCTION', metadata={'Title': 'Time Series Clustering and Dimensionality Reduction'}),\n",
       " Document(page_content='The Little Robot that Lived at the Library\\n\\nHow we built an emotive social robot to guide library customers to books Minja Axelsson · Follow Published in Towards Data Science · 9 min read · Jul 25, 2019 -- 1 Listen Share\\n\\nThe Oodi library\\n\\nOur team at Futurice designed and built a social robot to guide people to books at Helsinki’s new central library, Oodi. Opened in 2018, Oodi is the biggest of Helsinki’s 37 public libraries. It has 10,000 visitors a day, and an estimated 2 million visitors a year (compared to Finland’s 5.5 million population, that is a significant portion).\\n\\nAutomatic returns system\\n\\n', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' portion).\\n\\nAutomatic returns system\\n\\nThe MiR200 wagon moving books and their boxes\\n\\nOodi is big on automation and robotics. It has an automatic returns system: customers set their books on a conveyor belt, which brings the books to the basement, where they get sorted into boxes, which are picked up by a mobile MiR200 robot, which brings the books to the 3rd floor. At the 3rd floor, librarians place the books back on the shelves.\\n\\nAt the start of our project, we brainstormed how Oodi could use social robots: helping kids learn to read, instructing people on using equipment such as 3D printers, giving information about the library in several languages, and', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' giving information about the library in several languages, and helping people find their way at the library.\\n\\nWe eventually settled on a robot that would help customers find the books and book categories they want. Since Oodi is so big, customers have a hard time getting around, and library employees spend a significant amount of time advising people how to find things. But this is not the work librarians are meant to be doing, or want to be doing. Librarians are very knowledgeable about literature. Their expertise is better used in in-depth service, helping visitors find specific books that fit their needs best. This type of work can take 30–40 minutes. In comparison, “Where is the psychology section?” takes 1–', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' is the psychology section?” takes 1–3 minutes to answer. Stacked together, a whole day of 1–3 minute tasks becomes tedious, and a waste of skills.\\n\\nThis is where the robot steps (or rather, rolls) in. A whole day of menial tasks would not bother a robot. We realized we could re-purpose the MiR200 mobile robots that the library already had, and was using to move books between the basement and the 3rd floor.\\n\\nThe robot design team: Oodi librarians, Oodi’s customers, and Futurice’s roboticists\\n\\nThe robot would have the advantage of being able to access Oodi’s database directly,', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' to access Oodi’s database directly, and provide real-time information on which books are currently on the shelf. The robot could be more approachable to people who have social anxiety, and are afraid to approach library employees. Additionally, it could save both the customers’ time (no need to queue for a librarian), and the librarians’ time (who can help customers with more meaningful tasks).\\n\\nFirst draft\\n\\nA Mobile Robot with (the Illusion of) a Personality\\n\\nThe design team, consisting of Oodi’s librarians, Oodi’s customers, and Futurice’s roboticists, defined design guidelines for the robot that would be built on top of', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' for the robot that would be built on top of the MiR200 robot, using these social robot co-design canvases (available as open source):\\n\\nThe robot is sincerely a machine — it beeps expressively, and doesn’t talk\\n\\nThe robot has a touch-screen UI, and users don’t talk to the robot\\n\\nThe robot uses lights, sounds, and movement to communicate\\n\\nThe use of the robot should not depend on how familiar the user is with technology\\n\\nThe design needs to account for accessibility, the level of background noise, the library’s changing layout and furniture, and dodging customers\\n\\nThe design team decided that the robot should not be too humanoid. We wanted', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' the robot should not be too humanoid. We wanted a more abstract form for the robot, with expressive, non-speaking forms of communication. We wanted a design with a bit of imagination and whimsy.\\n\\nThe team also wanted to make sure that the robot aligned with Oodi’s strategy and policies. The following ethical considerations were underlined:\\n\\nGDPR (the EU’s data regulation) needs to be followed. Data about the person who looks for the book should not be combined with data about which book they were looking for.\\n\\nAccessibility is important. The library’s principle is that everyone is served equally. Physical limitations, different languages, and impaired vision need to be taken into account.\\n', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content=' impaired vision need to be taken into account.\\n\\nThe customer should be able to choose to be served by a human librarian.\\n\\nIf the robot is not working, it may cause frustration and rude behaviour by customers. This should be prepared for, so that librarians are not negatively affected.\\n\\nWe started testing the robot at the library, mapping out appropriate routes, and building the user journey. Luckily, we had some very excited testers.', metadata={'Title': 'The Little Robot that Lived at the Library'}),\n",
       " Document(page_content='Generating the dataset\\n\\nWe generate an artificial dataset. The first thing that came to my mind is an order registry, in which we store:\\n\\nid of the client\\n\\nof the client name of the product\\n\\nof the product date of purchase\\n\\nof purchase amount of product purchased\\n\\nof product purchased the unit price of a certain product\\n\\nAs this is only a toy example, we do not dive deeply into the logic behind the dataset. We can agree that it vaguely resembles a real-life scenario. For testing the performance of different approaches, we generate 10 million rows of data.', metadata={'Title': 'Speeding up data wrangling with dtplyr'}),\n",
       " Document(page_content='Made this in Microsoft Paint\\n\\nSo, for my first write-up, I am tackling a problem I encountered whilst working on my first data science project at my company. I work as a machine learning researcher at ALC- Innovative Transportation Solutions and my problem arose during a predictive modeling project. Multi-class imbalance. I had encountered class imbalance before in classroom projects and had employed the use of the ROSE package but never had I been exposed to a multi-class imbalance issue.\\n\\nGoogle Images: Binary class imbalance. I’ve dealt with binary class imbalance before and there are plenty of tools and articles about tackling this common data issue.\\n\\nBinary class-imbalance is a common headache in data science but can be', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content=' is a common headache in data science but can be easily solved(great article about it: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). Unfortunately for this project I had on my hands, there weren’t as many resources. I wanted to share my thought process for overcoming multi-class imbalance so someone else might find it useful in mediating their own distribution conflicts.\\n\\nThe Distribution\\n\\nMy company’s data is not open for sharing but here is a simulated version of the issue I was facing:\\n\\nThis data distribution across classes is completely imbalanced. This is bad because the goal of my', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='balanced. This is bad because the goal of my project is to build a multi-class classifier that can correctly assign what ‘Class’ a data point belongs to. Specifically, this imbalance is an issue because the predictive model, whichever I end up pursuing, would be biased towards Class1 and, to less of a degree but still, Class2. It would achieve decent accuracy by classifying the majority of the train and test set as Class1 or Class2 because of the imbalance; this is the ‘Accuracy Paradox.’ My model might achieve good accuracy on classification but that’s because the model would only be modelling the imbalanced distribution.\\n\\nGoogle Images: Confusion Matrix. If a ML model trains', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content=': Confusion Matrix. If a ML model trains on an imbalanced data set it will over-classify towards the majority class. In the image, for reference, the model would predict all classes to be ‘P’ while some of those should have been ‘N.’ In the case of multi-class imbalance the effects would be even more drastic where the model would predict ‘P’ (because it’s the majority class in this example) when the actual class was ’N’ or ‘O’ or ‘M’ etc.\\n\\nThis issue is compounded by the fact that the distinguishing characteristics between the classes is quite thin, in the case of my actual work project', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content=' thin, in the case of my actual work project. Their longitude/latitude features are only different by a minuscule amount. Sure, there are other predictive features but Geo-spatial data makes up the bulk of the predictive and interpret-ability aspects of the model. Yet, this difference in classes is important to the company and the decision making of the model and therefore this issue must be tackled.\\n\\nGoogle Images: Imbalanced Classifier. This image is an example of how an imbalanced data set would create an ‘accurate’ classifier that, in production, would really be a weak classification model hiding behind the guise of ‘High Accuracy.’\\n\\nROSE\\n\\nROSE, or', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='\\n\\nROSE\\n\\nROSE, or the Random Over Sampling Experiment, is a fantastic R package that deals with class imbalance quite well, but only binary class imbalance (two classes). How could I use this package to fix the multi-class data I was looking at?\\n\\nAfter trial-and-error, researching of options, and dropping a quick email to one of my great professors I determined the best course of action: writing a function that takes the whole data set, divides it up into 6 subsets using ‘Class’ as the dividing feature, and then use ROSE to balance those subsets out to my desired distribution. Then they would be compiled back into one, reasonably balanced data set. Each subset contains', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content=' one, reasonably balanced data set. Each subset contains Class1 and then one of the minority classes. Class2 was left out because it’s not under-represented.\\n\\nThe subset is then given to a ROSE argument to over-sample the minority class. The ROSE code for doing so can be seen below. The identity of the minority class is used as the formula and the ‘p’ argument is the probability of sampling the rare class, an alternative is setting N to the desired size of the data set. Below, with p=0.5, these arguments return a data set where the minority class is now represented in 50% of the data.\\n\\nlibrary(ROSE) BalancedData <- ovun', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='library(ROSE) BalancedData <- ovun.sample(MinorityClassi~, ImbalancedData, method=\"over\", p=0.5, subset=options(\"subset\")$subset, na.action=options(\"na.action\")$na.action, seed) index = createDataPartition(y=BalancedData$Class, p=0.7, list=FALSE)\\n\\ntrain = BalancedData[index,]\\n\\ntest = BalancedData[-index,] BalTrain <- droplevels.data.frame(train)\\n\\nBalTest <- droplevels.data.frame(test)\\n\\nAfter over-sampling the minority class and bringing', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='After over-sampling the minority class and bringing the distribution out of imbalanced hell I ran this balanced data set through my classifier. The best model was a Random Forest (compared against a logistic regression model as the baseline, a gini-criteria decision tree, an information-gain decision tree, and a neural network). The results were great, but not so great on the test set. Why? Because the over-sampled minority class data weren’t new data points. So the model couldn’t conceive that new points could fit into that class.\\n\\nI considered using SMOTE, synthetic-minority over-sampling technique but it’s results were negligible at best. I settled on', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='s results were negligible at best. I settled on using the ROSE formula in the ROSE package which: “creates a sample of synthetic data by enlarging the features space of minority and majority class examples” (cran.r-project.org/web/packages/ROSE/ROSE.pdf).\\n\\nSynthBalData <- ROSE(MinorityClassi~, ImbalancedData, p=0.5, hmult.majo=1, hmult.mino=1, subset=options(\"subset\")$subset, na.action=options(\"na.action\")$na.action, seed) index = createDataPartition(y=SynthBalData', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='DataPartition(y=SynthBalData$Class, p=0.7, list=FALSE)\\n\\nSynthBalTrain = SynthBalData[index,]\\n\\nSynthBalTest = SynthBalData[-index,] SynthBalTrain <- droplevels.data.frame(SynthBalTrain)\\n\\nSynthBalTest <- droplevels.data.frame(SynthBalTest)\\n\\nBy sticking to my original method of subset creation and applying ROSE I got back synthetic yet balanced data samples and compiled a new data set. I trained all the models on a simple random sample 70:30 train/test split of the data. The accuracy was high', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='test split of the data. The accuracy was high, and the models responded very well to new data points as they come in from my company’s server.', metadata={'Title': 'Multi-Class Imbalance'}),\n",
       " Document(page_content='This article will go through the theory to demystify this insufficiently known part of NLP. Then, in a second article, we will suggest tools to help you understand how to easily implement a Dependency Parser.\\n\\nWhen we think about a word’s neighbors, we could think about the neighborhood as their location in a sentence, their relation to other words (subject, determinant, etc.), called syntax, or as their meaning similarity, called semantics. What interests us here is the syntactical neighborhood.\\n\\nVocabulary\\n\\nFirst, let’s define some vocabulary to make it clearer for everyone.\\n\\nSemantics is the linguistic and philosophical field that studies meaning and interpretation. It relies a', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program.\\n\\nis the linguistic and philosophical field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program. Syntax is the linguistic field of grammar. It is the study of the rules for word patterns in sentences. Known in programming too, errors in syntax often lead to bugs, because rules are often much stricter than in oral language.\\n\\nWhat is a Dependency Parser ?\\n\\nA Dependency', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' Dependency Parser ?\\n\\nA Dependency Tree is a structure that can be defined as a directed graph, with |V| nodes (vertices), corresponding to the words, and |A| Arcs, corresponding to the syntactic dependencies between them. We may also want to attribute labels to dependencies, called relations. These relations give details about the dependency type (e.g. Subject, Direct Object Complement, Determinant…). You can find all the relations from Universal Dependencies by following this link : https://universaldependencies.org/u/dep/index.html.\\n\\nExample of Dependency Tree : “What is a parser ?”\\n\\nIn an arc h → d,', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content='�\\n\\nIn an arc h → d, h is the head and d is the dependent. The head is the most important node in a phrase, while the Root is the most important node in the whole sentence: it is directly or indirectly the head of every other node.\\n\\nA Dependency Parser simply transforms a sentence into a Dependency Tree.\\n\\nMetrics : how to recognize a good parser ?\\n\\nAn accurate Dependency Parser recognizes the dependencies and relations between words well. Two Metrics (scores) are useful for this:\\n\\n- Unlabeled Attachment Score (UAS), which corresponds to the number of correctly predicted dependencies over the number of possibilities;\\n\\n- Labeled Attachment Score', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' possibilities;\\n\\n- Labeled Attachment Score (LAS), which corresponds to the number of correctly predicted dependencies and relations over the number of possibilities.\\n\\nLAS is always less than or equal to UAS, because an incorrect dependency leads to a suboptimal UAS and LAS, while an incorrect relation (or label) only leads to a LAS decreasing.\\n\\nAlgorithm : How does it work ?\\n\\nAs you might have thought, we could create a Dependency Parser through rules developed by linguists. These parsers are called Rationalists. They are not at all efficient, since languages are very complex, and they change over time. Any small change in the language would lead to tremendous changes in the', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' in the language would lead to tremendous changes in the parser. Machine Learning allows for the development of Empiric parsers, which are data driven. Fed by many sentences, probabilities of dependencies or relations can be drawn. Linguistic knowledge may be used, but does not have the last word, which is a good point if you, like me, have forgotten your primary school lessons…\\n\\nSeveral steps are needed to create a Dependency Parser. Our inputs are the words of the sentence with their properties (index, Part of Speech tag, Lemma, Features); then, we must calculate features for all possible arcs in the sentence. Thanks to these features, we compute a score for each possibility, and we finally decode scores with a', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' each possibility, and we finally decode scores with a decoder.\\n\\nFeatures and Score\\n\\nEach word in the sentence has some attributes, like Part of Speech tags or Lemmas. You might know them if you have already read about NLP. You can check it out here, if not:\\n\\nWith these features, we train a Machine Learning regression model that returns the score to be exploited by the decoder.\\n\\nFeature selection is crucial, and some models allow us to bypass this part via a deep learning part. This is the case with the algorithm we will present in the following section.\\n\\nDecoders\\n\\nThere are a lot of different decoders already developed. However, we can divide them into two', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' developed. However, we can divide them into two categories: Transition-based decoders and Graph-based ones. Transition-based decoders are faster and need less memory to decode scores, but they are generally less accurate than Graph-based decoders. I will only go through Graph-based model principles in this article.\\n\\nOther algorithms can apply different transitions, but this one allows us to understand the main principle.\\n\\nGraph-Based Decoders\\n\\nIt is necessary to deal with graph theory to understand these algorithms.\\n\\nA graph G=(V, A) is a set of vertices V (called also nodes), that represent the tokens, and arcs (i, j)∈ A where i', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' (i, j)∈ A where i, j ∈ V. The arcs represent the dependencies between two words.\\n\\nIn a Graph-based dependency parser, graphs are directed, which means links have different directions, and there can be multiple arcs between nodes, this is called a multi-digraph.\\n\\nWeighted Multi Directed Graph (G)\\n\\nYou can note that some arrows are thicker than others. This represents the weight of arcs. The more an arc weighs, the stronger the link between two nodes. We could interpret this as the strength of the syntactic dependency for our parser. For Example, C and A seem to be very dependent on B, but B does not seem very dependent on C and A', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content=' B does not seem very dependent on C and A.\\n\\nGraph G is too connected. In order to get a Dependency Tree, we want:\\n\\nTo link each word only with its dependents — not with all the words. The total number of arcs should be equal to the number of nodes minus 1 (|A| = |V|-1).\\n\\nTo keep the same nodes (or tokens or words).\\n\\nTo make it acyclic: we do not want a head to be dependent on one of its dependents (direct or indirect).\\n\\nFortunately, all of this already has a name: what we want is a Spanning Tree!\\n\\nExample of Spanning Tree from the graph G', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content='\\nExample of Spanning Tree from the graph G\\n\\nAn other example of Spanning Tree\\n\\nIf I was clear on what a Spanning Tree is, you should know that there are multiple possibilities, since we only have a few conditions to get one. Here comes a trick: we want the best one, certainly, but how could we determine the “best” one?\\n\\nWe have 3 nodes here, and we want to keep them. However, we have 6 arcs and we want to keep only 2. The “best” Dependency Tree is the one that has the highest weights: this is called the Maximum Spanning Tree (MST).\\n\\nMaximum Spanning Tree of G\\n\\nMinimum Span', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content='Maximum Spanning Tree of G\\n\\nMinimum Spanning Tree of G\\n\\nThis Maximum Spanning Tree gives us our Dependency Tree, which we will use to find the closest syntactic neighbors in the sentence.\\n\\nConclusion\\n\\nThe insight given here is very light compared to the different existing algorithms. However, this should improve your intuition when developing your Dependency Parser.', metadata={'Title': 'Dependency Parser or how to find syntactic neighbours of a word'}),\n",
       " Document(page_content='Member-only story A gentle introduction to Recommendation Systems\\n\\nIntroduction to Recommendation Systems\\n\\nIf you are here, reading about Recommendation Systems, surely you already know what we’ll be talking about, so maybe you can just jump over this brief chapter. But if you came here attracted by the cover image, or if you want to know more about how Recommendation Systems emerged and grew up in the last years, then stay tunned with this section of the article.\\n\\nLet’s go back in time and try to picture this: it’s Friday night and you want to rent a videotape in your closest Blockbuster. You head over there with your girlfriend, already talking about which movie are you going', metadata={'Title': 'A gentle introduction to Recommendation Systems'}),\n",
       " Document(page_content=' girlfriend, already talking about which movie are you going to rent. Maybe a comedy? Or perhaps an action movie? A thunderstorm is coming, what settles the perfect mood for a horror movie or a thriller.\\n\\nWhile you’re thinking, your girlfriend tells you: ‘did you search for any good recommendation on the Internet?’. Unfortunately, the Internet was down all day at your job, and anyway, you’re still trying to get yourself around with all that mumbo jumbo around the World Wide Web.\\n\\nYou finally arrive there, but after almost an hour of deliberations, you still don’t know what to chose. You already saw all the popular movies, and the premier section of the', metadata={'Title': 'A gentle introduction to Recommendation Systems'}),\n",
       " Document(page_content=' the popular movies, and the premier section of the store is full of crap. You don’t want to be nitpicking, but there’s just nothing good. Your girlfriend asks the Blockbuster mate behind the desk, but he seems to know less about movies than your grandpa, and he is really not down with the kids. It’s already getting late and you’re knackered, so long story short, you decide to rent a good old movie you already saw. Is better to keep it sound and safe, than winging about your Friday’s night plan.\\n\\nThis example may seem old, but in so many aspects of life, people keep making decisions in a similar way. I', metadata={'Title': 'A gentle introduction to Recommendation Systems'}),\n",
       " Document(page_content=' people keep making decisions in a similar way. I can assure you that my grandpa doesn’t use Google when he wants to buy a book for himself. However, and luckily for the youngsters, the decision making today in…', metadata={'Title': 'A gentle introduction to Recommendation Systems'}),\n",
       " Document(page_content='Sentiment Analysis: a practical benchmark\\n\\nWith hands-on practical Python code, we demonstrate limitations of simple recurrent neural networks and show how embeddings improve fully connected neural networks and convolutional neural networks for the classification of sentiment.\\n\\nWe show how to work with sequence data, by doing sentiment classification on a movie review dataset. Sentiments are basically feelings which include emotions, attitude and opinions written in natural language.\\n\\nIMDB movie reviews dataset\\n\\nWe start by loading the IMDB dataset using Keras API. The reviews are already tokenized. We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small. We also want to have a finite length of reviews and not have to process', metadata={'Title': 'Sentiment Analysis: a practical benchmark'}),\n",
       " Document(page_content=' a finite length of reviews and not have to process really long sentences. Our training dataset has 25,000 customer reviews, together with their correct labels (either positive or negative).', metadata={'Title': 'Sentiment Analysis: a practical benchmark'}),\n",
       " Document(page_content='The Complete Guide to Decision Trees\\n\\nA complete introduction to decision trees, how to use them for regression and classification, and how to implement the algorithm in a project setting Marco Peixeiro · Follow Published in Towards Data Science · 9 min read · Jul 25, 2019 -- 2 Share\\n\\nThey are… don’t even try something else\\n\\nTree-based methods can be used for regression or classification. They involve segmenting the prediction space into a number of simple regions. The set of splitting rules can be summarized in a tree, hence the name decision tree methods.\\n\\nA single decision tree is often not as performant as linear regression, logistic regression, LDA, etc. However, by introducing bagging,', metadata={'Title': 'The Complete Guide to Decision Trees'}),\n",
       " Document(page_content=', etc. However, by introducing bagging, random forests, and boosting, it can result in dramatic improvements in prediction accuracy at the expense of some loss in interpretation.\\n\\nIn this post, we introduce everything you need to know about decision trees, bagging, random forests, and boosting. It will be a long read, but it will be worth it!', metadata={'Title': 'The Complete Guide to Decision Trees'}),\n",
       " Document(page_content='I love how simple and clear Keras makes it to build neural networks. It’s not hard to connect Keras to Tensorboard but that has always felt to me like a heavyweight solution is overly complicated for many of Keras’s users who often want to take a quick look at the underlying model.\\n\\nWith wandb, you can visualize your network’s performance and architecture with a single extra line of python code.\\n\\nTo show how this works, I modified a few scripts in the Keras examples directory.\\n\\nTo install wandb, just run “pip install wandb” and all of my Keras examples should work for you.\\n\\n1. Simple CNN\\u200d\\n', metadata={'Title': 'Lightweight Visualization of Keras Models'}),\n",
       " Document(page_content='.\\n\\n1. Simple CNN\\u200d\\n\\nI started with the requisite mnist_cnn.py.\\n\\nI added the “from wandb import magic” line below — you can also look at my mnist_cnn.py forked from the Keras examples with the one line change.\\n\\nNow when the model runs, wandb starts a process in the background saving relevant metrics and streaming them to wandb.com. You can go to https://app.wandb.ai/l2k2/keras-examples/runs/ovptynun/model and look at the output of my run.\\n\\n\\u200d\\n\\nI can see exactly the', metadata={'Title': 'Lightweight Visualization of Keras Models'}),\n",
       " Document(page_content='\\n\\u200d\\n\\nI can see exactly the data that my model is labeling and view the loss and accuracy curves automatically.\\n\\n2. Resnet on Cifar\\u200d\\n\\nNext, I forked cifar10_resnet.py and made the same one line change. You can see a nice visualization of a resnet at https://app.wandb.ai/l2k2/keras-examples/runs/ieqy2e9h/model.\\n\\nOn the system page, I can see that this model is using a little more of my single GPU than the mnist example.\\n\\n3. Siamese network\\u200d\\n\\nNext I tried the', metadata={'Title': 'Lightweight Visualization of Keras Models'}),\n",
       " Document(page_content='e network\\u200d\\n\\nNext I tried the siamese network example. Here I might want to look at the TensorFlow graph, luckily with our one line of code we automatically instrument and host TensorBoard. You can find this run at https://app.wandb.ai/l2k2/keras-examples/runs/fsc63n6a?workspace=user-l2k2.\\n\\nThis instrumentation took me under a minute per model, adds very little compute overhead, and should work for any Keras model you are working on. As you want to track more things you may want to replace the one line with:\\n\\nimport wandb wandb.init(', metadata={'Title': 'Lightweight Visualization of Keras Models'}),\n",
       " Document(page_content='\\n\\nimport wandb wandb.init(magic=True)\\n\\nThen you can use our custom wandb.log() function to save anything you want. You can learn more in our documentation.\\n\\nI really hope you find this useful!', metadata={'Title': 'Lightweight Visualization of Keras Models'}),\n",
       " Document(page_content='SQL Subqueries\\n\\nPhoto by Henri L. on Unsplash\\n\\nThe code I used for this blog can be found on my GitHub.\\n\\nEvery time I learn something new about SQL I start to find that there are so many applications that I am unaware of. After learning the basic syntax of queries and a few basic tools, I moved onto subqueries in SQL.\\n\\nSubqueries (aka inner queries or nested queries) are useful tools when you’re performing multiple steps. It’s feels Inception-like, since you’re querying into queries.\\n\\nSubqueries can be used in several areas within the query, so today we’ll cover using them in the', metadata={'Title': 'SQL Subqueries'}),\n",
       " Document(page_content=' today we’ll cover using them in the most common areas: SELECT , FROM , and WHERE clauses.\\n\\nSetting Up\\n\\nFor these examples, we’ll use the Chinook database which is also used in the SQL Basics blog.\\n\\nLet’s set up the database file, libraries, and functions:', metadata={'Title': 'SQL Subqueries'}),\n",
       " Document(page_content='Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗\\n\\nLanes Detection with Computer Vision Greg Surma · Follow 5 min read · Jul 25, 2019 -- 1 Share\\n\\nIn today’s article, we are going to use basic Computer Vision techniques to approach the street lanes detection problem which is crucial for self-driving cars. By the end of this article, you will be able to perform real-time lane detection with Python and OpenCV.\\n\\nReal-Time Lane Detection\\n\\nImplementation\\n\\nYou can find the full codebase for this project on GitHub and I encourage you to check it and follow along.\\n\\nLet’s start with defining our problem.\\n', metadata={'Title': 'Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗'}),\n",
       " Document(page_content='’s start with defining our problem.\\n\\nGiven an image of the road, we would like to detect street lanes on it.\\n\\nIn order to do it, let’s provide an image path and load it with OpenCV, then let’s invoke find_street_lanes pipeline with it.\\n\\ntest_image = cv2.imread(INPUT_FOLDER + TEST_IMAGE)\\n\\nstreet_lanes = find_street_lanes(test_image)\\n\\nAnd this is how our find_street_lanes pipeline looks like', metadata={'Title': 'Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗'}),\n",
       " Document(page_content='This is a continuation of my earlier post on compositional data analyses where I showed the pitfalls of treating compositional data as absolute data instead of relative data. In this post, I will summarize the techniques we can use to correctly analyze compositional data with specific examples demonstrated using RNA-Seq data.\\n\\nTwo main strategies exist for treating Compositional Data and specifically NGS data:\\n\\n1. Normalization to get back the absolute counts\\n\\n2. Compositional Data Analysis (CoDA) methods that transform the data using within sample references (Ex: ALR, CLR)\\n\\nNormalization to Absolute Counts\\n\\nThis is the most widely used technique in NGS data pre-processing when comparing across samples is desired. The', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='-processing when comparing across samples is desired. The relative read counts are ‘normalized’ to the total read depth to ‘recover’ the absolute counts. This, however, does not recover the absolute counts when the total absolute amounts of RNA or cells or the amount of relevant biological material significantly changes across samples. This more often leads to a false sense of security for the analyst and leads to treating these ‘normalized’ samples as absolute counts. This can result in erroneous conclusions when comparing across samples. Let’s prove that to ourselves using simulated data.\\n\\nSimulation Details\\n\\nHere, I simulated data for 100 genes, where\\n\\na. 5 genes have the true log fold change of 1', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' 5 genes have the true log fold change of 1 between control and experimental conditions (approximates tolerance or no growth under selection),\\n\\nb. 2 genes have the same true log fold change of > 1 in the experimental conditions (resistant and exhibit growth under selection), and\\n\\nc. 2 genes have the same true log fold change of < 1 in the experimental conditions (not resistant or tolerant),\\n\\nI simulated 5 different cases where different proportions of the remaining 91 genes are changed. Of the genes that change, ~90% are depleted, and ~10% are enriched in each case.\\n\\nThe depletion/enrichment of the other genes affects the relative count values and the read-depth normalized counts even though the total read', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' read-depth normalized counts even though the total read depth is fixed at 200K reads\\n\\nRead Depth Normalized (RDN) Counts\\n\\nDifferential Expression or Abundance: Even though all the reads have the same total depth (sum of counts), the log fold changes (LFCs) of genes calculated using the read depth normalized counts (RDN counts) are shifted compared to the true log fold changes (See Fig 1 below). Interestingly, the direction of the shift is not always predictable based on the fraction of genes changed. For example, when ~70% of the genes are changed, the LFCs calculated using the RDN counts are shifted down compared to the true LFCs. On the other hand,', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' true LFCs. On the other hand, the LFCs calculated using the RDN counts are shifted up compared to the true LFCs when 90% of the genes are changed. This is because the absolute true counts in the former case are higher than the latter case. In general, we cannot anticipate or estimate the true total absolute counts for a sample.\\n\\nFig 1: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using RDN Counts\\n\\n2. Correlation Between Genes: To see how things compare between relative counts and absolute counts., I calculated the correlation for the non-constant genes across all the 5 samples (each with either 0.1, 0.2,', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' with either 0.1, 0.2, 0.4, 0.6, 0.9 fractions of changed genes). I used both the true counts and relative counts using Polyester simulated count data at 200K read depth.\\n\\nFig 2: Comparing True Correlations Between Genes to Correlations Calculated using RDN Counts\\n\\nAs we can see from the figure above, some of the correlation coefficients calculated using the RDN counts, are significantly different from the true correlation coefficients, with a negative bias.\\n\\nThe 2 examples above show the pitfalls of using RDN counts to estimate the differential expression or correlation between genes. Instead of using RDN counts, one should always use spike-in controls when trying to recover', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' always use spike-in controls when trying to recover absolute counts from relative compositional data. We will show that next\\n\\nSpike-in Normalized Counts\\n\\nTo truly correct for the change in the absolute counts, we need spike-in controls or genes that we add into all our samples at the same abundance (amount) just before the sequencing step. Doing this will normalize all the samples to the same total abundance scale and makes the comparisons correct. This only works when the data are closed due to sequencing (because we are adding the spike-ins just before sequencing), and will not help if the constraint is biological or happens upstream of the sequencing step. In that case, we need to add in the spike-ins before this', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' need to add in the spike-ins before this constraining step, but it is not always possible to do so due to physical and biological limitations of adding the spike-ins.\\n\\nLet’s see how this works using our data. In our data, we have 92 different controls or spiked-in genes that have the true absolute abundance. Let’s use these to ‘normalize’ the data and therefore bring all samples to the same absolute count scale.\\n\\nDifferential Expression or Abundance: Fig 3 below is analogous to Fig 1 but with spike-in normalized counts instead of RDN counts. The plot has artificial jitter (noise) added to show all data, but the true data all', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' to show all data, but the true data all lie along the diagonal. This indicates the power of spike-ins. Properly designed spike-ins can recover the absolute counts (up to a constant multiplicative factor), provided the spike-ins are added just before the step that leads to closure or constraints in the data, which is not always possible.\\n\\nFig 3: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using Spike-in Normalized Counts\\n\\n2. Correlation Between Genes: Looking at correlations between genes, we see that the coefficients calculated using the spike-in normalized counts can recover the true coefficients. Fig 4 below:\\n\\nFig 4: Comparing True Correlations Between', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='\\nFig 4: Comparing True Correlations Between Genes to Correlations Calculated using Spike-in Normalized Counts\\n\\nSo, it seems like we found the solution to our problem. All we have to do is add some controls and we are good! Not so fast, unfortunately. In this simulated case, the source of closure for the compositional data is sequencing and we were able to add some controls right before we simulated the sequencing data. In real-life data generation process, the sources of closure can occur anywhere in the usually complicated workflow of extracting DNA/RNA. Also, the biological system itself could be inherently compositional (For example the capacity for a cell to produce RNA is limited), in which case no spike-ins', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' is limited), in which case no spike-ins introduced outside the cell can recover the true absolute counts.\\n\\nCompositional Data Analysis (CoDA) Methods\\n\\nAn alternative to spike-in normalization is using CoDA methods that typically transform the count data with respect to an in-sample reference. Additive Log-Transformation (ALR) and Centered Log-Transformation (CLR) are examples of some commonly used CoDA transformations. These methods are first proposed by John Aitchison originally in 1986. The core idea being that the log-ratio transformations of the components relative to another reference component can be treated as any other unconstrained data. This transforms the data from the original simplex space (as', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' the data from the original simplex space (as in our ternary diagram in the first part) to the Euclidean space. This allows us to use all classical analyses techniques on these data.\\n\\nA cautionary note: These techniques do not claim to open the data as do the ‘normalization’ methods from the previous section. These techniques are also applicable to all data, whether they are relative or absolute. Another point to note is normalizing using spike-ins is the same as using the Additive Log-Ratio (ALR) transformation. The benefit of using the general ALR transformation is that it is applicable even when we do not have spike-ins that have constant abundance across samples. The disadvantage with', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' that have constant abundance across samples. The disadvantage with the general ALR transformation is we need to choose the reference properly to make sense of the data and answer the relevant questions.\\n\\nLets now look at the CoDA methods in more detail using the same data set that we used as before.\\n\\n1.Differential Expression or Abundance: There are many methods to find changes in compositional data before and after treatment. Many of these methods surprisingly come from the Microbiome literature, whereas the gene expression literature mostly relies on traditional methods like DESeq2 and EdgeR, which do not explicitly take into account the compositional nature of the data. DESeq2 and EdgeR implicitly assume that the absolute abundances do not change', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' implicitly assume that the absolute abundances do not change due to the treatment. This is equivalent to using the Centered Log-Ratio (CLR) transformation from the CoDA methods. This transformation uses the geometric mean of the genes or components as the reference, and therefore all results have to be interpreted with respect to the geometric mean. At this stage, it is tempting to translate this assumption to mean that the geometric mean of the genes does not change between control and treatment. Maybe the geometric mean changes, maybe it does not, there is no way to know for sure without orthogonal information beyond the relative counts from sequencing. Most users of DESeq2 and other Differential Expression tools fall for this trap and conclude any significant changes called by', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' for this trap and conclude any significant changes called by the algorithms to mean significant changes in the absolute counts. Instead, these are just significant changes with respect to the geometric mean of all components.\\n\\nThere are emerging methods to apply statistical rigor to DA in compositional data. The most popular methods are ALDEx2 and ANCOM. The main philosophy of these methods is to rely on log-ratio tests of transformed relative data with respect to a reference component and to carefully interpret these results. The main issue with these methods is that the results can only be interpreted with respect to the chosen reference, and no guidance is provided on how to choose the reference. Giuliano Cruz pointed me to a more recent methodology that uses Differential Ranking (', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' a more recent methodology that uses Differential Ranking (DR) and lays out a more reasoned approach to choosing a reference. This is what I will use here briefly, and hopefully, in some future post go into the gory details of running some of these algorithms.\\n\\nThe main idea of DR is to choose some random reference component to calculate the log ratios for all components in both treatment and control. In the next step, these components are ranked in the order of their the difference Δ(log-ratio) between treatment and control conditions. This rank-order of Δ(log-ratio) values calculated using the known relative counts should be identical to the rank of the Δ(log-ratio) values calculated using the unknown true', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='-ratio) values calculated using the unknown true absolute counts. For example, I show below the Δ(log-ratio) values calculated using the relative counts vs. Δ(log-ratio) values calculated using absolute counts, for the case where 90% of the genes are differentially expressed:\\n\\nFig 5: Δ(log-ratio) values Calculated using Absolute vs. Relative Counts\\n\\nAs you can see, the magnitude of the Δ(log-ratio) values are different depending on whether we use the relative or absolute counts, but the rankings of Δ(log-ratio) values stay the same. This does not mean that the top-ranking genes have higher counts in treatment vs control,', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='ranking genes have higher counts in treatment vs control, and the low-ranking genes have lower counts. It could so happen, that the top-ranking genes have depleted absolute counts in the treatment conditions compared to the control condition, but the lower-ranked genes have even worse depletion in the treatment condition. In short, we cannot say anything about the changes in absolute reads between treatment and condition.\\n\\nI will now choose the top-ranking gene as my reference and again calculate the Δ(log-ratio) values using this new reference.\\n\\nFig 6: Δ(log-ratio) Values Calculated using the Top-Ranking Gene as Reference\\n\\nFrom this plot, we can use an arbitrary cut-off of 0.', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' can use an arbitrary cut-off of 0.5 and choose any genes beyond this as our potential DA genes to test further. Of course, if we want more genes to test, we can relax the cut-off.\\n\\nAnother recommendation to get around choosing reference is to have some sort of positive or negative controls in the population. Suppose, we know a gene that will increase in absolute abundance in the treatment condition, then we can use this gene as the natural reference for calculating log-ratios and rank-order the Δ(log-ratio) values. Any log-ratio greater than 1 implies that the gene is better than the positive control, and log-ratio less than 1 implies worse than the positive control. Even', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' than 1 implies worse than the positive control. Even better, is to have 2 controls to bound the effect size, and interpret the log-ratios with reference to both of these genes.\\n\\nIn my simulation, I only have one sample per replicate, and therefore could not do any statistical analyses. In a future post, I will generate multiple replicates per condition and play with ALDEx2, ANCOM, and DR algorithms to test their sensitivity and specificity.\\n\\n2. Correlation Between Genes: As shown in part 1 of this series, correlation is not sub-compositionally coherent and therefore does not follow one of the principles of CoDA. Briefly, the correlation coefficient between any two genes depends on the other', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' correlation coefficient between any two genes depends on the other components or genes present in the data. Aitchison first proposed using the variance of log-ratio transformed values (VLR) to estimate the dependency of 2 variables. For example, to calculate the dependency between features or genes, g, and h, across n samples, we would use:\\n\\nVLR is sub-compositionally coherent and therefore doesn’t lead to spurious correlations. The main issue with using VLR is that even though it equates to 0 when genes g and h are perfectly correlated, it doesn’t have an upper limit when the genes are perfectly independent. And that makes it difficult to compare VLR for one gene pair against VLR for', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' VLR for one gene pair against VLR for another gene pair because of this scaling issue. Several methods/metrics are proposed based on VLR to estimate the dependencies between compositions, the most notable being SparCC, SPIEC-EASI, and proportionality. In this blog, I only review proportionality in some detail. All these methods attempt to use VLR to derive metrics that are analogous to correlation coefficients and therefore can be compared across different pairs of components.\\n\\nThree proportionality based metrics are proposed in the R package propr based on work by Lovell et. al. and Quinn et. al. These metrics are calculated on log-transformed data. For definitions see the propr package. Ai below refers', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' definitions see the propr package. Ai below refers to the log-transformed values for a gene or component ‘i’ in the data. Ai could be absolute or relative counts and the definitions still apply.\\n\\nphi (Φ) = var(Ai -Aj)/var(Ai) rho (⍴) = var(Ai -Aj)/(var(Ai) + var(Aj)) phis (Φs) = var(Ai -Aj)/var(Ai +Aj)\\n\\nThe closest metric to traditional correlation coefficient is rho which ranges from -1 to 1. phi is unbounded and can vary from 0 to Inf,', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' unbounded and can vary from 0 to Inf, and phis is a symmetric variant of phi and is a monotonic function of rho. I will focus on rho in the rest of the blog.\\n\\na. Using Absolute Counts: We can recover the absolute counts from relative counts if we have a spike-in control. Since we already have spike-in data available to us, I will calculate the rho values using the spike-in transformed data, i.e. A¹ = log(TPM_counts¹/Spike_TPM_counts) for gene 1 using spike_TPM_counts as the normalization counts. This will recover the original absolute counts', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='ization counts. This will recover the original absolute counts. Now, we can calculate the rho values using the equation above. I plot the correlation between absolute counts and the rho values below:\\n\\nFig 7: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data\\n\\nAs can be seen from this plot, using proportionality we can capture most of the original correlations between the true absolute counts. Of course, this is a contrived example, where we have a good spike-in available to retrieve the absolute counts. Even with this contrived example, we still see some differences between the true correlations and the proportionality values calculated using the spike-in normalized counts. This', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' calculated using the spike-in normalized counts. This is due to the way the proportionality based metrics are calculated which makes them extremely sensitive to the estimates of the variances of the log-transformed values. Here we only have 5 samples to calculate the variances and in most cases, the first 3 samples have the same values. This I suspect leads to the formulae to calculate the metrics to break down. Have to grok on this a little bit more. The evidence for this hypothesis is that, if we only look for components that have distinct values in at least 4 different samples, then the correlation values and the proportionality metrics match pretty well as can be seen below:\\n\\nFig 8: Correlations of True Absolute Counts', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='Fig 8: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data: Only for Genes with At Least 4 Distinct Values out of 5 Samples\\n\\nIn general, rho and other proportionality based measures have good precision and poor recall, and having more samples gives better estimates for the variances and therefore for the rho values. Also, boot-strapping is generally used to establish a cut-off for calling relationships significant. For example, in the plot above, the cut-off for significant ‘rho’ values could be 0.75.\\n\\nb. Using Relative Counts: The world is unfairly complex, and we don’', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' world is unfairly complex, and we don’t usually have a nice spike-in lying around for us to use, unfortunately 😢 . So we have to instead use relative data, or more specifically, the additive log-transformed (ALR) relative data. Or we can use centered log-transformation (CLR) if we are confident that the geometric mean of the counts does not change across samples (which we know does not hold for our simulated data here). In essence, the best we can do in such cases is to calculate the relationships between relative data. So, let’s compare the rho values for relative data (with respect to a chosen reference gene) against the correlations between true absolute counts. The plots', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content=' against the correlations between true absolute counts. The plots below show this for the correlation between relative counts calculated using 2 randomly chosen reference genes:', metadata={'Title': 'Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2'}),\n",
       " Document(page_content='Linear regression is arguably one of the most important and most used models in data science. In this blog post, I will walk you through the process of creating a linear regression model and show you some cool data visualization tricks.\\n\\nWe will be using the Ames Housing dataset, which is an expanded version of the often cited Boston Housing dataset. The dataset has approximately 1,700 rows and 81 columns, and the goal is to predict the selling price in the market.\\n\\nLet’s Begin!\\n\\nJust like any other project, the first step is to import all the libraries. You can import more libraries as you figure out all the tools you will need to build your model.\\n\\nNow, before we process or change the', metadata={'Title': 'Linear Regression in Python'}),\n",
       " Document(page_content='\\n\\nNow, before we process or change the data, let’s just get an idea of what we are dealing with.\\n\\nThe short description of the data set shows us that it contains various data types and null values. It’s important to remember this as we process the data and build our model.\\n\\nData Preprocessing\\n\\nDatasets in the real world are never perfect. There will always be missing values and outliers that skew the dataset, affecting the accuracy of our predictive model. That’s why it is always a good idea to clean up the data before you start building your model.\\n\\nEarlier, I had mentioned that this particular dataset had 1,700 rows and 81 columns. If', metadata={'Title': 'Linear Regression in Python'}),\n",
       " Document(page_content=' had 1,700 rows and 81 columns. If a value is missing in a particular column, it would be unwise to delete that whole row because we would be losing a datapoint in every other column as well. There are two ways to solve this issue:\\n\\nReplace every null value in a given column with the median value of that particular column. (This is only applicable to columns with numerical values) Calculate your statistics while ignoring all null values. (I’ll show you what methods to use later in the blog).\\n\\nI opted for the second method, so I left the null values as they were in my dataset.\\n\\nCalculating Outliers\\n\\nThere are multiple ways to calculate outl', metadata={'Title': 'Linear Regression in Python'}),\n",
       " Document(page_content='iers\\n\\nThere are multiple ways to calculate outliers— z-score, inter-quartile range (IQR), and the Tukey method are just a few methods out there. I chose to use the IQR. For all intents and purposes, I am assuming that you are familiar with the concepts of IQR, so I will only go over how to code it in Python. If you feel like you could use a short overview, this blog post does a pretty solid job of explaining the key ideas behind inter-quartile range.\\n\\nIn order to calculate the first and third quartile, I used the describe() function on the dataset.\\n\\nsummary = data_set.describe()\\n\\nThe describe()', metadata={'Title': 'Linear Regression in Python'}),\n",
       " Document(page_content='set.describe()\\n\\nThe describe() function yields a neat and concise data frame with important statistics from each numerical column of the original dataset. As you can see in the image above, I now have access to the mean, standard deviation, and percentile values with just one line of code!\\n\\nNow, I store each statistic for every column in a data series. This allows me to access the percentile values in all the columns iteratively.\\n\\nNow let’s dive into the deep end. I’ll first explain at a high level how I calculated the IQR before I dump my code onto here.', metadata={'Title': 'Linear Regression in Python'}),\n",
       " Document(page_content='300,000. That’s the number of people each year who are estimated to be infected with Lyme disease in the United States.\\n\\nBut even more concerning, only 10% of those cases get reported annually to the Center for Disease Control and Prevention (CDC). That means that there’s possibly 270,000 people who get infected with this disease every year and may not even know they have it.\\n\\nFigure 1: Number of Reported Cases of Lyme Disease to the CDC from 2000–2017 (Data Source)\\n\\nAnd the number of people affected is growing too. As seen in Figure 1, the annual cases of Lyme disease have more than doubled in the last 20 years. Mostly concentrated in the Northeast and upper Midwest,', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='. Mostly concentrated in the Northeast and upper Midwest, it continues to spread to new counties every year.\\n\\nAnd it’s all as a result of the Black-legged tick, also known as the Deer tick. If you get bitten by an infected tick, the bacteria, Borrelia burgdorferialone, will enter your blood stream and begin its quiet destruction of your immune system.\\n\\nWithin about a week, 70 to 80 percent of people get the characteristic “bull’s-eye” rash, known as Erythema migrans. And this is generally followed by typical flu-like symptoms, like a fever, headache, chills, and general fatigue.\\n\\nThis doesn’t', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' general fatigue.\\n\\nThis doesn’t seem like much, but if left untreated, it will cause widespread inflammation to almost every body system. Some of the symptoms include debilitating arthritis and swelling of the joints, facial paralysis, heart palpitations, shortness of breath, and swelling of the brain and spinal cord leading to severe nerve pain and memory loss.\\n\\nBut it doesn’t have to be this way. If diagnosed in the early stages, Lyme disease can be completely cured with certain antibiotics. It all comes down to increasing public awareness and educating healthcare providers in at-risk areas.\\n\\nThis is where machine learning comes in. My idea was to build a classification model that could predict which U.S. counties would', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' that could predict which U.S. counties would have a high incidence of Lyme disease. In doing so, counties at high risk could be informed by the CDC in advance in order to proactively take measures against the infection.\\n\\nAcquiring the Data\\n\\nBut here’s the issue. Funding for research of Lyme disease is disproportionately lower than other diseases. Thus, there is very little research and surveillance being done on Lyme disease at the moment and publicly available data is extremely limited. That meant that I would have to build my dataset completely from scratch.\\n\\nChallenge accepted. Let the data wrangling commence.\\n\\nIf you want to follow along with my code, check out my GitHub repository; I’ve organized', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' out my GitHub repository; I’ve organized it chronologically with this article for your convenience.\\n\\nTarget Variable\\n\\nFirst, I needed to engineer my target variable. Luckily, the CDC has data on the number of reported cases per county from 2000 to 2017. Although the disease is severely under-reported as previously mentioned, this still gives a good picture of the geographic distribution of the disease.\\n\\nFor consistency, I decided to restrict the data to only 2010–2017 because, as shown in Figure 1, there is an unusually large spike in 2009. Also, it is likely that data from many years ago is not as representative of today, so removing data before 2010 would likely be beneficial for the predictability of the model.\\n', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' beneficial for the predictability of the model.\\n\\nBut still, the data in this form won’t work for a classification problem. One issue is that the number of cases is heavily dependent on how many people live in each county. To solve this, I acquired county population estimate data from the Census Bureau. I then merged the datasets together and divided the number of cases per county by that county’s estimated population at the time.\\n\\nThis resulted in a rate of Lyme disease cases per person. Now we have a metric that allows us to more reasonably compare counties to one another.\\n\\nThere’s still one more step though. For supervised learning algorithms, the target variable must be labeled. I found out that the', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' variable must be labeled. I found out that the CDC defines a high incidence county as having 10 or more confirmed cases per 100,000 persons. This allowed me to bin the data such that anything above that cutoff was considered a high incidence county and anything below it was a low incidence county.\\n\\nFeatures\\n\\nOkay, but how do we get features for this model? I decided to dive into the research to find out.\\n\\nIn 1998, Stafford et al. showed that tick abundance had a strong positive correlation with Lyme disease infections. So you’d think that by now we would have a well-established national tick surveillance program.\\n\\nFigure 2: CDC Map Showing Geographic Distribution of the Black Legged Tick (Source)', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' Distribution of the Black Legged Tick (Source)\\n\\nWell, no. As shown above, all the CDC offers today is a qualitative map showing where the Black-legged Tick might be. Not very helpful.\\n\\nIn the last few years, some states, like Connecticut and North Dakota, have taken the initiative themselves to measure tick concentrations , but until there is a coordinated national surveillance program, nation-wide tick population data will not exist.\\n\\nLuckily, there is a good proxy for this. Khatchikian et al. showed that environmental factors such as extreme winter temperatures and summer and winter precipitation directly regulate tick population dynamics. This is likely a result of the climate’s effect on acorns. If the climate is optimal', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' effect on acorns. If the climate is optimal for acorn growth, mice and deer populations that eat them will thrive. The ticks that live off of these host animals will also flourish as a result.\\n\\nThis is great news, as there is a plethora of climate data available from the National Oceanic and Atmospheric Administration (NOAA).\\n\\nI soon found out though that this would be a little harder that expected because there is a limit of 10,000 API requests per day. Given that I needed data for every weather station in every U.S. county for 7 years, it would have taken me about 56 days to download all the data. Not ideal.\\n\\nI was able to get around this though. Fortunately, I', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' able to get around this though. Fortunately, I found a page that allowed access to every Global Summary of the Year file and I downloaded all of it to my local computer. Of course, this meant that I now had data for every station in the world starting from around 1900, which amounted to approximately 78,000 CSV files, each corresponding to a particular station.\\n\\nObviously, I only wanted a small percentage of that. So, I had to write code to manually parse through each CSV file.\\n\\nFigure 3: Example of NOAA Global Summary of the Year CSV Files\\n\\nAbove, you can see the general format of each file (there are many more columns that aren’t shown here). Essentially, I took the latitude and', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' shown here). Essentially, I took the latitude and longitude from each file and used that as input for the reverse-geocoder python library, which outputs the country, state and county of that location. If it was a U.S. station, I then took only the rows from 2010 to 2017 and appended them to a global dataframe with their associated state and county as new columns.\\n\\nAfter over 24 hours of parsing, I was left with a mess of data set. First of all, there were 209 columns with cryptic names, like “DX90” and “EMSD”. I used the provided documentation to determine the meaning of each feature and then through careful research, meticulously removed the ones that', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' then through careful research, meticulously removed the ones that weren’t relevant. I also relabeled the columns to have more understandable names.\\n\\nFigure 4: The Percentage of Missing Values for Each of the 35 Selected Features\\n\\nThe next problem was the large number of missing values as seen in Figure 4. For whatever reason, many of the stations were not consistent in recording and/or reporting their measurements.\\n\\nI didn’t want to get rid of columns because then I would have lost some of the information that could have been gleaned from the climate data. So instead, I chose to impute all the missing values.\\n\\nNow, this is an area where we have to be super careful. We need to', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' we have to be super careful. We need to put in a placeholder that would be reasonably representative of the actual value.\\n\\nFor example, let’s say that the total precipitation value for Montgomery County, Pennsylvania in 2017 is missing. It is reasonable to believe that the average total precipitation of all the counties in Pennsylvania will come close to Montgomery County’s actual precipitation. And so that’s what I did for all the missing values; I found the average for that county’s state in that particular column and imputed the value.\\n\\nHawaii and Washington D.C. had no data in several of the columns, so I had to remove them completely. Hawaii is geographically isolated and has no incidence of Lyme', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' Hawaii is geographically isolated and has no incidence of Lyme disease so there is no issue removing it. Washington D.C. is significantly smaller than any state so it should have no effect on the modeling.\\n\\nMerging the Target with the Features\\n\\nAlright, so now that the data was nice and clean, I was almost ready to merge the features with the target.\\n\\nSince I was trying to build a predictive model, I would need to use the previous year’s climate data to predict the current year’s Lyme disease incidence.\\n\\nTo do this, I created a new column in the features data that contained the next year. I then merged the target data on that column in order to create a one year offset', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' that column in order to create a one year offset.\\n\\nPreparation for Modeling\\n\\nClass Imbalance\\n\\nBefore any modeling, it’s important to check if the data will work well for the algorithms you’ll be using. In the case of classification models, the classes need to be relatively balanced. That means that there should be an equal number of instances for the positive and negative classes.\\n\\nFigure 5: Class Imbalance in Data\\n\\nAs seen above, this data has a significant class imbalance problem with 85% of the counties having low Lyme disease incidence and only 15% of the counties having high Lyme disease incidence.\\n\\nBut why is this such an issue? Well, if a majority of', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' such an issue? Well, if a majority of the counties have a low Lyme disease incidence, then a model will get the best accuracy by guessing that all the counties have a low incidence. In this case, 85% of the time the model would be right. But this comes at a terrible cost of 0% accuracy for the minority class. In other words, our model would be utterly useless for the counties we care about that have a serious Lyme disease problem.\\n\\nResampling Techniques\\n\\nI experimented with many resampling techniques to counteract this. Random undersampling is where you randomly select a small percentage of the majority class and delete the rest of the data points. This removal results in a balance of the majority class with the', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' results in a balance of the majority class with the minority class. But this comes with the obvious downside of losing information, which could reduce the model’s predictability.\\n\\nThen there’s random oversampling, which randomly selects a portion of the infrequent class and just duplicates those data points. This also results in balanced classes; the caveat here is that having a bunch of rows that are exactly the same could lead to overfitting, where the model just memorizes the over-represented synthetic data and loses its generalizability to real world unseen data.\\n\\nThat’s where Synthetic Minority Over-sampling Technique, also known as SMOTE, comes in. Instead of just copying parts of the minority', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' in. Instead of just copying parts of the minority class, it seeks to create data that are similar but not exactly the same as the original data. On a high level, it randomly generates new points directly between the infrequent class’s data points, leading to completely unique synthetic data.\\n\\nAnd lastly, there’s the Adaptive Synthetic Sampling Approach or ADASYN, which is like SMOTE but creates more synthetic data for minority class samples that are harder to learn and fewer synthetic data for minority samples that are easier to learn. This results in a distribution of synthetic data that will strengthen the model’s ability to distinguish between the classes at the decision boundary.\\n\\nSplitting the Data\\n\\nBefore', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='.\\n\\nSplitting the Data\\n\\nBefore we can see how these techniques perform, we must partition the data. I split the data into three parts: 60% for training the different models, 20% for validating and optimizing the best model, and 20% as a test set to demonstrate the generalizability of the final model.\\n\\nInitial Modeling\\n\\nNow we get to the fun part. I built a modeling pipeline using the imbalanced-learn python package in combination with scikit-learn’s GridSearchCV. This allowed me to do an exhaustive grid search for every combination of hyperparameters and data transformations (like scaling and resampling techniques), while also doing 5-fold cross validation to more', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' while also doing 5-fold cross validation to more robustly test how each combination performed.\\n\\nThe algorithms I ran through this pipeline were:\\n\\nI’ve provided links to helpful articles in case you are interested in learning more about each model.\\n\\nEvaluating Each Model’s Performance\\n\\nFigure 6: ROC Curves for Each Model\\n\\nAfter optimizing each algorithm individually, I tested the resulting five models on the validation set. I then plotted them against each other using the Receiver Operating Characteristic Curve, also known as the ROC curve, in Figure 6. The area under the ROC curve (ROC AUC) was used to compare the models.\\n\\nSimply put, this metric represents the', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='.\\n\\nSimply put, this metric represents the cost versus benefit of the model at every threshold, quantifying how good the model is at distinguishing between the classes. The best models will have curves that are close to the upper left corner and take up the most area in the plot.\\n\\nAs you can see in Figure 6, Random Forest (in pink) is superior to the other models at basically every threshold, with Support Vector Machines (in orange) coming in second; the models had ROC AUC scores of 0.947 and 0.934 respectively. A score of 1 means it perfectly predicts the data, so these are very respectable results.\\n\\nInterestingly enough, random oversampling produced the best results for both of', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' oversampling produced the best results for both of these models, which demonstrates that sometimes simplicity can outperform even the most sophisticated methods.\\n\\nOptimizing the Best Model\\n\\nFigure 7: Distribution of the Best Random Forest Model’s Outputs\\n\\nAbove is the distribution of the predicted probabilities outputted by the best Random Forest model. These numbers represent how likely it is that a particular county will have a high incidence of Lyme disease.\\n\\nThe outputs are right skewed indicating that the model is predicting that most the counties have a low probability of having a high incidence. This is exactly what we want to see, especially since the classes are imbalanced.\\n\\nRight now, the default threshold for the model is 0.5', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' the default threshold for the model is 0.5, meaning that any county with a probability above 0.5 will be classified as high incidence and any county below 0.5 will be classified as low incidence. But what if this threshold is not the most optimal?\\n\\nThat’s a major reason why I picked ROC AUC as my evaluation metric, as it is independent of threshold. This meant that I could then tune the threshold to optimize the model for the costs associated with making mistakes. I set aside an additional 20% of my data specifically for this purpose.\\n\\nSo now we need to determine these costs. This is arguably the most important step in the process as we are going to place the model in the context of', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' are going to place the model in the context of the real world, and not just in the vacuum of mathematics. I had to approximate the costs myself but ideally the stakeholder you are working with would be able to give you this information.\\n\\nAs I mentioned earlier, the idea behind this model is to give the CDC a tool to determine which areas they need to target their efforts. If a county is classified as having high Lyme incidence, the CDC would take two specific actions:\\n\\nIncrease public awareness in order to prevent infections from happening in the first place Provide educational resources for healthcare providers to increase the number of early diagnoses.\\n\\nFalse Positive Cost\\n\\nOkay, but what if the model predicts that a county is going to have a', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' model predicts that a county is going to have a high incidence of Lyme disease but it actually doesn’t? This is called a false positive. One implication of this is that people would needlessly limit their time spent outdoors and possibly even decide not to travel to these areas. This decreased outdoor recreation and tourism could hurt local economies.\\n\\nTo approximate this, I used the estimated total economic contributions that National Parks have on local communities, which is about $20.2 billion annually. Obviously, this doesn’t take into account state parks or any other services that would be affected, but it will work for now.\\n\\nI then tried to find research that quantified how consumer sales decreased during an established epidemic and there wasn’', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' decreased during an established epidemic and there wasn’t alot available. Chou et al. found that the SARS epidemic in Asia cost countries between .2% and 1.56% in consumer related industries. SARS is a very different disease but it’s pretty much all there is at the moment to go off of. I chose to use 1% to roughly approximate the economic cost.\\n\\nHere is how I arrived at the cost per county of a false positive:\\n\\nTotal Cost of a False Positive:\\n\\n(20.2 billion x .01) / $64,311 lost per county (20.2 billion x .01) / 3,141 counties\\n\\nFalse Negative Cost\\n\\nNow what about the', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='\\nFalse Negative Cost\\n\\nNow what about the cost of a false negative? That would be where our model predicts that a county won’t have a problem with Lyme disease when it really does.\\n\\nThis is obviously going to be way more expensive of a mistake. More people would needlessly get infected by Lyme disease due to lack of public awareness. And more people would suffer from chronic Lyme disease, as fewer doctors would be educated on early Lyme disease diagnosis.\\n\\nZhang et al. found that the average early Lyme disease patient costs $1,310 annually, while a chronic Lyme disease patient costs $16,199. That’s over 12 times more expensive. This was done in 2000, so I had to account', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' was done in 2000, so I had to account for an inflation factor of 1.41.\\n\\nAdditionally, Aucott et al. approximated that 64% of new Lyme disease cases each year were early while 36% were chronic. As previously mentioned, there are about 300,000 of these new cases every year.\\n\\nAlso, to simplify my calculation, I made the assumption that the CDC’s intervention would be completely effective, resulting in all the new Lyme disease cases being caught early.\\n\\nTo find the cost of a false negative, I would just need to find the cost difference between when the CDC does not intervene and when it does intervene. Here were all my calculations:\\n\\nNumber of Annual Early and Chronic', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' calculations:\\n\\nNumber of Annual Early and Chronic Cases:\\n\\n.64 x 300,000 = 192,000 cases of early Lyme disease\\n\\n.36 x 300,000 = 108,000 cases of chronic Lyme disease Inflation-Adjusted Cost of Early vs Chronic:\\n\\n$1,310 x 1.49 = $1,952 per patient with early Lyme disease\\n\\n$16,199 x 1.49 = $24,137 per patient with chronic Lyme disease Average Cost for High Incidence County Without CDC Intervention:\\n\\n192,000 early cases / 3,141 counties = 61 early cases per county\\n\\n61 early cases * $1,952 = $119,065 108,000 chronic cases /', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='119,065 108,000 chronic cases / 3,141 counties = 34 chronic cases per county\\n\\n34 chronic cases * $24,137 = $820,641 $119,065 + $820,641 = $939,706 Average Cost for High Incidence County With CDC Intervention:\\n\\n300,000 early cases / 3,141 counties = 95 early cases per county\\n\\n95 early cases * $1,952 = $185,430 ------------------------------ Total Cost of a False Negative:\\n\\n$939,706 - $185,430 = $754,276 lost per county\\n\\nFinding the Best Threshold\\n\\nFinally, I calculated the ratio of the costs between the false negatives and positives:\\n', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' the costs between the false negatives and positives:\\n\\nRatio of False Negative vs False Positive:\\n\\n$754,276 / $64,311 = 11.73\\n\\nAccording to this, false negatives are almost 12 times more costly than false positives. I then plugged this value into my code and found the optimal threshold was 0.17. This means that any county above 0.17 will be classified as high incidence and any below will be low incidence. Refer back to Figure 7 to visualize where that cutoff would be.\\n\\nFinal Results\\n\\nTo prove the generalizability of my final model, I trained the model on all 80% of the previously used data (the 60% training set and the 20% threshold optimizing set', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='% training set and the 20% threshold optimizing set) and then tested with the completely unseen 20% test set from the beginning.\\n\\nThe results were even better than during training with a ROC AUC of 0.961.\\n\\nFigure 8: Confusion Matrix of Final Model’s Outputs\\n\\nAdditionally, the model had a great recall of about 0.949, meaning that it correctly classified about 95% of the high incidence counties. As seen in Figure 8, it only had 28 false negatives out of the 554 high incidence counties.\\n\\nThis of course came at a cost to the precision which was 0.503. This means that only about 50% of the counties that the model predicted to be high', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content=' of the counties that the model predicted to be high incidence were actually high incidence. As seen in Figure 8, the false positives and true positives are roughly the same.\\n\\nBecause false negatives were so much more costly than false positives, these results make complete sense. It was necessary for the model to be imprecise in order for it to correctly classify as many of the high incidence counties as it did.', metadata={'Title': 'Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.'}),\n",
       " Document(page_content='So, what can we learn from our models? Well, first of all, our average price models seem to show us that avocado prices reach their peak each year in the Autumn. In October 2017, conventional avocados peaked at $1.65 and our model forecasts that Autumn spikes will continue to occur through 2018 and 2019. However, the model predicts that conventional avocado prices will trend downwards over the following two years. Between 2015 and 2018, the average conventional avocado price was $1.09. Between 2018 and 2020, the forecasted average price is $1.01, a decrease of about 7 percent.\\n\\nIn stark contrast, the model forecasts that organic avocado prices will experience moderate growth, bolstered by less volatility. The forecasted average', metadata={'Title': 'Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models'}),\n",
       " Document(page_content=', bolstered by less volatility. The forecasted average price between 2018 and 2020 is $1.84, compared to $1.55 between 2015 and 2018. The model certainly demonstrated a moderate upward trend for organic avocado prices.\\n\\nHowever, the growth in average prices is also due in part to decreased volatility, with the standard deviation of prices down to $0.16 for the forecasted period from the $0.36 standard deviation experienced between 2015 and 2018.\\n\\nBoth conventional and organic avocados are forecasted to experience strong growth in terms of units sold over the 2018–2020 period. Conventional avocado production is forecasted to grow over 40 percent from 33.7 million to 47.6 million avocados sold each week', metadata={'Title': 'Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models'}),\n",
       " Document(page_content=' 47.6 million avocados sold each week on average.\\n\\nOrganic avocados are forecasted to experience even more meteoric growth. A consistent growth trend between 2015 and 2018 is predicted to continue through 2019 with a whopping 74 percent growth between 2018 and 2020. Between 2015 and 2018, suppliers sold about 970,000 organic avocados weekly. Between 2018 and 2020, the number is forecasted to jump to 1.68 million organic avocados.\\n\\nDiscussion\\n\\nI think these models are really exciting. First, in terms of pattern recognition, it is interesting to see the models picking up on seasonal trends. The price models captured the Autumn price hikes and the volume models captured the spikes that occur in avocado volume each', metadata={'Title': 'Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models'}),\n",
       " Document(page_content=' models captured the spikes that occur in avocado volume each February.\\n\\nAdditionally, the diagnostics look pretty good. Mean Absolute Percentage Error, or MAPE, measures the error between the forecast and observed values, so a lower value indicates a better fitting model. The highest MAPE amongst the four models is for the organic volume model, at around 6 percent, indicating the model is about 94 percent accurate. I am very happy with this level of fit and I am confident in all of the models. At least, I am as confident as an forecasting newbie could be.\\n\\nIf I had to editorialize (which I will gladly do) I would say that the growth forecast for organic avocado volume is somewhat bullish. I am not sure how', metadata={'Title': 'Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models'}),\n",
       " Document(page_content=' volume is somewhat bullish. I am not sure how realistic 74 percent growth is over only two years, but testing several models yielded similar results.\\n\\nAlternatively, I think that the conventional avocado price model is somewhat bearish. With the Autumn price hike increasing each year since 2015, I was surprised that the model forecasted overall price decreases. However, if it ends up being the case through 2020, you will not see me complaining at brunch.', metadata={'Title': 'Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models'}),\n",
       " Document(page_content='How does Facebook define Terrorism in Relation to Artificial Intelligence?\\n\\nHow useful is the terminology terrorism? I would argue it is not useful because it obscures the specific debates into a reactionary pattern of violence against violence. However in a political science perspective this would to some degree be a social constructivist approach. Artificial intelligence being increasingly securitised will inevitably be mixed up in the policy process of these large social media companies. So let me explore how Facebook is addressing this issue.\\n\\nIn this article I will look at:\\n\\n(1) Facebook and its definition of terrorism;\\n\\n(2) into the stated approach to artificial intelligence;\\n\\n(3) Facebook’s growing security team;\\n\\n(4)', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='s growing security team;\\n\\n(4) the practical side and possible trauma of human moderation;\\n\\n(5) the question of a US-centric focus on terror on social media;\\n\\n(6) government requests for user data;\\n\\n(7) the coming creation of the global oversight board that may set a precedence for the use of AI for both organisations and governments;\\n\\n(8) vague Snapchat terrorism, a comparative outlook – an outro.\\n\\n1. Facebook and its Definition of Terrorism\\n\\nIn 2018 one of the largest social platforms on the planet decided to attempt defining terrorism, and it reads as the following:\\n\\n“Any nongovernmental organization that engages in premeditated acts of', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='al organization that engages in premeditated acts of violence against persons or property to intimidate a civilian population, government or international organization in order to achieve a political, religious or ideological aim.”\\n\\nIn the blog post made the 23rd of April 2018 called Hard Questions: How Effective Is Technology in Keeping Terrorists off Facebook? A central paragraph by my own approximation reads:\\n\\nThe democratizing power of the internet has been a tremendous boon for individuals, activists, and small businesses all over the world. But bad actors have long tried to use it for their own ends. White supremacists used electronic bulletin boards in the 1980s, and the first pro-al-Qaeda website was established in the mid-1990s. While the challenge of', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' the mid-1990s. While the challenge of terrorism online isn’t new, it has grown increasingly urgent as digital platforms become central to our lives. At Facebook, we recognize the importance of keeping people safe, and we use technology and our counterterrorism team to do it. [bold added]\\n\\nThe claims Facebook makes through this blog post:\\n\\nOur definition is agnostic to the ideology or political goals of a group. Our counterterrorism policy does not apply to governments. Facebook policy prohibits terrorists from using our service, but it isn’t enough to just have a policy. We need to enforce it.\\n\\nDespite making this claim they simultaneously say their focus lies on ISIS, al-Qaeda, and their affiliates — the groups', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' al-Qaeda, and their affiliates — the groups that currently pose the broadest global threat. However these are additionally of most interest and priority to the United States.\\n\\n2. How does Facebook use Artificial Intelligence to Counter Terrorism?\\n\\nThis blog post additionally refers to a post written by Facebook called Hard Questions: How We Counter Terrorism. It is written by Monika Bickert, Director of Global Policy Management, and Brian Fishman, Counterterrorism Policy Manager. This post was made already on the 15th of June 2017.\\n\\nThe top point of this post is Artificial Intelligence. We want to find terrorist content immediately, before people in our community have seen it. Facebook has clearly been using AI since at least 2017 to remove posts', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' been using AI since at least 2017 to remove posts associated with terrorism (they claim it was recent at the time). At the time they seemed to focus their efforts on ISIS and Al-Qaeda.\\n\\nImage matching: When someone tries to upload a terrorist photo or video, their systems look for whether the image matches a known terrorism photo or video. This way they can avoid people uploading the same video.\\n\\nLanguage understanding: Facebook had started to experiment with using AI to understand text that might be advocating for terrorism. They were at the time experimenting with removing text relating to what they had already seen as previously removed (historic data)\\n\\nRemoving terrorist clusters: Facebook claims to know from studies of terrorists that they tend to radicalize and operate', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' of terrorists that they tend to radicalize and operate in clusters. This offline trend is reflected online as well. They use signals like whether an account is friends with a high number of accounts that have been disabled for terrorism, or whether an account shares the same attributes as a disabled account.\\n\\nRecidivism: Facebook said they had gotten much faster at detecting new fake accounts created by repeat offenders. Through this work, they have been able to dramatically reduce the time period that terrorist recidivist accounts are on Facebook. They argue this process is ‘adversarial’ that the other party keeps developing new methods.\\n\\nCross-platform collaboration: Because they didn’t want terrorists to have a place anywhere in the family of Facebook', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' to have a place anywhere in the family of Facebook apps, they have began work on systems to enable us to take action against terrorist accounts across all our platforms, including WhatsApp and Instagram.\\n\\nIn the first quarter of 2018 they reported to have taken down 837 million pieces of spam and 2.5 million pieces of hate speech and disabled 583 million fake accounts globally. This was in relation to the statement saying it was assisted by using technology like: “…machine learning, artificial intelligence and computer vision..” to detect ‘bad actors’ and move more quickly. They mentioned this particularly in relation to the election.\\n\\nIn 2019 They removed what they call ‘inauthentic behaviour from Iran, Israel and Russia', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='uthentic behaviour from Iran, Israel and Russia (focused on Ukraine) in particular.\\n\\nLive-streamed attacks like Christchurch shooting require human moderation. LeCun said at a recent event that Facebook was years away from using AI to moderate live video at scale. LeCun the problem with the lack of training data. “Thankfully, we don’t have a lot of examples of real people shooting other people,” you could train for recognition of violence using footage from movies, but then content containing simulated violence would be inadvertently removed along with the real thing.\\n\\nAutomated systems are claimed to be used mainly as assistants to human moderators.\\n\\nAI is not a silver bullet to moderation. Understanding artificial intelligence', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' not a silver bullet to moderation. Understanding artificial intelligence in this context is of course not enough. Facebook has a community operations team that has to distinguish from a personal profile or a news story. This ‘more nuanced approach’ requires human expertise. Understanding how Facebook uses artificial intelligence is of course not enough without understanding how their actual safety and security team manages these tools as well as frameworks.\\n\\n3. Facebook’s Growing Safety and Security Team\\n\\nFacebook feed, since the company’s 200-person counterterrorism team removed them. (In the wake of the Cambridge Analytica privacy scandal, Facebook is under pressure to show that it can police itself.) Reported in 2018.\\n\\nFacebook was scheduled to be growing by 3,', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='\\nFacebook was scheduled to be growing by 3,000 people over 2017— that work 24 hours a day and in dozens of languages to review these reports and determine the context. The link refers to a post made by Mark Zuckerberg stating that they already have 4,500 people hired in addition to those they had scheduled to hire.\\n\\nIn July the 6th 2018 (updated the 4th of December) Ellen Silver from Facebook as VP of operations claimed to be scaling globally, covering every time zone and over 50 languages. They had also rapidly grown their staff in safety and security:\\n\\n“The teams working on safety and security at Facebook are now over 30,000. About half of this team are content reviewers — a mix of full-', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' team are content reviewers — a mix of full-time employees, contractors and companies we partner with.”\\n\\n4. Insecurity Causing Trauma for Facebook Workers\\n\\nIn February 2019 The Verge published an article called The Trauma Floor: The secret lives of Facebook moderators in America. This article does of course describe the challenging conditions in which these moderators work, however it also mentions a stat of 15,000 moderators working around the world. It did seem rather a few of these were subcontracted through companies such as Cognizant having to sign NDAs, with secrecy supposedly protecting employees.\\n\\n“Collectively, the employees described a workplace that is perpetually teetering on the brink of chaos. It is an environment', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' on the brink of chaos. It is an environment where workers cope by telling dark jokes about committing suicide, then smoke weed during breaks to numb their emotions. It’s a place where employees can be fired for making just a few errors a week — and where those who remain live in fear of the former colleagues who return seeking vengeance.”\\n\\nIt is perhaps ironic that in attempting to handling terror there is a degree of trauma caused to the handlers. Certain of they key findings by the report by The Verge seems interesting to stress or at least consider:\\n\\nModerators in Phoenix will make just $28,800 per year — while the average Facebook employee has a total compensation of $240,000.\\n\\nEmployees are', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' $240,000.\\n\\nEmployees are micromanaged down to bathroom breaks. Two Muslim employees were ordered to stop praying during their nine minutes per day of allotted “wellness time.”\\n\\nModerators cope with seeing traumatic images and videos by telling dark jokes about committing suicide, then smoking weed during breaks to numb their emotions. Moderators are routinely high at work.\\n\\nEmployees are developing PTSD-like symptoms after they leave the company, but are no longer eligible for any support from Facebook or Cognizant.\\n\\nEmployees have begun to embrace the fringe viewpoints of the videos and memes that they are supposed to moderate. The Phoenix site is home to a flat Earther and a Holocaust denier', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' a flat Earther and a Holocaust denier. A former employee tells us he no longer believes 9/11 was a terrorist attack.\\n\\nAccording to the article these centres operate through accuracy standards which means posts reviewed are being reviewed. Facebook has set a goal of 95% accuracy, but Cognizant is usually never that high (closer to 80–92%). A moderator must suggest the correct community standard violation or risk loosing accuracy. The Verge article mentions a few different set of truths that a moderator has to consider.\\n\\nCommunity Guidelines, publicly posted ones and internal documents. Known Questions. A 15,000-word second document with commentary. Discussions amongst moderators attempting to reach a consensus. Facebook’s own internal tools', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' a consensus. Facebook’s own internal tools for distributing information.\\n\\nFurther it is said that the challenge of keeping a job may be rather difficult: “The job resembles a high-stakes video game in which you start out with 100 points — a perfect accuracy score — and then scratch and claw to keep as many of those points as you can. Because once you fall below 95, your job is at risk.”\\n\\nFired employees regularly threatened to return to work and harm their old colleagues. An NDA usually seem to stop you from talking about the work you were doing or even state that you ever worked for Facebook, according to The Verge: “They do the work as long as they can — and when', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' the work as long as they can — and when they leave, an NDA ensures that they retreat even further into the shadows. To Facebook, it will seem as if they never worked there at all. Technically, they never did.”\\n\\nFacebook has a clear idea of how their policies should be managed:\\n\\n“We want to keep personal perspectives and biases out of the equation entirely — so, in theory, two people reviewing the same posts would always make the same decision.”\\n\\nIn a statement that contradicts the article by The Verge they state: “A common misconception about content reviewers is that they’re driven by quotas and pressured to make hasty decisions.” They is stated to have', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='asty decisions.” They is stated to have four clinical psychologists across three regions who are tasked with designing, delivering and evaluating resiliency programs. Yet it is questionable whether this decentralised mental care without professionals on-the-ground is advisable given the work these employees have to go through.\\n\\n5. US-Centric Global Moderation of Terror\\n\\nWe can ask a simple question: when policy and guidelines are designed in US for the world what perspectives are prevalent in the given framework? As you may have guessed for the section title I am sceptical whether a universal framework based on one location can work well across the planet.\\n\\nTheir enforcement have focused heavily on Islamic Terrorist groups rather than right-wing extremism or other forms', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' groups rather than right-wing extremism or other forms of ‘terror’. They have had a partnership with Microsoft, Twitter and YouTube on hashes relating to terrorist content. These are all companies based in the United States.\\n\\nCounterspeech programs. Facebook support several major counterspeech programs. For example, last year we worked with the Institute for Strategic Dialogue to launch the Online Civil Courage Initiative. The project challenge was to design, pilot, implement and measure the success of a social or digital initiative, product or tool designed to push back on hate and violent extremism. Reportedly it engaged with more than 100 anti-hate and anti-extremism organizations across Europe.\\n\\nThey’ve also worked with Affinis Labs', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='They’ve also worked with Affinis Labs to host hackathons in places like Manila, Dhaka and Jakarta, where community leaders joined forces with tech entrepreneurs to develop innovative solutions to push back against extremism and hate online.\\n\\nWe want Facebook to be a hostile place for terrorists.\\n\\nSaying this they quoted the 1984, the Irish Republican Army (IRA) statement after a failed assassination: “Today we were unlucky, but remember that we only have to be lucky once — you will have to be lucky always.” In one way the statement resounds, yet you cannot avoid everything forever. If there is no room for failure, then any smudge on the perfect surface can stain the image — of course this', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' perfect surface can stain the image — of course this is important for Facebook. We can ask whether this decision of decentralised moderation makes it easier to blame external actors for any ‘externalities’ relating to safety and security.\\n\\n6. Government Requests for User Data\\n\\nIt is of course possible to access Facebook’s data if there is a security event that requires access. Government requests for user data increased globally by 7% from 103,815 to 110,634 in the second half of 2018. With the United States continues to submit the highest number of requests, followed by India, the United Kingdom, Germany and France. This reflected a normal growth according to Facebook.\\n\\nAs part of the requests 58% included', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='\\n\\nAs part of the requests 58% included a non-disclosure order prohibiting Facebook from notifying the user. In an internal review of their US national security reporting metrics Facebook found that it had undercounted requests from the Foreign Intelligence Surveillance Act (FISA). Facebook divides these requests into emergency requests and legal processes.\\n\\nFacebook may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.\\n\\nIt may be useful to understand these two different data requests:\\n\\nLegal Process Requests: Requests we receive from governments that are accompanied by legal process, like a search warrant. We disclose account records solely in accordance with our terms of service and applicable', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' solely in accordance with our terms of service and applicable law.\\n\\nEmergency Disclosure Requests: In emergencies, law enforcement may submit requests without legal process. Based on the circumstances, we may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.\\n\\n“Government officials sometimes make requests for data about people who use Facebook as part of official investigations. The vast majority of these requests relate to criminal cases, such as robberies or kidnappings”\\n\\nDuring this period Facebook and Instagram took down 2,595,410 pieces of content based on 511,706 copyright reports; 215,877 pieces of content based on 81,243 trademark reports; and 7', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' based on 81,243 trademark reports; and 781,875 pieces of content based on 62,829 counterfeit reports.\\n\\nFacebook recently started partnering with ethics institutions focused on artificial intelligence. The focus of this partnership seem to be in the direction of safety, at least in Munich the Institute they have partnered with will address issues that affect the use and impact of artificial intelligence, such as safety, privacy, fairness and transparency. I have previously described that this can be problematic: an issue of self-policing ethical conduct.\\n\\n7. The Global Oversight Board Ensuring a Global Perspective\\n\\nFacebook is creating a global oversight board. In a post by Nick Clegg, the new VP of Global Affairs and Communications in January 2019 a', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' VP of Global Affairs and Communications in January 2019 a draft charter was released. The draft lists 11 questions alongside considerations and suggested approaches. More recently in late June 2019 another post was made by Facebook on this topic.\\n\\nIt was stated they (Facebook) had traveled around the world hosting six in-depth workshops and 22 roundtables attended by more than 650 people from 88 different countries. They had personal discussions with more than 250 people and received over 1,200 public consultation submissions.\\n\\nSubsequently a 44-page report was released by Facebook called Global Feedback & Input on the Facebook Oversight Board for Content Decisions. This talks of a global constitution, board membership, content decisions and governance. Nick Clegg states in the introduction:\\n\\n', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' Nick Clegg states in the introduction:\\n\\n“Our task is to build systems that protect free expression, that help people connect with those they care about, while still staying safe online. We recognize the tremendous responsibility we have not only to fairly exercise our discretion but also to establish structures that will evolve with the times. Our challenge now, in creating this Oversight Board, is to shore up, balance, and safeguard free expression and safety for everyone on our platforms and those yet to come onto them, across the world.”\\n\\nThe report argues that there needs to be more democracy in Facebook. There needs to be a system to appeal decisions. The report gives different examples of moderation. It also states that Facebook undertook research to study the', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' It also states that Facebook undertook research to study the range of oversight models that exist globally which identified six ”families“ of oversight design. The grid they presented looks like this.\\n\\nAccording to the report public reason giving will be a crucial feature of the Oversight Board, one which drives at the heart of the legitimacy of its decisions.\\n\\nThe Draft Charter suggests that Facebook will select the first cohort of members, with future selection to be taken over by the Board itself. The report stated that questioned were raised to this proposal of leaving future selection up to the Board itself, as this could result in both a “recursion problem” and possibly the “perpetuation of bias.” A few approaches were suggested', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' of bias.” A few approaches were suggested for membership in the board:\\n\\nMembership be left to a fully democratic vote by Facebook users. A hybrid approach, combining selection procedures so that Facebook, outside groups, and users could all participate. Soliciting public comment on a slate of applicants. Inviting civil society groups to select some of the Board members. Asking governments to weigh in on names and candidates. Opening a public nomination process. A randomised lottery system to select members from Facebook users.\\n\\nThere was an agreed importance of diversity, though it was mentioned that perfect representation is not possible. It was mostly agreed that Facebook employees (current and former) should be excluded from the Board. It was suggested a fixed term', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' from the Board. It was suggested a fixed term of three years, renewable once.\\n\\nIn the report it is suggested two ways to submit both for Facebook to send important or disputed content and for the users. Facebook has proposed that smaller panels, not the Board as a whole, will hear and deliberate on cases. It was clear that: “A strong consensus emerged that the Board’s decisions should influence Facebook’s policy development.”\\n\\nIt was noted that Facebook was to establish an independent trust to remunerate (pay) board members. It was argued this board needed its own staff and that these be wholly independent of Facebook. The scope for the board will be content governance. However it was indicated that the', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' be content governance. However it was indicated that the board could hear other policy issues, such as: “…algorithmic ranking, privacy, local law, AI, monetization, political ads, and bias.”\\n\\nThus it can be said that Facebook and the field of artificial intelligence may be rather influenced by decisions made by this board in the future should it possibly be established. Indeed considering the scale of Facebook this can both influence private companies to adopt certain practices or nations to make legislation based on the decisions made by this semi-independent council. In the conclusion of the internal report it is stated:\\n\\n“Facebook finds itself in a historically unique position. It cannot deprive or grant anyone the freedom of expression, and yet', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' or grant anyone the freedom of expression, and yet it is a conduit through which global freedom of expression is realized.”\\n\\nVague Snapchat Terrorism? A comparative look — an outro\\n\\nIn their Community Guidelines Snapchat does not define terrorism, yet they write: “Terrorist organizations are prohibited from using our platform and we have no tolerance for content that advocates or advances terrorism.” Yet we may ask ourselves two questions: what is a terrorist organisation and what does advocating terrorism mean in practice if it remains undefined? You could take the: “I know terrorism when I see it”-approach yet that leaves a lot up to ambiguous choices without transparency of decisions involved. This seems part of the wicked problem of terrorism', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='. This seems part of the wicked problem of terrorism: definitions.\\n\\nTerrorism in international politics is hard to define, and how you define it may also says a lot about how you think about politics more broadly. Although it is notoriously difficult to define it may be one of the future discussions to be undertaken should an oversight board from Facebook appear. The focus that Facebook has had on Islamic terror as opposed to right-wing extremism or gun violence in the United States is a worrying example. Yet their move to establish a board may be an appropriate response.\\n\\nThe policing or ways that different governments request user data should continue to be under strong scrutiny with transparency. The state is an actor that can inflict violence; state-inflicted violence can be', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' inflict violence; state-inflicted violence can be ambiguous, particularly when there are claims to state-sponsoring of terrorism. Most certainly the state can act using terror, and it is occurring, so does terrorist have to be in a minority group; is it genocide or terror; and does this distinction matter?\\n\\nTerror in some cases is about scaring people — violence is used in a restrictive case. Is it illegitimate use of violence by non-state actors aiming for the spread of ideology? If so whose ideology in a board run by Facebook, and the concerns of diversity is real. When to justify intervention and not alongside how it is justified may be important as pragmatic definitions arise as products of the prevailing interests.\\n\\nWhen is an act', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content=' the prevailing interests.\\n\\nWhen is an act of violence a weapons of the weak in asymmetrical power distributions? What is the difference between narco traffickers and large resource interests that funds political power? The goals aspect is worth considering: knowing someone’s intention, yet the environment that shapes this intention is equally as important. Moderating terror in terrible working conditions is just one example of many.\\n\\nIf we take seriously that we are individuals with ideas, there are some patterns, but a lot of it is quite hard to predict. If it is hard to predict human behaviour then it is hard to know people’s aims and quite difficult to see the people’s intention.\\n\\nWhere is the money coming from? We', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='\\n\\nWhere is the money coming from? We have data brokers and there is not currently enough regulation to ensure that the flow of data is responsible or is sold of unintentionally to groups intending to use the data for such purposes. Terrorism obscures — it is not a value-neutral term. Technology is not value-neutral at all. It ties into ideas of securitization and state powers alongside its ethical discourse of technological for good.\\n\\nSlapping the terrorist label puts it into a different category. Understanding can be an important tool in how to prevent it. Putting the T-word on it is tempting in a rapid pace of content moderation, yet we need to engage with it.\\n\\nAs much as there is a need to be', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='\\nAs much as there is a need to be respectful of the way large companies are trying to moderate and cooperate with state institutions we need to be critical. Robert Cox said it well: “Theory is always for someone or for something.” In this respect perhaps technology is always for someone or something too. I will end with a video that was shared in my class today that proposes a critical view on the labelling of terrorism:', metadata={'Title': 'How does Facebook define Terrorism in Relation to Artificial Intelligence?'}),\n",
       " Document(page_content='Whenever you are manipulating data, the very first thing you should do is investigating relevant statistical properties. In particular, you might be interested in knowing whether your data follow a known distribution.\\n\\nWhy is this important? Think about the goal of your data analysis: once you are provided with a sample of observations, you want to compute some statistics (i.e. mean, standard deviation…) as well as build confidence intervals and conduct hypotheses tests. To do so, you need to assume your data to be following a known distribution, such as Normal, X-square or T-student.\\n\\nUnfortunately, most of the time your data are presented to you without having a known distribution, hence you don’t know the shape of their density', metadata={'Title': 'Bootstrap sampling'}),\n",
       " Document(page_content=' don’t know the shape of their density function. Here Bootstrap sampling comes to aid: the aim of this technique is assessing stats and properties of a potential distribution without actually knowing its shape.\\n\\nHow does it work? Imagine you are provided with a set of data (your population) and you get a sample of size n of them.\\n\\nNow, if you proceed with a re-sampling of your initial sample for B times (you can set B as large as you want. In general, it is set equal to or greater than 10000), you will generate B further samples, each with length n (with the possibility of one or more values to be repeated).\\n\\nNow, for each sample, you can compute', metadata={'Title': 'Bootstrap sampling'}),\n",
       " Document(page_content='\\nNow, for each sample, you can compute the estimation of the parameter you are interested in. It will be a generic function of each sample T(x^1*) and we will refer to it as θ̂1*.\\n\\nNow, the idea is that, if we collect all the statistics we computed, we can generate an approximation of the probability function of our initial population. One standard choice for an approximating distribution is the empirical distribution function of the observed data.\\n\\nIn statistics, an empirical distribution function is the distribution function associated with the empirical measure of a sample. It is a cumulative distribution which jumps…', metadata={'Title': 'Bootstrap sampling'}),\n",
       " Document(page_content='Photo by Mario Gogh on Unsplash\\n\\nIntroduction\\n\\nWe know that larger companies contain more than thousand employees working for them, so taking care of the needs and satisfaction of each employee is a challenging task to do, it results in valuable and talented employees leave the company without giving the proper reason.\\n\\nEmployee churn is a major problem for many firms these days. Great talent is scarce, hard to keep and in high demand. Given the well-known direct relationship between happy employees and happy customers, it becomes of utmost importance to understand the drivers of employee dissatisfaction.\\n\\nThis post emphasizes on predicting retention of an employee within an organization such that whether the employee will leave the company or continue with it. It uses the data of', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content=' or continue with it. It uses the data of previous employees who have worked for the company and by finding a pattern it predicts the retention in the form of yes or no.\\n\\nThe parameters we are using, such as salary, number of years spent in the company, promotions, number of hours, work accident, financial background, etc. Through this paper, an organization can choose its strategies to keep great representatives from leaving the organization. The data has 14,999 examples (samples). Below are the features and the definitions of each one:\\n\\nsatisfaction_level: Level of satisfaction {0–1}.\\n\\nlast_evaluationTime: Time since last performance evaluation (in years).\\n\\nnumber_project: Number', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content='in years).\\n\\nnumber_project: Number of projects completed while at work.\\n\\naverage_montly_hours: Average monthly hours at the workplace.\\n\\ntime_spend_company: Number of years spent in the company.\\n\\nWork_accident: Whether the employee had a workplace accident.\\n\\nleft: Whether the employee left the workplace or not {0, 1}.\\n\\npromotion_last_5years: Whether the employee was promoted in the last five years.\\n\\nsales: Department the employee works for.\\n\\nsalary: Relative level of salary {low, medium, high}.\\n\\nThe source code that created this post can be found here.\\n\\nData Set\\n\\n', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content=' be found here.\\n\\nData Set\\n\\nloading the data into the data frame and separating the result column.\\n\\nFinalCode.py hosted by GitHub\\n\\nData Preprocessing\\n\\nThe dataset has ‘salary’ and ‘sales’ column as categorical data, So we have to perform OneHotEncoding & LabelEncoding to convert this data into numerical form and To create dummy features we have to drop the first one to avoid linear dependency where some learning algorithms may struggle.\\n\\nAfter that, we will split the data into training and testing datasets.\\n\\nPreprocessing\\n\\nRegression Models\\n\\nBecause we want results in the form of ‘Yes’ or ‘No’', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content='�Yes’ or ‘No’ such that whether an employee will leave the company or not, So the best suitable regression model is Logistic Regression for this dataset. Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign.\\n\\nTo calculate the accuracy of the result our model has generated we will be going to use Confusion Matrix as an evaluation parameter.\\n\\nLogistic Regression\\n\\nClassifiers\\n\\nA decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content=' feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. It partitions the tree in a recursive manner call recursive partitioning.\\n\\nRandom forests is a supervised learning algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests create decision trees on randomly selected data samples, gets a prediction from each tree and selects the best solution through voting. It also provides a pretty good indicator of the feature importance.\\n\\nHere in our dataset, we will use these two classifiers to classify our result in the form', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content=' two classifiers to classify our result in the form of Yes and No.\\n\\nDecision Tree and Random Forest Classifier\\n\\nConclusion\\n\\nFactors affecting the employment\\n\\nHere in the above graph numbers on x-axis from 0 to 6 are representing WithHigherProjects,WithLowerSalary,WithHigherTime,WithPromotion,WithWorkAccident,WithorNotWorkAccident,WithoutWorkAccident. Each of these are the factors which can affect employment as WithHigherTime represents, employees who have more than four year of work experience but still haven’t got any promotion is 1750 which is a significant amount, WithLowerSalary represents employees whose salary level is low even when their evaluation score was higher', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content=' level is low even when their evaluation score was higher than 3 such employees are 750.\\n\\nThus after evaluating this dataset, we get to know that lower salary levels, no promotions even when employees are working more than 4 years are the two main reasons for the employees to leave the organization.', metadata={'Title': 'Predict Employee Retention'}),\n",
       " Document(page_content='Regularization for Machine Learning Models\\n\\nA common problem in machine learning is overfitting, where a model falsely generalizes noise in the training data:\\n\\nA popular approach to remedy this problem and make the model more robust is regularization: A penalty term is added to the algorithm’s loss function. This changes the model’s weights which result from minimizing the loss function.\\n\\nThe most popular regularization techniques are Lasso, Ridge (aka Tikhonov) and Elastic Net. For the exemplary case of simple linear regression with only one weight parameter w (the slope of the linear fit), their penalty terms look like this (including a scaling parameter λ):\\n\\nLasso (L1) : λ', metadata={'Title': 'Regularization for Machine Learning Models'}),\n",
       " Document(page_content='\\nLasso (L1) : λ·|w|\\n\\n: λ·|w| Ridge (L2) : λ·w²\\n\\n: λ·w² Elastic Net (L1+L2): λ₁·|w| + λ₂·w²\\n\\nThe different terms have different effects: Compared to L1, the quadratic L2 regularization becomes negligible at small weights (close to zero), but stronger at large weights. This leads to the following behaviours, casually phrased:', metadata={'Title': 'Regularization for Machine Learning Models'}),\n",
       " Document(page_content='These days I have been obsessed with researching data maps. I challenged myself and made a data map using Excel. The graphic below is the dynamic map of Hurricane Irma that I have drawn with Excel.\\n\\nIf you are interested, I will be happy to share with you the process of making a hurricane map with Excel. It is to use the bubble chart to outline the dynamic path of the hurricane and to show the change in wind strength. Here are the specific steps for making a hurricane map.\\n\\n1. Prepare Materials\\n\\n① Find a map of the US Atlantic that includes lines of latitude and longitude.\\n\\nNote that the map we need must have latitude and longitude lines. Many maps provided on the public network are', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' lines. Many maps provided on the public network are inaccurate. I downloaded this map from National Hurricane Center and Central Pacific Hurricane Center.\\n\\n② Download the storm track statistics of Hurricane Irma from Weather Underground, including date, time, latitude, longitude, wind, and so on.\\n\\n2. Process Data\\n\\n① Remove the units of data such as latitude, longitude, wind speed, wind pressure, etc. And the date and time should be converted into a format that is easy for Excel to process.\\n\\n② We can see that the hurricane statistics are recorded every six or three hours in the data material. Here we keep the data recorded every six hours.\\n\\n3. Draw a Bubble', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' six hours.\\n\\n3. Draw a Bubble Chart\\n\\n① Insert the above map into the table, and then draw a bubble chart on it. The X axis of the bubble chart represents the longitude data, the Y axis represents the latitude data, and the bubble size depends on the value of the wind pressure.\\n\\n② Design the format of Chart Area. The maximum and minimum values of the coordinate axes in the bubble chart are set according to the latitude and longitude readings. And make the spacing on the coordinates coincide with the spacing of the latitudes and longitudes on the map. Here I set the spacing to 5, which ensures that the data points drawn by the bubble chart match the actual latitude and long', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' by the bubble chart match the actual latitude and longitude on the map.\\n\\n③ Drag and drop the border of the plot area to make it coincide with the coordinate axes in the bottom Atlantic map. Then hide the axis data and set the border to “No line”.\\n\\n4. Make Dynamic Effects\\n\\nTo achieve the dynamic effects of the hurricane trajectory, we can use Slider Control to control the time. Each time we slide, the time is increased by 6 hours. The cell linked by the slider passes the data to the table area on the left side of the chart. And then it obtains the corresponding latitude and longitude data and wind data by querying the time point. At the same time', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' querying the time point. At the same time, two sets of data sources of the chart are generated from this time data. The entire control chain is thus formed and the final control of the chart by the slider is completed.\\n\\nFurther than that, if I want to implement automatic control of the slider, I will need VBA code to make the chart automatically show dynamic effects.\\n\\nOperation Process:\\n\\n① Write two macros with VBA. One macro controls the start, the other controls the stop.\\n\\n② Draw 4 controls, representing start, stop, loop, and slider. Specify the above macros.\\n\\n③ The format of the slider control specifies the left cell, which records', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' the slider control specifies the left cell, which records the sliding data. And the upper cell “Data & Time” calls the data of this cell.\\n\\n④ Prepare dynamic data.\\n\\nIn order to achieve the above dynamic effects, two sets of data are actually prepared in the bubble chart. One set shows all the path points that the hurricane has traveled before the current time point, and the other set marks the location of the hurricane at the current time point.\\n\\nHere we need to use the LOOKUP function and call the data of the cell “Data & Time”.\\n\\nFor the first set of data, we select all the data less than or equal to the current time point and set the', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' or equal to the current time point and set the other data points to #N/A.\\n\\nFor the second set, we select the data that matches the current time, and the other data is also set to #N/A. Here, #N/A doesn’t display data points in the chart.\\n\\n⑤ Bind the bubble chart to the data source.\\n\\nIn the final step, we bind the bubble chart to the data source and the hurricane trajectory map is complete.\\n\\nThe textbox in the chart can get the information of the data points directly from the cells by linking with them. In addition to the bubble chart, a set of column charts is added to the map to show the wind', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content=' charts is added to the map to show the wind, and its data binding operation form is the same as that of the bubble chart.\\n\\nWell, the general idea of making a hurricane map with Excel is like this. Below is an official map of Hurricane Irma. Is it very similar to the one I made with Excel?\\n\\nFrom Weather Underground\\n\\nTips\\n\\nExcel is very powerful, but if you want to use it to make some complicated charts, you must have a code base and learn VBA language, which is time consuming. I shared an article 4 Uses of Data Maps in Business Analysis, in which all maps are made with the zero-code visualization tool FineReport. The operation is very simple. If you don', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content='. The operation is very simple. If you don’t understand the code, you can try this tool to create data visualization charts.\\n\\nYou might also be interested in…\\n\\nTop 16 Types of Chart in Data Visualization\\n\\nHow Can Beginners Design Cool Data Visualizations?\\n\\nA Beginner’s Guide to Business Dashboards', metadata={'Title': 'I Made a Dynamic Hurricane Map with Excel!'}),\n",
       " Document(page_content='Trees in data science\\n\\nOne of the most easy to interpret models in machine learning are CART’s(Classification and Regression Trees) known popularly as decision trees. In this post I wish to give an overview of Decision Trees, some primary concepts surrounding them and finally Random Forests. The contents are as follows\\n\\nUnderstanding Decision Trees\\n\\nPurity\\n\\nBootstrapping and Bagging\\n\\nRandom Forests\\n\\nLets Go!\\n\\nDecision Trees\\n\\nBasic structure of a Decision Tree (Source: cway-quan)\\n\\nIn the machine learning universe trees are actually upside down versions of real trees. Suppose we have a dataset consisting of our features ‘X’ and', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content=' consisting of our features ‘X’ and a target ‘Y’. What a decision tree does is that it finds patterns within X and splits the dataset into smaller subsets based on these patterns.\\n\\nVisualize these splits in the slightly simplified image above. ‘Y’ here is whether or not a job offer is to be accepted. The ‘X’ contains features like “commute_time”, “salary”, “free_coffee”.\\n\\nBased on patterns in ‘X’ the tree is split into branches until it reaches a point where it arrives at a pure answer to ‘Y’. In our scenario, job offers', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content='Y’. In our scenario, job offers which are accepted have to provide a salary > 50k, commute time < 1hr and free coffee. In this manner the tree reaches the last leaf which is a pure decision about ‘Y’.\\n\\nPurity in decision Trees\\n\\nDecision trees conduct splitting based on the purity of the node. This purity is measured based on the distribution of ‘Y’. If our ‘Y’ is continuous our problem is a regression problem and the nodes are split based on MSE(Mean Squared Error). If ‘Y’ is discrete, our model is dealing with a classification problem and a different measure of purity is required.\\n\\nA widely', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content=' measure of purity is required.\\n\\nA widely used metric to measure in classification cases is Gini Impurity. The formula for Gini impurity is given as follows:\\n\\nSource: General Assembly DSI curriculum (Authors:David Yerrington, Matt Brems)\\n\\nWhile deciding which split to make at a given node, it picks the split that maximizes the drop in Gini impurity from the parent node to the child node.\\n\\nBootstrapping and Bagging\\n\\nTo understand bootstrapping and bagging, the first step would be to understand why they are needed in the first place. It is basically trying to emulate the “wisdom of the crowd” principle where in the aggregated', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content=' the crowd” principle where in the aggregated result of multiple models is better than the results of a single model. The following image by Lorna Yen gives a great idea about boot strapping.\\n\\n(Author: Lorna yen, Source)\\n\\nBootstrapping as shown above is just the random sampling of data with replacement. Bagging is just a process of building decision trees on each of these samples and getting an aggregate prediction. So to summarize Bagging involves the following steps:\\n\\nFrom the original data of size n, bootstrap k samples of size n with replacement Build a decision tree on each bootstrapped sample. Pass test data through all trees and develop one aggregate prediction\\n\\nBagging is therefore also', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content=' one aggregate prediction\\n\\nBagging is therefore also called Bootstrapped Aggregating.\\n\\nRandom Forest Model\\n\\nA closer look at the below image gives a basic intuition on random forests.\\n\\nA basic hurdle in bagging is that the individual decision trees are highly correlated as the same features are used in all trees. So the predictions of our models suffer from the issue of variance. To know more on variance or bias you can read this link. Decorrelating our models is a solution and is exactly what Random Forests do.\\n\\nRandom forests follow similar steps in bagging except that they use at each split in the learning process, a random subset of the features. This mitigates the variance problem in bagging', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content=' This mitigates the variance problem in bagging and generally produces much better results. This efficient and simple methodology has made Random forests a widely implemented Machine learning Model.\\n\\nBonus\\n\\nCode for importing the explained three classification models in sklearn.', metadata={'Title': 'Trees in data science'}),\n",
       " Document(page_content='Missing data points can sometimes feel like missing jigsaw pieces (photo: author)\\n\\nFor a few months now, we’ve been looking at different ways to use SQL in data science — most recently on using SQL to detect outliers (not the same as detecting anomalies, although that’s in the future).\\n\\nIn the first post in this sporadic series of posts, we looked at how to assess missing data by counting the proportion of nulls in a column. However, simply finding and counting missing data points is of limited use — you need to have tools to use if you find these missing data points.\\n\\nThe core ways to handle missing values should be familiar to all data scientists, a phrase which here means �', metadata={'Title': 'Using SQL to Improve Missing Values'}),\n",
       " Document(page_content=' all data scientists, a phrase which here means ‘if you aren’t familiar, you should memorise the following list’:\\n\\nListwise deletion: if a variable has so many missing cases that it appears useless, delete it. Casewise deletion: if there are too many factors missing for a particular observation, delete it. Dummy Variable Adjustment: if the variable is missing for a particular case, use an assumed value in its stead. Depending on the problem the median may appear the intuitive choice or a value that represents a ‘neutral’ setting. Imputation: use an algorithm to fill in the value, from a simple random number at the most basic end of the spectrum, to a value imp', metadata={'Title': 'Using SQL to Improve Missing Values'}),\n",
       " Document(page_content=' basic end of the spectrum, to a value imputed by its own model at the more complex end.\\n\\nSQL is clearly better at some of these than others. Listwise deletion is as simple as leaving the column name out of the SELECT statement; casewise deletion may be as simple as a WHERE clause testing for nulls.\\n\\nUsing these two methods comes with risks — you can easily introduce bias if the data are not Missing Completely at Random. It’s not hard to think of situations where measurements aren’t collected due to specific circumstances reflected in the data. An example given by Frank Harrell in his book ‘Regression Modeling Strategies’ is blood pressure measurements which are not taken on patients expected', metadata={'Title': 'Using SQL to Improve Missing Values'}),\n",
       " Document(page_content=' blood pressure measurements which are not taken on patients expected to die shortly — hence excluding patient observations missing blood pressure measurements in a model of patient outcomes could selectively exclude patients who died, creating an obvious skew.\\n\\nWhile the above example illustrates the risk of casewise deletion, the disadvantage of listwise deletion can also be seen from the…', metadata={'Title': 'Using SQL to Improve Missing Values'}),\n",
       " Document(page_content='Reference Paper: Reducing Network Agnostophobia: https://arxiv.org/pdf/1811.04110.pd\\n\\nFor classification models for many domains and scenarios it is important to predict when the input given to the model does not belong to the classes it was trained on.\\n\\nFor computer vision / object detector models author provide following justification:\\n\\nObject detectors have evolved over time from using feature-based detectors to sliding windows [34], region proposals [32], and, finally, to anchor boxes [31]. The majority of these approaches can be seen as having two parts, the proposal network and the classification network. During training, the classification network includes a background class to identify a proposal as not having an object', metadata={'Title': 'Predicting Unknown Unknowns'}),\n",
       " Document(page_content=' class to identify a proposal as not having an object of interest. However, even for the state-of-the-art systems it has been reported that the object proposals to the classifier “still contain a large proportion of background regions” and “the existence of many background samples makes the feature representation capture less intra-category variance and more inter-category variance (...) causing many false positives between ambiguous object categories” [41]. In a system that both detects and recognizes objects, the ability to handle unknown samples is crucial. Our goal is to improve the ability to classify correct classes while reducing the impact of unknown inputs.\\n\\nIt is also an important problem to tackle in many domains including healthcare, robotics irrespective of vision or N', metadata={'Title': 'Predicting Unknown Unknowns'}),\n",
       " Document(page_content=' domains including healthcare, robotics irrespective of vision or NLP.\\n\\nNomenclature:\\n\\nPast approaches to handle this have relied on two fundamental approaches:\\n\\nGiven an input provide uncertainty score based on how close is the input to inputs seen in training. ==> P (U | x) Given an input x predict probability of that input belonging to all classes Ci (i=1 to n) the model was trained on. We then threshold on having a minimum probability for class with highest probability to reject input as out of set or unknown.\\n\\nHere are summary of some of the related approaches:', metadata={'Title': 'Predicting Unknown Unknowns'}),\n",
       " Document(page_content='Reinforcement Learning Introduction\\n\\nAn introduction to reinforcement learning problems and solutions Y Tech · Follow 4 min read · Jul 25, 2019 -- Share\\n\\nThis post will be an introductory level on reinforcement learning. Throughout this post, the problem definitions and some most popular solutions will be discussed. After this article, you should be able to understand what is reinforcement learning, and how to find the optimal policy for the problem.\\n\\nThe Problem Description\\n\\nThe agent-environment interaction in reinforcement learning\\n\\nThe Setting\\n\\nThe reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.\\n\\nlearning to interact with its environment. At each time step, the agent receives the environment’s state (the environment', metadata={'Title': 'Reinforcement Learning Introduction'}),\n",
       " Document(page_content=' receives the environment’s state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state .\\n\\n(the environment presents a situation to the agent), and the agent must choose an appropriate in response. One time step later, the agent receives a (the environment indicates whether the agent has responded appropriately to the state) and a new . All agents have the goal to maximize the expected cumulative reward.\\n\\nEpisodic vs. Continuing Tasks\\n\\nContinuing tasks are tasks that continue forever, without end.\\n\\nare tasks that continue forever,', metadata={'Title': 'Reinforcement Learning Introduction'}),\n",
       " Document(page_content=' end.\\n\\nare tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point.\\n\\n* In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.\\n\\n* Episodic tasks come to an end whenever the agent reaches a terminal state.\\n\\nCumulative Reward\\n\\nThe discounted return at time step t is G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3) + ...\\n\\nThe agent selects actions with the goal of maximizing expected (discounted) return.\\n\\nThe discount rate γ', metadata={'Title': 'Reinforcement Learning Introduction'}),\n",
       " Document(page_content=') return.\\n\\nThe discount rate γ is something that you set, to refine the goal that you have the agent.\\n\\n* It must satisfy 0 ≤ γ ≤ 1 .\\n\\n* If γ = 0 , the agent only cares about the most immediate reward.\\n\\n* If γ = 1 , the return is not discounted.\\n\\n* For larger values of γ , the agent cares more about the distant future. Smaller values of γ result in more extreme discounting.\\n\\nMDPs and One-Step Dynamics', metadata={'Title': 'Reinforcement Learning Introduction'}),\n",
       " Document(page_content='Throughout history, we have seen how organisations across entire industries have embraced robotic technology, and how today, it is almost impossible for some of these organisations to operate without it. Every day, we are witnesses of how technology is integrating into nearly every aspect of our life at work, and in many cases, we depend on it to perform most of our daily tasks.\\n\\nOver many decades, organisations have been using robots to automate daily processes. In fact, every day we are surrounded by them, whether at home, in the street or at the office — from ATMs and vending machines to more sophisticated surgical robots and self-driving cars. These robots come in different forms and shapes, and perhaps they don’t look like humans, but the', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' don’t look like humans, but the enterprise loves them, and they surely are a key driver for massive productivity.\\n\\nNow, imagine a regular day — wake up in the morning, go to your workplace, grab a coffee, and start a conversation with your colleague… Everything seems normal, you engage in the conversation, exchange thoughts and new ideas, then finish and move on to your desk to start working. An hour later, your manager asks you to analyse and compare five long reports. You don’t have time for that, it would take too many hours of work and tons of energy; plus there are other things that you have to do. So, you ask your colleague for help and amazingly get the results back within', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' colleague for help and amazingly get the results back within an hour.\\n\\nThis sounds very unrealistic, right? No normal colleague would do this work for you or even finish it so fast. But what if I tell you that your colleague is a ROBOT? A robot that looks and acts the same as any other human being. It can talk like you, think like you, and almost reason like you do. But it is still a robot. How would you feel?\\n\\nSOURCE: © ADOBE STOCK\\n\\nArtificial Intelligence, Machine Learning & Robotics:\\n\\nMany robots are not artificially intelligent, they are programmed to perform repetitive tasks or movements to manipulate objects in the real world. However, some experts suggest that what constitutes', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' world. However, some experts suggest that what constitutes a robot is its ability to think and make decisions, but doesn’t it imply the use of AI?\\n\\nOn the other hand, Artificial Intelligence (AI) aims to replicate human intelligence behaviour by addressing skills like problem-solving, learning, perception, and reasoning. It can involve some level of Machine learning (ML), which is based on the idea of granting machines access to data that will allow them to learn by themselves.\\n\\nAI together with ML and Robotics intend to create a man-made machine/robot with human intellectual capacities that can be able to formulate original ideas by itself. We have not achieved this yet, but we have made a lot of progress.\\n', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' but we have made a lot of progress.\\n\\nYou have probably already heard about Sophia, the social robot that looks like a human. If you know what I’m talking about, think about a more sophisticated version of Sophia working and collaborating with humans.\\n\\nOther big developments include the so-called “Robotic Process Automation” (RPA). These are software robots that help businesses and employees do simple jobs by replicating human interaction. It is only a software and not a physical AI Robot, but this is definitely a significant breakthrough.\\n\\nThe real challenge is to make AI understand how natural intelligence works because we know how our brain functions, and we can teach AI how to think and learn, but we still', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' AI how to think and learn, but we still don’t know exactly how all these connections can lead AI to the use of abstract reasoning or “common sense”.\\n\\nSOURCE: © ADOBE STOCK\\n\\nMy Colleague is a Robot:\\n\\nGoing back to where we started — robots designed to share a workspace with humans by carrying out physical and intellectual tasks, building up ideas with humans, learning from our environment, questioning decisions, and finding solutions together, will reduce many risks and exponentially increase productivity.\\n\\nRobots can already do many things much better than humans, but it still takes humans to interpret their work and apply the results in strategic and creative ways. For this reason, we need to make', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' ways. For this reason, we need to make sure that robots are seen as complementary rather than competitive, and assign them the work that no one wants to do, the one that is intense and repetitive; leaving the part of the work that involves judgement and expertise, to humans.\\n\\nImagine one more time, the same scenario where you engage in a conversation with your colleague, but now you know for sure and from the beginning that it is a robot. Would it feel weird?\\n\\nProbably, and I know it sounds scary as well, but if we give this deeper thought and think about the benefits and not the drawbacks of having an artificially intelligent coworker, it might just change our perspective. Some of these benefits include:\\n\\nSafety', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content='. Some of these benefits include:\\n\\nSafety: Tasks that involve the use of heavy machinery, sharp objects, very high or low temperatures, chemicals, and others, will be performed by robots. This will protect workers in dangerous and unhealthy working conditions.\\n\\nSpeed and Consistency: AI robots work fast and without any distractions, they have no need for vacations and are available 24/7.\\n\\nNo Errors: Robots have almost no room for mistakes, they are precise and deliver quality.\\n\\nHappiness & Productivity: Most importantly, all these perks are intended to increase both, employee happiness and productivity. As mentioned before, these robots will take over those tasks that we don’t enjoy. From dangerous, tedious', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' don’t enjoy. From dangerous, tedious, and repetitive basic tasks to more complicated ones that require highly analytical skills.\\n\\nSOURCE: © ADOBE STOCK\\n\\nExamples of Work That Can Be Complemented By AI Robots:\\n\\nData Journalists: This type of journalists are those who are focused on analysing data. AI robots could perfectly perform these work much faster and efficiently.\\n\\nSecretaries: Administrative tasks like answering phone calls, sending emails, scheduling meetings, and others (including physical, manual tasks), can be done by AI robots.\\n\\nDocument-Review (Attorneys): Many attorneys have to search through thousands of documents looking for specific information. AI robots can filter information in a flash, they can also', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' can filter information in a flash, they can also analyse it and generate reports. This work applies to other related fields as well.\\n\\nPharmacists: When you go to a pharmacy, an AI robot could scan your prescription and get your medicine. For over-the-counter medicine, you could indicate your symptoms and the robot will suggest a recommendation. Also, these robots could potentially have access to data from hospitals and your health records to make suggestions more accurate.\\n\\nAI Police and Intelligence Assitant: AI Robots could potentially assist the police and agencies like the CIA by collecting, storing, sorting through, and highlighting key data that is necessary for investigations. They could also perform some physical duties like patrolling, arresting, and even directing traffic.', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' like patrolling, arresting, and even directing traffic.\\n\\nBotenders: Robots can be taught how to mix and serve drinks. Anything from beers to signature cocktails. They can make hundreds of them within minutes.\\n\\nThere are many concerns about robots replacing people and eliminating jobs, but these robots could potentially work alongside humans, collaborate and complement our work rather than taking over jobs. In fact, technology will create more jobs than it will eliminate. Many jobs will change, and the new ones will require a new set of skills that we must acquire through advanced education and training systems boosted by AI.\\n\\nIf we are going to share a workspace with robots and see them as partners rather than adversaries, they must first experience the world as a human,', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' they must first experience the world as a human, meaning that they would need to be able to learn about us. This will make the interaction between humans and robots much easier and can also allow them to keep learning much faster.\\n\\nMorality and AI Coworkers\\n\\nWhat do we expect from artificially intelligent coworkers in terms of morality?\\n\\nSOURCE: © ADOBE STOCK\\n\\nAI and robotics will have a huge impact on society, values, and human rights.\\n\\nIf we want machines to operate autonomously, at one point, they will require to collect a lot of data. But how much of these data do we want to share with robots? If a robot causes an accident, who would be responsible? Are', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content=' causes an accident, who would be responsible? Are we willing to give up our privacy to interact with robots? Can we even trust a robot?\\n\\nMore questions will continue to arise as technology develops, and it is up to us to answer those questions by working together to implement a structure that regulates and protects the contour of these innovations.\\n\\nThere are always two sides to every story, times of uncertainty will come and technology if gone wrong, can become dangerous. Therefore, we have to manage robots carefully, in the end, it is humans who will take charge of controlling, checking, and running the bots.\\n\\nAI and robotics should no longer be feared, but rather be seen as a tool for collaboration.', metadata={'Title': 'What if Your Colleague is a Robot'}),\n",
       " Document(page_content='Machine Learning for Content Moderation — Challenges\\n\\nOverview\\n\\nFor an introduction to the topic of machine learning for content moderation, read the Introduction of this series:\\n\\nNow that we have gone over an overview of machine learning systems used for automatic content moderation, we can address the main challenges faced by these systems. These potential problems can lead to difficulties in evaluating the model, determining approaching classifier thresholds, and using it fairly and without unintentional bias. Since content moderation systems act on complex social phenomena, they face problems that are not necessarily encountered in other machine learning contexts.\\n\\nVarying definitions\\n\\nFor many applications of content moderation, it is difficult to provide an explicit definition of the phenomenon of interest. These topics are often very complex', metadata={'Title': 'Machine Learning for Content Moderation — Challenges'}),\n",
       " Document(page_content=' phenomenon of interest. These topics are often very complex social phenomena, whose definitions are topics of constant academic debate. For example, cyber-bullying has various definitions in academic texts, and so it is difficult to create an all-encompassing definition that everyone can agree on. For this reason, instructions provided to manual content labelers may not be clear enough to produce very reliable labels.\\n\\nThis leads to two problems.\\n\\nFirst, it may appear to users that the content moderation system is inconsistent, if some things that users deem to be violating rules are removes and some are not. This may lead users to not trust the content moderation mechanisms or believe that it is unfairly targeting certain users. Since, from the user perspective, these systems', metadata={'Title': 'Machine Learning for Content Moderation — Challenges'}),\n",
       " Document(page_content='. Since, from the user perspective, these systems are nebulous black boxes, it is difficult to explain how such inconsistencies may arise.\\n\\nThe other problem is that the labeled training data may have contradictory data points. If labeling inconsistencies cause two very similar data points to have opposing labels, then…', metadata={'Title': 'Machine Learning for Content Moderation — Challenges'}),\n",
       " Document(page_content='Linear Regression\\n\\nLinear Regression is a famous supervised learning algorithm used to predict a real-valued output. The linear regression model is a linear combination of the features of the input examples.\\n\\nA note on the notation. x_{i} means x subscript i and x_{^th} means x superscript th.\\n\\nRepresentation of the Data\\n\\nAs discussed in the definition, linear regression is a supervised learning algorithm, therefore, has a set of N labelled examples, represented as :\\n\\nData used to construct a linear regression model.\\n\\nHere, x_{i} represents a set of properties corresponding to the i_{^th} example. These set of properties are collectively called a feature vector.', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' set of properties are collectively called a feature vector. All the examples from i=1,2,3,…,n have a corresponding real-valued y, which denotes a physical quantity such as cost, temperature, or any other continuous value.\\n\\nHere each feature vector is 3-dimensional consisting of the area of house in square metres, the number of rooms and the age of house in years. The target variable is the price of house in USD.\\n\\nModel\\n\\nNow, as we have our examples ready we want to make our model f(x) that will help us to predict the output y for an unseen x.\\n\\nModel of linear regression learning algorithm. Here, w is an R-dimensional parameter vector (', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=', w is an R-dimensional parameter vector (x is an R-dimensional feature vector) and b is the bias.\\n\\nThe job of the model is to predict a real-value y for an unseen value of the feature vector x. But, we want to find a model such that it does the best job in predicting the values of y, therefore, we want to find values of w and b such that the predictions are as close as possible to the actual answers. It is obvious that different values of w and b result in producing different models, of varying capabilities. Therefore, our job is to find the optimal set of values w* and b* which will minimize the error between the predictions made by the model f(x)', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' the predictions made by the model f(x) and the actual results y for the training set.\\n\\nThe Best Model\\n\\nAs discussed earlier we have N examples and a model f(x) for which we need to find the optimal values of w and b. Let us use all these N examples for finding the optimal values of w and b, popularly called as training our model. We need to find values of w and b such that the following expression is minimum.\\n\\nCost function for linear regression.\\n\\nThis is our objective function as we are going to minimize it. Learning algorithms have functions which we try to minimize or maximise. These functions are called as loss function or the cost function. This particular form is called', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' or the cost function. This particular form is called the mean squared error loss function.\\n\\nIf you observe the loss function:\\n\\nIt is simply subtracting the model’s output, f(x_{i}) and the actual output y_{i},\\n\\nSquaring it,\\n\\nAnd finally taking its average.\\n\\nTo understand this better let us assume John, recently appeared for an examination having 10 mathematical questions and the answer key has been published. Now John decides to find out how well has he performed? so, he compares his answer, f(x)_{i} with the corresponding answer y_{i} on the answer key. If the difference between John’s answer and the actual answer f(', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content='’s answer and the actual answer f(x)_{i}-y_{i} is 0 he answered that question correctly. If he answered all the questions correctly then the average will also be 0 which corresponds to the best performance, implying the best model. Squaring the error helps to accentuate the error of the model. We could have also taken a cube or higher power but then the derivatives would have been more difficult to work out. We worry about the derivatives of the cost function as setting them to zero gives the optimal value w* and b* for the model.\\n\\nGeneral Questions and Examples\\n\\nLet us discuss a few questions that perplexed me while studying about linear regression. But, before we start let’', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' regression. But, before we start let’s take a look at a very primitive example of linear regression.\\n\\nSo, John and his friends decided to start studying linear regression from scratch so they began by collecting examples themselves. The examples they collected are shown below.\\n\\nThe tabular form of data. Here x is a single dimensional feature vector any y is a real-valued output corresponding to each feature vector.\\n\\nAfter having collected the data, John decides to fit a linear regression model to it.\\n\\nThe linear regression model of form f(x)=wx+b.\\n\\nThis is a model of form f(x)=wx+b where w is a scalar, as x, the feature vector is', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' scalar, as x, the feature vector is one dimensional. A better comprehension of this model is to compare this to the equation of a straight line y=mx+c where m is analogous to w and c to b. This is a linear model.\\n\\nBut, can we do better? Can we come up with a model that performs better than the current one? Yes, we can. It is a common confusion that linear regression only comprises of models that are straight lines. However, we can also fit curves to our data by transforming the data. Let’s transform our feature vector by squaring each x_{i} value.\\n\\nAfter having transformed our feature vector let us try to fit a model on the new feature', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' us try to fit a model on the new feature vector x² and the output y (original feature vector x is not considered for training the model instead, it’s transformation x_{^ 2} has been used to train the model).\\n\\nThe model is a better fit than the previous linear model. Here the original feature vector x_{i} is transformed to it’s square and then the model is computed.\\n\\nSo, now we have predicted a polynomial model that is better than the linear one by transforming the original feature vector x_{i} to its square. The new model corresponds to f(x)=wx²+b.\\n\\nThis is a plot of the polynomial regression model. Note', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content=' plot of the polynomial regression model. Note that this plot is between X and Y with the polynomial model of degree two. The previous plot was between X² and Y, therefore, it was linear.\\n\\nThe capability of the model to predict better results has increased by transforming the feature vectors but we need to be aware of over fitting. Over fitting happens when the model predicts too well during the training phase but makes an error while predicting unseen examples. Over fitting does not reflect the real-world scenario of being dynamic. It does not produce generalised models.\\n\\nThis is an example of overfitting where the model is too accurate on the training examples.\\n\\nLet’s say that the feature vector is R-', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content='�s say that the feature vector is R-dimensional. We have seen the case where R=1 and also predicted a linear and a polynomial model. If R=2 a plane is predicted as the model. Generally linear regression models a hyper plane for a data set with R-dimensional feature vector, x and 1-dimensional output, y.\\n\\nHyper plane is a subspace with one less dimension than that of its surrounding space. In case of a 1 dimensional line the point is a hyper plane, in case of a 2 dimensional region a line is a hyper plane, in case of a 3 dimensional space the plane is a hyper plane and so on.\\n\\nThe Bias Term\\n\\nLet’s discuss the', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content='ias Term\\n\\nLet’s discuss the utility of the bias term. Consider the equation of a straight line y=mx. In this case m controls the slope of the line and can rotate the line anywhere but only about the origin.\\n\\nSuppose you decide to use this model for a trivial linear regression problem. However, any hypothesis that you generate will always pass through the origin and might fail to generalise. Adding the bias term will result in the hypothesis y=mx+c thereby, allowing you to move your line anywhere in the plane. The bias term helps in generalising the hypothesis.', metadata={'Title': 'Linear Regression'}),\n",
       " Document(page_content='This article is the 2nd and last article on Dependency Parsing. We will give you some easy guidelines for implementation and the tools to help you improve it.\\n\\nVocabulary\\n\\nA TreeBank is a parsed text corpus that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive.\\n\\nis a parsed text corpus that annotates syntactic or semantic sentence structure.', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive. CoNLL-U — Computational Natural Language Learning-Universal is a revised version of the CoNLL-X format. Sentences from TreeBanks are separated, and each word or punctuation mark is disposed on a distinct line. Each of the following items follows the word, separated by tabulations:\\n\\n', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' the word, separated by tabulations:\\n\\n–ID: word index in the sentence, starting at 1\\n\\n–FORM: word form or punctuation\\n\\n–LEMMA: Lemma or stem of word form\\n\\n–UPOS: Universal part of speech tag\\n\\n–XPOS: Language-specific part of speech tag; will not be used in our model\\n\\n–FEATS: Unordered list of morphological features, defined by Universal Dependencies; indicates the gender and number of a noun, the tense of a verb, etc.\\n\\n–HEAD: Head of the word, indicates the index of the word to which the current one is related\\n\\n–DEPREL: Universal Dependencies relation; indicates', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content='–DEPREL: Universal Dependencies relation; indicates the relation between two words (subject or object of a verb, determiner of a noun, etc.)\\n\\n–DEPS: Language-specific part dependencies; will not be used in our model\\n\\n–MISC: Commentary or other annotation\\n\\nAn example of CoNLL-U format\\n\\nAn Entry is a word, or a punctuation mark in a sentence. It has multiple attributes, defined above. A sentence is typically a concatenation of entries (a word itself is an attribute of an entry: its form), separated by space.\\n\\nThe Implementation\\n\\nThe implementation of the Bist-Parser comes from the authors of its paper. An update has been', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' the authors of its paper. An update has been published on GitHub by Xiezhq Hermann. You can find it here. It works on Python 3.x, with torch 0.3.1 (with or without Cuda). It is very complete and can be used as is. However, in order to adapt the code to your data or upgrade it, you must get through every module, which can be a difficult task. This part of the article will lead you through all files and processes.\\n\\nUniversal Dependencies (UD) is an open-community framework for grammatical annotation. It provides corpora and tools that greatly help to develop a Dependency Parser.\\n\\nFrom UD, you can download a corpus of sentences', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content='From UD, you can download a corpus of sentences of your choice (in any language available, even Old French!), use them as is, and start training your Bist-Parser with this type of command:\\n\\npython src/parser.py --outdir [results directory] --train training.conll --dev development.conll --epochs 30 --lstmdims 125 --lstmlayers 2 [--extrn extrn.vectors]\\n\\nYou can detail hyperparameters here, caught by the model thanks to the file parser.py\\n\\nAs you may know, when you train a model on a corpus, the model is biased towards this corpus. You could train your model on multiple corpora', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content='. You could train your model on multiple corpora in order to generalize it more. Several techniques allow you to increase scores, with TreeBank Embedding as an example. Here, we have just concatenated some TreeBanks, without any further processing.\\n\\nutils\\n\\nCreate a ConllEntry class: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict.\\n\\nclass:', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' and what it should predict.\\n\\nclass: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict. Read a CoNLL-U file and transform each sentence into a ConllEntry.\\n\\nCount vocabulary: This function creates a Counter of ConllEntry attributes and allows you to know how these attributes are distributed through your dataset. If you want to determine the most frequent words or relations in your', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' to determine the most frequent words or relations in your dataset, this function can be useful.\\n\\nmstlstm\\n\\nThis file contains your model. All your hyper-parameters and most of your monitoring work happen in this file.\\n\\nThe method forward iterates through each entry in the sentence. It first computes the vectors for each entry attribute. With our model, we get multiple vectors that describe the word, the PoS tag and the feats. Those vectors are then concatenated to form a vector with a bigger dimension for each entry. These entries are then concatenated together to form the sentence vector.\\n\\nFirst, it converts entries into vectors. Here, the principal attributes are the embedding of words', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=', the principal attributes are the embedding of words, lemmas (onto) and PoS tags (pos). However, we advise you to add as many features as possible. For example, you may have access to features of words that indicate whether the noun is singular or plural, its gender, or tense… Embedding these features allows your BiLSTM to find many more patterns.\\n\\nEvolution of PoS embedding on two dimensions\\n\\nThen, it feeds the BiLSTM with these vectors (for = forward, back = backward). Line 52 evaluates the scores of the sentence. This is the part where the full Weighted Digraph is created. On line 57, it evaluates the relation score. This', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' line 57, it evaluates the relation score. This is an interesting trick in this model: rather than evaluating all the possibilities at the same time (|possibilities|=|arcs|.|labels|, which is way too high), it predicts the dependencies first, then the relations.\\n\\nWe will see about errs, lerrs and e later.\\n\\nIn the illustrations below, you can see the evolution of dependency evaluation through batches. A dark blue cell corresponds to a weighted arc. The example comes from a typical French sentence, “Les commotions cérébrales sont devenu si courantes dans ce sport qu’on les considère presque comme la routine', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content=' les considère presque comme la routine.” You can spot spelling mistakes in the sentence; this is not rare in TreeBanks.', metadata={'Title': 'Bist-Parser : an end-to-end implementation of a Dependency Parser'}),\n",
       " Document(page_content='Photo by fabio on Unsplash\\n\\n“Look at the data — the numbers don’t lie.” It’s an often given piece of advice, but a less often understood one. Because what the person giving the advice really means is “Look at the data, and think about what it means for the situation we’re facing. Once you consider it in the broader context, you’ll see — and intuitively know — what to do next.” That’s a very different challenge to meet, but an infinitely more valuable one when you’re trying to make sense of a complex situation. And it’s possible only when you realise that connecting your data is', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' possible only when you realise that connecting your data is even more important than collecting it.\\n\\nWhy Collection is no Longer Enough\\n\\nData collection is now mainstream. Storage is cheap, source are abundant and most products being built today are done so with the expectation that organisations will want access to their data in ways outside of the product’s offerings, via bulk export or automated delivery. Despite this helpful (and rapidly becoming expected) type of access, most data is still viewed within the single perspective that it was collected. Sure — graphs, charts and dashboards are possible and certainly helpful, but they’re often created using data that’s already been actioned, and usually to just re-tell the same story that�', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' to just re-tell the same story that’s already been told. The innovation factor is low, because the connection factor is also low.\\n\\nBut apart from these technical considerations, there is another more fundamental reason why disconnected data isn’t as helpful as it could be. Human beings are evolutionary creatures, and at our most basic level we are instinctively wired to consider the entirety of a situation before responding to it. Early human tribes didn’t seek out short-term gain if it compromised their chances of long-term survival (though it could be argued that the modern world has reduced our ability for this type of thinking) because to do so would contradict our unique awareness for considering relationships in our environment. The same holds true', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' considering relationships in our environment. The same holds true for data collected from technical systems — a broader context is lacking by default, and context is key to understanding.\\n\\nWhat Connection Makes Possible\\n\\nData is enriched through connection, which offers a far more realistic representation of complex environments. When we look at data that is connected through a relevant framework, we engage the part of our mind that understands consequences, dependencies and how the quality of relationships affect the outcome we are seeking.\\n\\nThe argument is sometimes made that this type of thinking is still possible with disconnected data, it just requires us to put the pieces together in our head. This is the same argument that says human beings can multitask, but multiple studies have confirmed that we don’', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' but multiple studies have confirmed that we don’t — rather, we single-task with quick switching. This switching comes at a mental cost though, which can lead to poor outcomes through reluctance to gather all of the disconnected pieces every time a decision needs to be made.\\n\\nConnecting data into a seamless, holistic perspective removes the need for switching. It enables us to understand the contributing factors of an event and act appropriately. Over time, this heightened awareness leads to the development of new knowledge, which if delivered back into the framework creates a unique feedback loop whereby insight gives rise to further insight. As knowledge is shared, unexpected opportunities for collaboration emerge. The innovation factor rises in response to the rising connection.\\n\\nConnection is Always Relevant', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' rising connection.\\n\\nConnection is Always Relevant — Especially When Humans Are Involved\\n\\nUnderstanding how to connect data in an organisation can sometimes be difficult. Over time people tend to specialise in a given area, and that specialisation can lead to the belief that one type of data has limited relevance to another. But if looked at closely, these same areas will usually be found to have unseen ties and concerns to others. Connecting the data of all areas in an organisation helps to generate a wider context that speaks to our evolutionary understanding of nature and our place in it, where no one entity truly exists in isolation.\\n\\nThis is even more true for organisations that work with human networks. We are deeply connected beings, with our behaviours often', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content=' We are deeply connected beings, with our behaviours often influenced far more by those around us (and even those around them) than we believe. These social ties often vary in type and strength, both of which affect the flow of influence and change. Proper consideration of these diverse relationships is fundamental to making informed decisions, and only possible when data is properly connected.\\n\\nThis article first appeared on InsightableTimes.org at https://insightabletimes.org/in-the-new-era-of-knowledge-connection-beats-collection-everytime', metadata={'Title': 'In the New Era of Knowledge, Connection Beats Collection Every Time'}),\n",
       " Document(page_content='In this post, I will outline the steps I took to discover similar and dissimilar coffee-neighborhoods in Istanbul locals’ favorite districts.\\n\\nAs part of my IBM data science course project, I was made to come up with a problem and find a solution to it by gathering, exploring and analyzing location data. Being a lover of Istanbul and coffee, I decided to come up with something that strides the two subjects.\\n\\nIstanbul is one of the biggest and most populous cities in the world, the only city that exists on two continents. Both parts of the city are divided by the Bosphorous strait. Two Districts loved by Istanbul residents are Beşiktaş (be-shik-t', metadata={'Title': 'Where to Coffee Like an Istanbul Local'}),\n",
       " Document(page_content='iktaş (be-shik-tash) and Kadıköy (ka-di-koy) on the European and Asian side respectively. While these districts have a lot in common, they have their fair share of differences as well, the surge of coffee shops for one. In fact, according to Foursquare, 8 of 15 best coffee shops in Istanbul are located in Beşiktaş and Kadıköy.\\n\\nThere is a fierce debate among residents about the neighborhood to best enjoy a cup of coffee. This report will address the issue by providing insights drawn from data. This study will be of interest to both visitors of Istanbul and locals who yet to discover the hidden similarities between the two', metadata={'Title': 'Where to Coffee Like an Istanbul Local'}),\n",
       " Document(page_content=' who yet to discover the hidden similarities between the two most sought after neighborhoods. The report will help readers to:\\n\\nBe more familiar with the discussed neighborhoods Understand the relationship between coffee shops and other neighborhood attributes Discover the similarities between neighborhoods in terms of coffee shops and other attributes Be able to make better-informed decisions about where to coffee in Istanbul like a resident\\n\\nThe neighborhoods that will be examined are shown on the map with the red markers.', metadata={'Title': 'Where to Coffee Like an Istanbul Local'}),\n",
       " Document(page_content='Hey Alexa, Tell Me a Joke!\\n\\nI don’t remember what her response was, but I do remember being so excited with the new device that I posted an AMA (which is an acronym for Ask Me Anything) on Reddit. The headline was: I just got Alexa, type in what you want me to ask her!\\n\\nFor the next day, Redditors would send me questions to ask, I’d ask Alexa, and I’d reply to those Redditors with her response. Some of them might’ve been Alexa engineers themselves; they asked questions that gave Easter Egg answers, sort of like ordering from a secret menu (if you have Alexa, try asking her: Alexa, who is', metadata={'Title': 'Domo Arigato, Misses Roboto'}),\n",
       " Document(page_content=' Alexa, try asking her: Alexa, who is the fairest of them all?). That day, I also earned the highest amount of Karma I’d ever earned on Reddit (Karma is a points system on Reddit that is equivalent to likes on Facebook or Twitter). Thanks, Alexa.\\n\\nBefore long, I was purchasing smart bulbs and smart outlets, all switched on and off by Alexa and thus, via the transitive property (if A=B and B=C, then A=C), my voice. It was just too convenient compared to finding your phone, unlocking it, opening up the smart bulb app, and then manually adjusting lighting with your fingers. And while Alexa was already convenient throughout my home, she was even better', metadata={'Title': 'Domo Arigato, Misses Roboto'}),\n",
       " Document(page_content=' already convenient throughout my home, she was even better in bed— I no longer needed to get up to turn off the lights before going to sleep.\\n\\nA Human Connection', metadata={'Title': 'Domo Arigato, Misses Roboto'}),\n",
       " Document(page_content='Mae Fah Luang University Campus on March 2019. (Photo by MFU Photoclub with permission)\\n\\nIn the previous blog, I looked at the winter air pollution in Bangkok. The main source of pollution comes from particles smaller than 2.5 micrometer (PM 2.5 particles). These particles are smaller than the width of a human hair and can easily enter our bodies, even making their way into our blood. Last week (March 17, 2019), many provinces in the northern part of Thailand had the worst Air Quality Index (AQI) in the world due to particle pollution. So far, no long term solution has been proposed because the source of the PM 2.5 particle pollution has not been clearly pinpointed', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content='.5 particle pollution has not been clearly pinpointed. In this notebook, I identify the sources of high PM 2.5 particles in Bangkok through a machine learning model. The code can be found in my GitHub page.\\n\\nHigh PM2.5, Who Are the Culprits ?\\n\\nThere are three major theories regarding the source of air pollution in Bangkok: (1) The temperature inversion effect where cold air along with pollution is trapped close to the surface of the Earth. This theory was proposed by the government at the beginning of the 2019 winter season. The government blamed emission from old diesel engines for the pollution. (2) Agricultural burning, either locally or from surrounding provinces. During winter, a lot of open agricultural burning', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content='. During winter, a lot of open agricultural burning occurs throughout the country. Some officials have tried to tackle the air pollution problem by reducing open agricultural burning. (3) Pollution from other provinces or countries. Some NGOs blamed the pollution on near by power plants.\\n\\nMy analysis procedure is as follows: Build a machine learning model(ML) to predict the air pollution level in Bangkok using environmental factor such as weather, traffic index, and fire maps. Include date-time features such as local hour, and weekday versus weekend in the model to capture other effects from human activities. Identify dominant sources of pollution using the feature of importance provided by the ML model.\\n\\nIf the source of the pollution is local, then the AQI will', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=' the pollution is local, then the AQI will depend on factors such as weather patterns (wind speed, humidity, average temperature), local traffic, and hour of day. If the pollution is from agricultural burning, the AQI will depend on active fires with some time lag to account for geographical separation. Fire activities are included based on the distance from Bangkok. On the other hand, if the pollution not correlated with the fire map, then the model should put more weight on weather patterns, such as wind direction and wind speed.\\n\\nHere are a list of features I considered and their data sources:\\n\\nActive fire information from NASA’s FIRMS project\\n\\nWeather pattern: temperature, wind speed, humidity, and rain, scraped', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=' wind speed, humidity, and rain, scraped from the Weather Underground website\\n\\nTraffic index from Longdo Traffic\\n\\nDate time features: hour of day, time of day, and holiday patterns (explored in the Part I blog post)\\n\\nLet me first walk through all the features included in the model.\\n\\nAgricultural Burning is a Major Problem !\\n\\nFarmers in Southeast Asia pick January — March as their burning season. For the north and northeastern provinces in Thailand, these burning activities are large enough to make these provinces among the most polluted places in the world during this time. For Bangkok, one might argue that because the region is heavily industrial rather than agricultural, it may not be affected as much by agricultural', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=', it may not be affected as much by agricultural burning. But this is not the case.\\n\\nBecause of the tiny size of PM 2.5 particles, they remain suspended in the atmosphere for prolonged periods and can travel over very long distances. From the weather data, the average wind speed is 10 km/hour. The reported PM 2.5 level is a rolling average over 24 hours. A rough estimate is that the current PM 2.5 reading may be from sources as far as 240 km away. The picture below shows the fire map measured by NASA’s satellites, indicative of agricultural burning, on Jan 8, 2018 and on Feb 8, 2018. The yellow circle indicates the area within 240 km of Bangkok. The number of', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=' area within 240 km of Bangkok. The number of fires on Jan 8, which has an acceptable level of pollution, is much lower than the number of fires on Feb 8, which has an unhealthy level of pollution.\\n\\nFire spots from NASA’s satellites\\n\\nIn fact, the fire pattern closely aligns with the PM 2.5 pattern.\\n\\nThe number of fires aligns with spikes in PM 2.5 levels\\n\\nWeather Patterns\\n\\nThe temperature inversion effect often occurs during winter because the temperature is cooler near the ground. The hotter air on top traps the cool air from flowing. This stagnant atmospheric condition allows the PM 2.5 particles to remain suspended in the air for longer. On the other hand, higher', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=' air for longer. On the other hand, higher humidity or rain will help remove particles from the atmosphere. This is one reason why in the past when the air pollution was high, the government has sprayed water in the air. Unfortunately, this mitigation does not appear to be effective, since the volume of water is minuscule compared to actual rain. How much influence does weather pattern have on air pollution? Let’s compare the weather in winter versus other seasons.\\n\\ncompare the weather pattern in winter and other seasons\\n\\nTemperature, wind speed and humidity are all lower in winter, but not by a large amount. Now, let’s look at the relationship of each of these with the PM 2.5 level.\\n', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content=' these with the PM 2.5 level.\\n\\nEffect of temperature, wind speed, and humidity on PM 2.5 level in winter\\n\\nHigher temperature (which disrupts the temperature inversion effect), wind speed and humidity have a negative correlation with the pollution level.\\n\\nEffect of wind on PM 2.5 level in winter\\n\\nOn windy days, the pollution is clearly better. The median of the distribution for PM 2.5 levels is lower on windy days compared to on days without wind.\\n\\nIn fact, the pollution level also depends on the wind direction, as seen in this plot. I selected only four major wind directions for simplicity.\\n\\nPM2.5 relationship with the wind direction in winter\\n', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content='.5 relationship with the wind direction in winter\\n\\nOn the days where the wind comes from the south, the pollution level is lower likely because the Thai gulf is to the south of Bangkok. The clean ocean wind improves the air quality. Wind from the other three directions pass overland. However, having any wind is better than the stagnant atmospheric conditions on calm days.\\n\\nThe shift in the median PM 2.5 level is smaller between rainy days and days with no rain. There are fewer rainy days during the winter season, so the data is somewhat noisy, but a difference can be observed in the cumulative density function.', metadata={'Title': 'Identifying the Sources of Winter Air Pollution in Bangkok Part II'}),\n",
       " Document(page_content='Cleaning, Analyzing, and Visualizing Survey Data in Python\\n\\nA tutorial using pandas , matplotlib , and seaborn to produce digestible insights from dirty data Charlene Chambliss · Follow Published in Towards Data Science · 10 min read · Mar 30, 2019 -- 6 Listen Share\\n\\nIf you work in data at a D2C startup, there’s a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, there’s a good chance it’ll be SurveyMonkey data.\\n\\nThe way SurveyMonkey exports data is not necessarily ready for analysis right out of the box,', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=' necessarily ready for analysis right out of the box, but it’s pretty close. Here I’ll demonstrate a few examples of questions you might want to ask of your survey data, and how to extract those answers quickly. We’ll even write a few functions to make our lives easier when plotting future questions.\\n\\nWe’ll be using pandas , matplotlib , and seaborn to make sense of our data. I used Mockaroo to generate this data; specifically, for the survey question fields, I used \"Custom List\" and entered in the appropriate fields. You could achieve the same effect by using random.choice in the random module, but I found it easier to let Mockaroo create the whole', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=' found it easier to let Mockaroo create the whole thing for me. I then tweaked the data in Excel so that it mirrored the structure of a SurveyMonkey export.\\n\\nOh boy…here we go\\n\\nYour first reaction to this might be “Ugh. It’s horrible.” I mean, the column names didn’t read in properly, there are a ton of NaNs, instead of numerical representations like 0/1 or 1/2/3/4/5 we have the actual text answers in each cell…And should we actually be reading this in with a MultiIndex?\\n\\nBut don’t worry, it’s not as bad as you might think. And we', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=\" not as bad as you might think. And we’re going to ignore MultiIndexes in this post. (Nobody really likes working with them anyway.) The team needs those insights ASAP — so we’ll come up with some hacky solutions.\\n\\nFirst order of business: we’ve been asked to find how the answers to these questions vary by age group. But age is just an age--we don't have a column for age groups! Well, luckily for us, we can pretty easily define a function to create one.\\n\\nBut if we try to run it like this, we’ll get an error! That’s because we have that first row, and its value for age is the\", metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=\" first row, and its value for age is the word “age” instead of a number. Since the first step is to convert each age to an int , this will fail.\\n\\nWe need to remove that row from the DataFrame, but it’ll be useful for us later when we rename columns, so we’ll save it as a separate variable.\\n\\nYou will notice that, since removing headers , we've now lost some information when looking at the survey data by itself. Ideally, you will have a list of the questions and their options that were asked in the survey, provided to you by whoever wants the analysis. If not, you should keep a separate way to reference this info in a document or\", metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=' separate way to reference this info in a document or note that you can look at while working.\\n\\nOK, now let’s apply the age_group function to get our age_group column.\\n\\nGreat. Next, let’s subset the data to focus on just the first question. How do the answers to this first question vary by age group?\\n\\nGreat. We have the answers in a variable now. But when we go to plot this data, it’s not going to look very good, because of the misnamed columns. Let’s write up a quick function to make renaming the columns simple:\\n\\nRemember headers from earlier? We can use it to create our new_names', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=' We can use it to create our new_names_list for renaming.\\n\\nIt’s already an array, so we can just pass it right in, or we can rename it first for readability.\\n\\nIsn’t that so much nicer to look at? Don’t worry, we’re almost to the part where we get some insights.\\n\\nNotice how groupby and other aggregation functions ignore NaNs automatically. That makes our lives significantly easier.\\n\\nLet’s say we also don’t really care about analyzing under-30 customers right now, so we’ll plot only the other age groups.\\n\\nOK, this is all well and good, but the', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=', this is all well and good, but the 60+ group has more people in it than the other groups, and so it’s hard to make a fair comparison. What do we do? We can plot each age group in a separate plot, and then compare the distributions.\\n\\n“But wait,” you might think. “I don’t really want to write the code for 4 different plots.”\\n\\nWell of course not! Who has time for that? Let’s write another function to do it for us.\\n\\nI believe it was Jenny Bryan, in her wonderful talk “Code Smells and Feels,” who first tipped me off to the following:', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content='� who first tipped me off to the following:\\n\\nIf you find yourself copying and pasting code and just changing a few values, you really ought to just write a function.\\n\\nThis has been a great guide for me in deciding when it is and isn’t worth it to write a function for something. A rule of thumb I like to use is that if I would be copying and pasting more than 3 times, I write a function.\\n\\nThere are also benefits other than convenience to this approach, such as that it:\\n\\nreduces the possibility for error (when copying and pasting, it’s easy to accidentally forget to change a value)\\n\\nmakes for more readable code\\n\\nbuild', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content='\\n\\nmakes for more readable code\\n\\nbuilds up your personal toolbox of functions\\n\\nforces you to think at a higher level of abstraction\\n\\n(All of which improve your programming skills and make the people who need to read your code happier!)\\n\\nHooray, laziness!\\n\\nThis is, of course, generated data from a uniform distribution, and we would thus not expect to see any significant differences between groups. Hopefully your own survey data will be more interesting.\\n\\nNext, let’s address another format of question. In this one, we need to see how interested each age group is in a given benefit. Happily, these questions are actually easier to deal with than the former type. Let', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=\" easier to deal with than the former type. Let’s take a look:\\n\\nAnd look, since this is a small DataFrame, age_group is appended already and we won't have to add it.\\n\\nCool. Now we have the subsetted data, but we can’t just aggregate it by count this time like we could with the other question — the last question had NaNs that would be excluded to give the true count for that response, but with this one, we would just get the number of responses for each age group overall:\\n\\nThis is definitely not what we want! The point of the question is to understand how interested the different age groups are, and we need to preserve that information\", metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=' groups are, and we need to preserve that information. All this tells us is how many people in each age group responded to the question.\\n\\nSo what do we do? One way to go would be to re-encode these responses numerically. But what if we want to preserve the relationship on an even more granular level? If we encode numerically, we can take the median and average of each age group’s level of interest. But what if what we’re really interested in is the specific percentage of people per age group who chose each interest level? It’d be easier to convey that info in a barplot, with the text preserved.\\n\\nThat’s what we’re', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content=\"\\nThat’s what we’re going to do next. And — you guessed it — it’s time to write another function.\\n\\nQuick note to new learners: Most people won’t say this explicitly, but let me be clear on how visualizations are often made. Generally speaking, it is a highly iterative process. Even the most experienced data scientists don’t just write up a plot with all of these specifications off the top of their head.\\n\\nGenerally, you start with .plot(kind='bar') , or similar depending on the plot you want, and then you change size, color maps, get the groups properly sorted using order= , specify whether the labels should be rotated,\", metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content='= , specify whether the labels should be rotated, and set x- or y-axis labels invisible, and more, depending on what you think is best for whoever will be using the visualizations.\\n\\nSo don’t be intimidated by the long blocks of code you see when people are making plots. They’re usually created over a span of minutes while testing out different specifications, not by writing perfect code from scratch in one go.', metadata={'Title': 'Cleaning, Analyzing, and Visualizing Survey Data in Python'}),\n",
       " Document(page_content='Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people aren’t talking about it.\\n\\nFor me, “every once in a while” was yesterday when I was scrolling through the #jobs channel in the SharpestMinds Slack workspace, and the “something” is a big problem in the data science industry that I really don’t think we’re taking seriously enough: the vast majority of data science job descriptions do not convey the actual requirements of the position they’re advertising.\\n\\nHow do I know this? For one, quite a few of the jobs posted to', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' one, quite a few of the jobs posted to our internal board included notes from the users (usually SharpestMinds mentors) who posted them, saying things like, “I know the posting says they’re looking for X and Y, but they’re actually fine with Z.” As often as not I’d also get direct messages from them saying the same thing.\\n\\nIn other words, when senior data scientists are called upon to recruit “for real”, their first move is often to throw away the job posting altogether.\\n\\nThis is not good, for several reasons. First, a misleading job description means that recruiters get a *ton* of irrelevant applications, and that candidates', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' *ton* of irrelevant applications, and that candidates waste a *ton* of time applying to irrelevant positions. But there’s another problem: job descriptions are the training labels that any good aspiring data scientist will use to prioritize their personal and technical skills development.\\n\\nDespite the obvious downsides of these mangled job postings, companies keep putting them out there, so a very natural question to ask is: why? Why are job postings so confusing (in that they fail to clearly specify the skills they expect from a candidate), or so outrageously over-reaching (“looking for a machine learning engineer with 10 years’ experience in deep learning…”)?\\n\\nThere are many reasons. For one, companies make hiring decisions', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' many reasons. For one, companies make hiring decisions based on a candidate’s (perceived) ability to solve a real problem that they actually have. Because there are many ways to solve any given data science problem, it can be hard to narrow down the job description to a specific set of technical skills or libraries. That’s why it usually makes sense to put in an application for a company if you think you can solve the problems they have, even if you don’t know the specific tools they ask for.\\n\\nAnother possible reason is that many companies don’t actually know what they want — especially companies with relatively new data science teams — either because the early stage of their data science effort forces everyone to be', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' stage of their data science effort forces everyone to be a jack of all trades, or because they lack the expertise they need to even know what problems they have, and who can help solve them. If you come across an oddly non-specific posting, it’s worth taking the time to figure out which bucket it belongs to, since the former can be a great experience builder, whereas the latter can be a recipe for disaster.\\n\\nBut perhaps the most important reason is that job postings are often written by recruiters, who are not remotely technical. This has the unfortunate side-effect of resulting in occasionally incoherent asks (“Must have 10+ years’ experience with deep learning…”, “…including natural language', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content='…”, “…including natural language toolkits, such as OpenCV…”) or asks that no human being could possibly satisfy.\\n\\nThe net result of this job qualifications circus is that I regularly get questions from our mentees about whether they’re qualified for an opening, despite their having read all the information available on the internet about that position. Those questions are actually surprisingly consistent — so much so that I think it’s worth listing the answers to the most common ones here, in the form of simple rules you can follow to make sure you’re applying to the right roles (and not being scared away by fake requirements):\\n\\nIf a company asks for more than 6 years of deep', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' a company asks for more than 6 years of deep learning experience, then their posting was written by someone who has zero technical knowledge (AlexNet came out in 2012, so this basically narrows the field down to Geoff Hinton’s entourage). Unless you want to build a data science team from the ground up (which you shouldn’t if you’re new to the field), this should be a big red flag.\\n\\nIf you have no prior experience, don’t bother applying to jobs that ask for more than 2 years of it.\\n\\nWhen they say “or equivalent experience”, they mean, “or about 1.5X that much experience working in a MSc or a', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' that much experience working in a MSc or a PhD where you worked on something at least related to this”.\\n\\nIf you meet 50% of the requirements, that might be enough. If you meet 70%, you’re good to go. If you meet 100%, there’s a good chance you’re overqualified.\\n\\nCompanies *usually* care less about the languages you know than the problems you can solve. If they say Pytorch and you only know TensorFlow, you’re probably going to be ok (unless they stress the Pytorch part explicitly).\\n\\nDon’t ignore lines like, “you should be detail-oriented and goal-driven, and', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' be detail-oriented and goal-driven, and thrive under pressure”. They sound like generic, cookie-cutter statements — and sometimes they are — but they’re usually written in a genuine attempt to tell you what kind of environment you’ll be getting yourself into. At the very least, you should use these as hints about what aspects of your personality you should emphasize to establish rapport with your interviewers.\\n\\nNone of these rules are universally applicable, of course: the odd company will insist on hiring only candidates who meet all their stated requirements, and others will be particularly interested in people who know framework X, and will disregard people who can solve similar problems, but with different tools. But because there’s', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content=' with different tools. But because there’s no way to know that from job descriptions alone (unless they’re explicit about it), your best bet is almost always to bet on yourself, and throw your hat in the ring.\\n\\nIf you want to connect, you can find me on Twitter at @jeremiecharris!', metadata={'Title': 'The problem with data science job postings'}),\n",
       " Document(page_content='R and Python are the bread and butter of today’s machine learning languages. R provides powerful statistics and quick visualizations, while Python offers an intuitive syntax, abundant support, and is the choice interface to today’s major AI frameworks.\\n\\nIn this article we’ll look at the steps involved in creating libraries in R and Python. This is a skill every machine learning practitioner should have in their toolbox. Libraries help us organize our code and share it with others, offering packaged functionality to the data community.\\n\\nNOTE: In this article I use the terms “library” and “package” interchangeably. While some people differentiate these words I don’t find this distinction useful, and rarely', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='’t find this distinction useful, and rarely see it done in practice. We can think of a library (or package) as a directory of scripts containing functions. Those functions are grouped together to help engineers and scientists solve challenges.\\n\\nTHE IMPORTANCE OF CREATING LIBRARIES\\n\\nBuilding today’s software doesn’t happen without extensive use of libraries. Libraries dramatically cut down the time and effort required for a team to bring work to production. By leveraging the open source community engineers and scientists can move their unique contribution towards a larger audience, and effectively improve the quality of their code. Companies of all sizes use these libraries to sit their work on top of existing functionality, making product development more productive and focused.', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' functionality, making product development more productive and focused.\\n\\nBut creating libraries isn’t just for production software. Libraries are critical to rapidly prototyping ideas, helping teams validate hypotheses and craft experimental software quickly. While popular libraries enjoy massive community support and a set of best practices, smaller projects can be converted into libraries overnight.\\n\\nBy learning to create lighter-weight libraries we develop an ongoing habit of maintaining code and sharing our work. Our own development is sped up dramatically, and we anchor our coding efforts around a tangible unit of work we can improve over time.\\n\\nARTICLE SCOPE\\n\\nIn this article we will focus on creating libraries in R and Python as well as hosting them on, and installing from, GitHub. This', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' them on, and installing from, GitHub. This means we won’t look at popular hosting sites like CRAN for R and PyPI for Python. These are extra steps that are beyond the scope of this article.\\n\\nFocusing only on GitHub helps encourage practitioners to develop and share libraries more frequently. CRAN and PyPI have a number of criteria that must be met (and they change frequently), which can slow down the process of releasing our work. Rest assured, it is just as easy for others to install our libraries from GitHub. Also, the steps for CRAN and PyPI can always be added later should you feel your library would benefit from a hosting site.\\n\\nWe will build both R and Python libraries using the', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='We will build both R and Python libraries using the same environment (JupyterLab), with the same high-level steps for both languages. This should help you build a working knowledge of the core steps required to package your code as a library.\\n\\nLet’s get started.\\n\\nSETUP\\n\\nWe will be creating a library called datapeek in both R and Python. The datapeek library is a simple package offering a few useful functions for handling raw data. These functions are:\\n\\nencode_and_bind\\n\\nremove_features\\n\\napply_function_to_column\\n\\nget_closest_string\\n\\nWe will look at these functions later. For now we need', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' look at these functions later. For now we need to setup an R and Python environment to create datapeek, along with a few libraries to support packaging our code. We will be using JupyterLab inside a Docker container, along with a “docker stack” that comes with the pieces we need.\\n\\nInstall and Run Docker\\n\\nThe Docker Stack we will use is called the jupyter/datascience-notebook. This image contains both R and Python environments, along with many of the packages typically used in machine learning.\\n\\nSince these run inside Docker you must have Docker installed on your machine. So install Docker if you don’t already have it, and once installed, run the', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' already have it, and once installed, run the following in terminal to pull the datascience-notebook:\\n\\ndocker pull jupyter/datascience-notebook\\n\\nThis will pull the most recent image hosted on Docker Hub.\\n\\nNOTE: Anytime you pull a project from Docker Hub you get the latest build. If some time passes since your last pull, pull again to update your image.\\n\\nImmediately after running the above command you should see the following:\\n\\nOnce everything has been pulled we can confirm our new image exists by running the following:\\n\\ndocker images\\n\\n… showing something similar to the following:\\n\\nNow that we have our Docker stack let’s setup Jup', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' our Docker stack let’s setup JupyterLab.\\n\\nJupyterLab\\n\\nWe will create our libraries inside a JupyterLab environment. JupyterLab is a web-based user interface for programming. With JupyterLab we have a lightweight IDE in the browser, making it convenient for building quick applications. JupyterLab provides everything we need to create libraries in R and Python, including:\\n\\nA terminal environment for running shell commands and downloading/installing libraries;\\n\\nenvironment for running shell commands and downloading/installing libraries; An R and Python console for working interactively with these languages;\\n\\nfor working interactively with these languages; A simple text', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' working interactively with these languages; A simple text editor for creating files with various extensions;\\n\\nfor creating files with various extensions; Jupyter Notebooks for prototyping ML work.\\n\\nThe datascience-notebook we just pulled contains an installation of JupyterLab so we don’t need to install this separately. Before running our Docker image we need to mount a volume to ensure our work is saved outside the container.\\n\\nFirst, create a folder called datapeek on your desktop (or anywhere you wish) and change into that directory. We need to run our Docker container with JupyterLab, so our full command should look as follows:\\n\\ndocker run -it -v `', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=':\\n\\ndocker run -it -v `pwd`:/home/jovyan/work -p 8888:8888 jupyter/datascience-notebook start.sh jupyter lab\\n\\nYou can learn more about Docker commands here. Importantly, the above command exposes our environment on port 8888, meaning we can access our container through the browser.\\n\\nAfter running the above command you should see the following output at the end:\\n\\nThis tells us to copy and paste the provided URL into our browser. Open your browser and add the link in the address bar and hit enter (your token will be different):\\n\\nlocalhost:8888/?token=11e5027e', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"8888/?token=11e5027e9f7cacebac465d79c9548978b03aaf53131ce5fd\\n\\nThis will automatically open JupyterLab in your browser as a new tab:\\n\\nWe are now ready to start building libraries.\\n\\nWe begin this article with R, then look at Python.\\n\\nCREATING LIBRARIES IN R\\n\\nR is one of the “big 2” languages of machine learning. At the time of this writing it has well-over 10,000 libraries. Going to Available CRAN Packages By Date of Publication and running…\\n\\ndocument.getElementsByTagName('tr').length\\n\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"ElementsByTagName('tr').length\\n\\n…in the browser console gives me 13858. Minus the header and final row this gives 13856 packages. Needless to say R is not in need of variety. With strong community support and a concise (if not intuitive) language, R sits comfortably at the top of statistical languages worth learning.\\n\\nThe most well-known treatise on creating R packages is Hadley Wickam’s book R Packages. Its contents are available for free online. For a deeper dive on topic I recommend looking there.\\n\\nWe will use Hadley’s devtools package to abstract away the tedious tasks involved in creating packages. devtools is already installed in our Docker Stacks\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' devtools is already installed in our Docker Stacks environment. We also require the roxygen2 package, which helps us document our functions. Since this doesn’t come pre-installed with our image let’s install that now.\\n\\nNOTE: From now on we’ll use the terminal in JupyterLab in order to conveniently keep our work within the browser.\\n\\nOpen terminal inside JupyterLab’s Launcher:\\n\\nNOTE: If you’d like to change your JupyterLab to dark theme, click on Settings at the top, JupyterLab Theme, then JupyterLab Dark:\\n\\nInside the console type R, then….', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='\\n\\nInside the console type R, then….\\n\\ninstall.packages(\"roxygen2\")\\n\\nlibrary(roxygen2)\\n\\nWith the necessary packages installed we’re ready to tackle each step.\\n\\nSTEP 1: Create Package Framework\\n\\nWe need to create a directory for our package. We can do this in one line of code, using the devtools create function. In terminal run:\\n\\ndevtools::create(\"datapeek\")\\n\\nThis automatically creates the bare bone files and directories needed to define our R package. In JupyterLab you will see a set of new folders and files created on the left side.\\n\\nNOTE: You will also see your new directory structure created', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=': You will also see your new directory structure created on your desktop (or wherever you chose to create it) since we mounted a volume to our container during setup.\\n\\nIf we inspect our package in JupyterLab we now see:\\n\\ndatapeek\\n\\n├── R\\n\\n├── datapeek.Rproj\\n\\n├── DESCRIPTION\\n\\n├── NAMESPACE\\n\\nThe R folder will eventually contain our R code. The my_package.Rproj file is specific to the RStudio IDE so we can ignore that. The DESCRIPTION folder holds our package’s metadata (a detailed discussion can be found here). Finally, NAMSPACE is', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' found here). Finally, NAMSPACE is a file that ensures our library plays nicely with others, and is more of a CRAN requirement.\\n\\nNaming Conventions\\n\\nWe must follow these rules when naming an R package:\\n\\nmust be unique on CRAN (you can check all current R libraries here);\\n\\n(you can check all current R libraries here); can only consist of letters , numbers and periods ;\\n\\n, and ; cannot contain an underscore or hyphen ;\\n\\nor ; must start with a letter ;\\n\\n; cannot end in a period;\\n\\nYou can read more about naming packages here. Our package name “datapeek” passes the above criteria. Let’', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='” passes the above criteria. Let’s head over to CRAN and do a Command+F search for “datapeek” to ensure it’s not already taken:\\n\\nCommand + F search on CRAN to check for package name uniqueness.\\n\\n…looks like we’re good.\\n\\nSTEP 2: Fill Out Description Details\\n\\nThe job of the DESCRIPTION file is to store important metadata about our package. These data include others packages required to run our library, our license, and our contact information. Technically, the definition of a package in R is any directory containing a DESCRIPTION file, so always ensure this is present.\\n\\nClick on the DESCRIPTION file in', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='.\\n\\nClick on the DESCRIPTION file in JupyterLab’s directory listing. You will see the basic details created automatically when we ran devtools::create(“datapeek”) :\\n\\nLet’s add our specific details so our package contains the necessary metadata. Simply edit this file inside JupyterLab. Here are the details I am adding:\\n\\nPackage : datapeek\\n\\n: Title : Provides useful functions for working with raw data.\\n\\n: Version : 0.0.0.1\\n\\n: Authors@R : person(“Sean”, “McClure”, email=”sean.mcclure', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"=”sean.mcclure@example.com”, role=c('aut','cre'))\\n\\n: Description : The datapeek package helps users transform raw data for machine learning development.\\n\\n: The datapeek package helps users transform raw data for machine learning development. Depends : R (≥ 3.5.1)\\n\\n: License : MIT\\n\\n: Encoding : UTF-8\\n\\n: LazyData: true\\n\\nOf course you should fill out these parts with your own details. You can read more about the definitions of each of these in Hadley’s chapter on metadata. As a brief overview…the package , title , and version parts are\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='…the package , title , and version parts are self-explanatory, just be sure to keep title to one line. Authors@R must adhere to the format you see above, since it contains executable R code. Note the role argument, which allows us to list the main contributors of our library. The usual ones are:\\n\\naut : author\\n\\ncre : creator or maintainer\\n\\nctb : contributors\\n\\ncph : copyright holder\\n\\nThere are many more options, with the full list found here.\\n\\nYou can add multiple authors by listing them as a vector:\\n\\nAuthors@R: as.person(c(\\n\\n\"Sean McClure <sean.mcclure@', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='ure <sean.mcclure@example.com> [aut, cre]\",\\n\\n\"Rick Deckard <rick.deckard@example.com> [aut]\",\\n\\n\" Rachael Tyrell <rachel.tyrell@example.com> [ctb]\"\\n\\n))\\n\\nNOTE: If you do plan on hosting your library on CRAN be sure your email address is correct, as CRAN will use this to contact you.\\n\\nThe description can be multiple lines, limited to 1 paragraph. We use depends to specify the minimum version of R our package depends on. You should use an R version equal or greater than the one you used to build your library. Most people today', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' you used to build your library. Most people today set their License to MIT, which permits anyone to “use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software” as long as your copyright is included. You can learn more about the MIT license here. Encoding ensures our library can be opened, read and saved using modern parsers, and LazyData refers to how data in our package are loaded. Since we set ours to true it means our data won’t occupy memory until they are used.\\n\\nSTEP 3: Add Functions\\n\\n3A: Add Functions to R Folder\\n\\nOur library wouldn’t do much without functions. Let’', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='�t do much without functions. Let’s add the 4 functions mentioned in the beginning of this article. The following GIST shows our datapeek functions in R:\\n\\nWe have to add our functions to the R folder, since this is where R looks for any functions inside a library.\\n\\ndatapeek\\n\\n├── R\\n\\n├── datapeek.Rproj\\n\\n├── DESCRIPTION\\n\\n├── NAMESPACE\\n\\nSince our library only contains 4 functions we will place all of them into a single file called utilities.R, with this file residing inside the R folder.\\n\\nGo into the directory in JupyterLab and open the R', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' in JupyterLab and open the R folder. Click on Text File in the Launcher and paste in our 4 R functions. Right-click the file and rename it to utilities.R.\\n\\n3B: Export our Functions\\n\\nIt isn’t enough to simply place R functions in our file. Each function must be exported to expose them to users of our library. This is accomplished by adding the @export tag above each function.\\n\\nThe export syntax comes from Roxygen, and ensures our function gets added to the NAMESPACE. Let’s add the @export tag to our first function:\\n\\nDo this for the remaining functions as well.\\n\\nNOTE: In larger libraries we would', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='.\\n\\nNOTE: In larger libraries we would only export functions that need to be usable outside our package. This helps reduce the chances of a conflict with another library.\\n\\n3C: Document our Functions\\n\\nIt is important to document our functions. Documenting functions provides information for users, such that when they type ?datapeek they get details about our package. Documenting also supports working with vignettes, which are a long-form type of documentation. You can read more about documenting functions here.\\n\\nThere are 2 sub-steps we will take:\\n\\nadd the document annotations\\n\\nthe document annotations run devtools::document()\\n\\n— Add the Document Annotations\\n\\nDocumentation is added above our function', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' Annotations\\n\\nDocumentation is added above our function, directly above our #’ @export line. Here’s the example with our first function:\\n\\nWe space out the lines for readability, adding a title, description, and any parameters used by the function. Let’s do this for our remaining functions:\\n\\n— Run devtools::document()\\n\\nWith documentation added to our functions we then run the following in terminal, just outside the root directory:\\n\\ndevtools::document()\\n\\nNOTE: Make sure you’re one level outside the datapeek directory.\\n\\nYou may get the error:\\n\\nError: ‘roxygen2’ >= 5.0.0', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"gen2’ >= 5.0.0 must be installed for this functionality.\\n\\nIn this case open terminal in JupyterLab and install roxygen2. You should also install data.table and mltools, since our first function uses these:\\n\\ninstall.packages('roxygen2')\\n\\ninstall.packages('data.table')\\n\\ninstall.packages('mltools')\\n\\nRun the devtools::document() again. You should see the following:\\n\\nThis will generate .Rd files inside a new man folder. You’ll notice an .Rd file is created for each function in our package.\\n\\nIf you look at your DESCRIPTION file it will now\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' you look at your DESCRIPTION file it will now show a new line at the bottom:\\n\\nThis will also generate a NAMESPACE file:\\n\\nWe can see our 4 functions have been exposed. Let’s now move onto ensuring dependencies are specified inside our library.\\n\\nSTEP 4: List External Dependencies\\n\\nIt is common for our functions to require functions found in other libraries. There are 2 things we must do to ensure external functionality is made available to our library’s functions:\\n\\nUse double colons inside our functions to specify which library we are relying on; Add imports to our DESCRIPTION file.\\n\\nYou’ll notice in the above GIST we simply listed our libraries at the', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' above GIST we simply listed our libraries at the top. While this works well in stand-alone R scripts it isn’t the way to use dependencies in an R package. When creating R packages we must use the “double-colon approach” to ensure the correct function is read. This is related to how “top-level code” (code that isn’t an object like a function) in an R package is only executed when the package is built, not when it’s loaded.\\n\\nFor example:\\n\\nlibrary(mltools) do_something_cool_with_mltools <- function() {\\n\\nauc_roc(preds, actuals)\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='_roc(preds, actuals)\\n\\n}\\n\\n…won’t work because auc_roc will not be available (running library(datapeek) doesn’t re-execute library(mltools)). This will work:\\n\\ndo_something_cool_with_mltools <- function() {\\n\\nmltools::auc_roc(preds, actuals)\\n\\n}\\n\\nThe only function in our datapeek package requiring additional packages is our first one:\\n\\nUsing the double-colon approach to specify dependent packages in R.\\n\\nNotice each time we call an external function we preface it with the external library and double colons.\\n\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' the external library and double colons.\\n\\nWe must also list external dependencies in our DESCRIPTION file, so they are handled correctly. Let’s add our imports to the DESCRIPTION file:\\n\\nBe sure to have the imported libraries comma-separated. Notice we didn’t specify any versions for our external dependencies. If we need to specify versions we can use parentheses after the package name:\\n\\nImports:\\n\\ndata.table (>= 1.12.0)\\n\\nSince our encode_and_bind function isn’t taking advantage of any bleeding-edge updates we will leave it without any version specified.\\n\\nSTEP 5: Add Data\\n\\nSometimes it makes sense to include data inside', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='\\n\\nSometimes it makes sense to include data inside our library. Package data can allow our user’s to practice with our library’s functions, and also helps with testing, since machine learning packages will always contain functions that ingest and transform data. The 4 options for adding external data to an R package are:\\n\\nbinary data parsed data raw data serialized data\\n\\nYou can learn more about these different approaches here. For this article we will stick with the most common approach, which is to add external data to an R folder.\\n\\nLet’s add the Iris dataset to our library in order to provide users a quick way to test our functions. The data must be in the .rda format, created using', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' be in the .rda format, created using R’s save() function, and have the same name as the file. We can ensure these criteria are satisfied by using devtools’ use_data function:\\n\\nAbove, I read in the Iris dataset from its URL and pass the data frame to devtools::use_data() .\\n\\nIn JupyterLab we see a new data folder has been created, along with our iris.rda dataset:\\n\\ndatapeek\\n\\n├── data\\n\\n└── iris.rda\\n\\n├── man\\n\\n├── R\\n\\n├── datapeek.Rproj\\n\\n├──', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='ek.Rproj\\n\\n├── DESCRIPTION\\n\\n├── NAMESPACE\\n\\nWe will use our added dataset to run tests in the following section.\\n\\nSTEP 6: Add Tests\\n\\nTesting is an important part of software development. Testing helps ensure our code works as expected, and makes debugging our code a much faster and more effective process. Learn more about testing R packages here.\\n\\nA common challenge in testing is knowing what we should test. Testing every function in a large library is cumbersome and not always needed, while not enough testing can make it harder to find and correct bugs when they arise.\\n\\nI like the following quote from Martin Fowler regarding when to test:\\n\\n“Whenever', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' regarding when to test:\\n\\n“Whenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead.” — Martin Fowler\\n\\nIf you prototype applications regularly you’ll find yourself writing to the console frequently to see if a piece of code returns what you expect. In data science, writing interactive code is even more common, since machine learning work is highly experimental. On one hand this provides ample opportunity to think about which tests to write. On the other hand, the non-deterministic nature of machine learning code means testing certain aspect of ML can be less than straightforward. As a general rule, look for obvious deterministic pieces of your code that should return the same output', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' pieces of your code that should return the same output every time.\\n\\nThe interactive testing we do in data science is manual, but what we are looking for in our packages is automated testing. Automated testing means we run a suite of pre-defined tests to ensure our package works end-to-end.\\n\\nWhile there are many kinds of tests in software, here we are taking about “unit tests.” Thinking in terms of unit tests forces us to break up our code into more modular components, which is good practice in software design.\\n\\nNOTE: If you are used to testing in languages like Python, notice that R is more functional in nature (i.e., methods belong to functions not classes), so there', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='., methods belong to functions not classes), so there will be some differences.\\n\\nThere are 2 sub-steps we will take for testing our R library:\\n\\n6A: Creating the tests/testthat folder;\\n\\n6B: Writing tests.\\n\\n— 6A: Creating the tests/testthat folder\\n\\nJust as R expects our R scripts and data to be in specific folders it also expects the same for our tests. To create the tests folder, we run the following in JupyterLab’s R console:\\n\\ndevtools::use_testthat()\\n\\nYou may get the following error:\\n\\nError: ‘testthat’ >= 1.0.2 must be', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"’ >= 1.0.2 must be installed for this functionality.\\n\\nIf so, use the same approach above for installing roxygen2 in Jupyter’s terminal.\\n\\ninstall.packages('testthat')\\n\\nRunning devtools::use_testthat() will produce the following output:\\n\\n* Adding testthat to Suggests\\n\\n* Creating `tests/testthat`.\\n\\n* Creating `tests/testthat.R` from template.\\n\\nThere will now be a new tests folder in our main directory:\\n\\ndatapeek\\n\\n├── data\\n\\n├── man\\n\\n├── R\\n\\n├── tests\\n\\n└──\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='\\n├── tests\\n\\n└── testthat.R\\n\\n├── datapeek.Rproj\\n\\n├── DESCRIPTION\\n\\n├── NAMESPACE\\n\\nThe above command also created a file called testthat.R inside the tests folder. This runs all your tests when R CMD check runs (we’ll look at that shortly). You’ll also notice testthat has been added under Suggests in our DESCRIPTION file:\\n\\n— 6B: Writing Tests\\n\\ntestthat is the most popular unit testing package for R, used by at least 2,600 CRAN package, not to mention libraries on Github. You can check out the latest news regarding', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' Github. You can check out the latest news regarding testthat on the Tidyverse page here. Also check out its documentation.\\n\\nThere are 3 levels to testing we need to consider:\\n\\nexpectation (assertion): the expected result of a computation;\\n\\nthe expected result of a computation; test: groups together multiple expectations from a single function, or related functionality from across multiple functions;\\n\\ngroups together multiple expectations from a single function, or related functionality from across multiple functions; file: groups together multiple related tests. Files are given a human readable name with context().\\n\\nAssertions\\n\\nAssertions are the functions included in the testing library we choose. We use assertions to check whether our own functions', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='. We use assertions to check whether our own functions return the expected output. Assertions come in many flavors, depending on what is being checked. In the following section I will cover the main tests used in R programming, showing each one failing so you can understand how it works.\\n\\nEquality Assertions\\n\\nexpect_equal()\\n\\nexpect_identical()\\n\\nexpect_equivalent\\n\\n# test for equality\\n\\na <- 10\\n\\nexpect_equal(a, 14) > Error: `a` not equal to 14. # test for identical\\n\\nexpect_identical(42, 2) > Error: 42 not identical to 2. # test for equivalence\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' identical to 2. # test for equivalence\\n\\nexpect_equivalent(10, 12) > Error: 10 not equivalent to 12.\\n\\nThere are subtle differences between the examples above. For example, expect_equal is used to check for equality within a numerical tolerance, while expect_identical tests for exact equivalence. Here are examples:\\n\\nexpect_equal(10, 10 + 1e-7) # true\\n\\nexpect_identical(10, 10 + 1e-7) # false\\n\\nAs you write more tests you’ll understand when to use each one. Of course always refer to the documentation referenced above when in doubt.\\n\\nTesting for String Matches\\n\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='.\\n\\nTesting for String Matches\\n\\nexpect_match()\\n\\n# test for string matching\\n\\nexpect_match(\"Machine Learning is Fun\", \"But also rewarding.\") > Error: \"Machine Learning is Fun\" does not match \"But also rewarding.\".\\n\\nTesting for Length\\n\\nexpect_length\\n\\n# test for length\\n\\nvec <- 1:10\\n\\nexpect_length(vec, 12) > Error: `vec` has length 10, not length 12.\\n\\nTesting for Comparison\\n\\nexpect_lt\\n\\nexpect_gt\\n\\n# test for less than\\n\\na <- 11\\n\\nexpect_lt(a, 10) > Error: `', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"lt(a, 10) > Error: `a` is not strictly less than 10. Difference: 1 # test for greater than\\n\\na <- 11\\n\\nexpect_gt(a, 12) > Error: `a` is not strictly more than 12. Difference: -1\\n\\nTesting for Logic\\n\\nexpect_true\\n\\nexpect_false\\n\\n# test for truth\\n\\nexpect_true(5 == 2) > Error: 5 == 2 isn't true. # test for false\\n\\nexpect_false(2 == 2) > Error: 2 == 2 isn't false.\\n\\nTesting for Outputs\\n\\nexpect_output\\n\\nexpect_message\\n\\n#\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='output\\n\\nexpect_message\\n\\n# testing for outputs\\n\\nexpect_output(str(mtcars), \"31 obs\") > Error: `str\\\\(mtcars\\\\)` does not match \"31 obs\". # test for warning\\n\\nf <-function(x) {\\n\\nif(x < 0) {\\n\\nmessage(\"*x* is already negative\")\\n\\n}\\n\\n} expect_message(f(1)) > Error: `f(1)` did not produce any messages.\\n\\nThere are many more included in the testthat library. If you are new to testing, start writing a few simple ones to get used to the process. With time you’ll build an', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' process. With time you’ll build an intuition around what to test and when.\\n\\nWriting Tests\\n\\nA test is a group of assertions. We write tests in testthat as follows:\\n\\ntest_that(\"this functionality does what it should\", {\\n\\n// group of assertions here\\n\\n})\\n\\nWe can see we have both a description (the test name) and the code (containing the assertions). The description completes the sentence, “test that ….”\\n\\nAbove, we are saying “test that this functionality does what it should.”\\n\\nThe assertions are the outputs we wish to test. For example:\\n\\ntest_that(\"trigonometric functions match identities\",', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='_that(\"trigonometric functions match identities\", {\\n\\nexpect_equal(sin(pi / 4), 1 / sqrt(2))\\n\\nexpect_equal(cos(pi / 4), 1 / sqrt(10))\\n\\nexpect_equal(tan(pi / 4), 1)\\n\\n}) > Error: Test failed: \\'trigonometric functions match identities\\'\\n\\nNOTE: It is worth considering the balance between cohesion and coupling with our test files. As stated in Hadley’s book, “the two extremes are clearly bad (all tests in one file, one file per test). You need to find a happy medium that works for you. A good starting place is to', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' works for you. A good starting place is to have one file of tests for each complicated function.”\\n\\nCreating Files\\n\\nThe last thing we do in testing is create files. As stated above, a“file” in testing is a group of tests covering a related set of functionality. Our test file must live inside the tests/testthat/ directory. Here is an example test file for the stringr package on GitHub:\\n\\nExample Test File from the stringr package on GitHub.\\n\\nThe file is called test-case.R (starts with “test”) and lives inside the tests/testthat/ directory. The context at the top simply allows us to provide a simple description of', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' top simply allows us to provide a simple description of the file’s contents. This appears in the console when we run our tests.\\n\\nLet’s create our test file, which will contain tests and assertions related to our 4 functions. As usual, we use JupyterLab’s Text File in Launcher to create and rename a new file:\\n\\nCreating a Test File in R\\n\\nNow let’s add our tests:\\n\\nFor the first function I am going to make sure a data frame with the correct number of features is returned:\\n\\nNotice how we called our encode_and_bind function, then simply checked the equality between the dimensions and the expected output. We run our automated', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' dimensions and the expected output. We run our automated tests at any point to ensure our test file runs and we get the expected output. Running devtools::test() in the console runs our tests:\\n\\nWe get a smiley face too!\\n\\nSince our second function removes a specified feature I will use the same test as above, checking for the dimensions of the returned frame. Our third function applies a specified function to a chosen column, so I will write a test that checks the result of given specified function. Finally, our fourth function returns the closest matching string, so I will simply check the returned string for the expected result.\\n\\nHere is our full test file:\\n\\nNOTE: Notice the relative path to the data in', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='NOTE: Notice the relative path to the data in the test file.\\n\\nTesting our Package\\n\\nAs we did above, we run our tests using the following command:\\n\\ndevtools::test()\\n\\nThis will run all tests in any test files we placed inside the testthat directory. Let’s check the result:\\n\\nWe had 5 assertions across 4 unit tests, placed in one test file. Looks like we’re good. If any of our tests failed we would see this in the above printout, at which point we would look to correct the issue.\\n\\nSTEP 7: Create Documentation\\n\\nThis has traditionally been done using “Vignettes” in R. You can learn about', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='ettes” in R. You can learn about creating R vignettes for your R package here. Personally, I find this a dated approach to documentation. I prefer to use things like Sphinx or Julep. Documentation should be easily shared, searchable and hosted.\\n\\nClick on the question mark at julepcode.com to learn how to use Julep.\\n\\nI created and hosted some simple documentation for our R datapeek library, which you can find here.\\n\\nOf course we will also have the library on GitHub, which I cover below.\\n\\nSTEP 8: Share your R Library\\n\\nAs I mentioned in the introduction we should be creating libraries on a regular basis, so others can', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' creating libraries on a regular basis, so others can benefit from and extend our work. The best way to do this is through GitHub, which is the standard way to distribute and collaborate on open source software projects.\\n\\nIn case you’re new to GitHub here’s a quick tutorial to get you started so we can push our datapeek project to a remote repo.\\n\\nSign up/in to GitHub and create a new repository.\\n\\n…which will provide us with the usual screen:\\n\\nWith our remote repo setup we can initialize our local repo on our machine, and send our first commit.\\n\\nOpen Terminal in JupyterLab and change into the datapeek directory:\\n\\nInitialize', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' the datapeek directory:\\n\\nInitialize the local repo:\\n\\ngit init\\n\\nAdd the remote origin (your link will be different):\\n\\ngit remote add origin https://github.com/sean-mcclure/datapeek.git\\n\\nNow run git add . to add all modified and new (untracked) files in the current directory and all subdirectories to the staging area:\\n\\ngit add .\\n\\nDon’t forget the “dot” in the above command. Now we can commit our changes, which adds any new code to our local repo.\\n\\nBut, since we are working inside a Docker container the username and email associated with our local repo', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\" container the username and email associated with our local repo cannot be autodetected. We can set these by running the following in terminal:\\n\\ngit config --global user.email {emailaddress}\\n\\ngit config --global user.name {name}\\n\\nUse the email address and username you use to sign into GitHub.\\n\\nNow we can commit:\\n\\ngit commit -m 'initial commit'\\n\\nWith our new code committed we can do our push, which transfers the last commit(s) to our remote repo:\\n\\ngit push origin master\\n\\nNOTE: Since we are in Docker you’ll likely get asked again for authentication. Simply add your GitHub username and password when prompted. Then run the\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' GitHub username and password when prompted. Then run the above command again.\\n\\nSome readers will notice we didn’t place a .gitignore file in our directory. It is usually fine to push all files inside smaller R libraries. For larger libraries, or libraries containing large datasets, you can use the site gitignore.io to see what common gitignore files look like. Here is a common R .gitignore file for R:\\n\\nExample .gitignore file for an R package\\n\\nTo recap, git add adds all modified and new (untracked) files in the current directory to the staging area. Commit adds any changes to our local repo, and push transfers the last commit(s) to our remote repo.', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' last commit(s) to our remote repo. While git add might seem superfluous, the reason it exists is because sometimes we want to only commit certain files, this we can stage files selectively. Above, we staged all files by using the “dot” after git add .\\n\\nYou may also notice we didn’t include a README file. You should indeed include this, however for the sake of brevity I have left this step out.\\n\\nNow, anyone can use our library. 👍 Let’s see how.\\n\\nSTEP 9: Install your R Library\\n\\nAs mentioned in the introduction I will not be discussing CRAN in this article. Sticking with GitHub make it easier to share', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='. Sticking with GitHub make it easier to share our code frequently, and we can always add CRAN criteria later.\\n\\nTo install a library from GitHub, users can simply run the following command on their local machine:\\n\\ndevtools::install_github(\"yourusername/mypackage\")\\n\\nAs such, we can simply instruct others wishing to use datapeek to run the following command on their local machine:\\n\\ndevtools::install_github(\"sean-mcclure/datapeek\")\\n\\nThis is something we would include in a README file and/or any other documentation we create. This will install our package like any other package we get from CRAN:\\n\\nUsers then load the', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' from CRAN:\\n\\nUsers then load the library as usual and they’re good to go:\\n\\nlibrary(datapeek)\\n\\nI recommend trying the above commands in a new R environment to confirm the installation and loading of your new library works as expected.\\n\\nCREATING LIBRARIES IN PYTHON\\n\\nCreating Python libraries follows the same high-level steps we saw previously for R. We require a basic directory structure with proper naming conventions, functions with descriptions, imports, specified dependencies, added datasets, documentation, and the ability to share and allow others to install our library.\\n\\nWe will use JupyterLab to build our Python library, just as we did for R.\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' library, just as we did for R.\\n\\nLibrary vs Package vs Module\\n\\nIn the beginning of this article I discussed the difference between a “library” and a “package”, and how I prefer to use these terms interchangeably. The same holds for Python libraries. “Modules” are another term, and in Python simply refer to any file containing Python code. Python libraries obviously contain modules as scripts.\\n\\nBefore we start:\\n\\nI stated in the introduction that we will host and install our libraries on and from GitHub. This encourages rapid creation and sharing of libraries without getting bogged down by publishing criteria on popular package hosting sites for R and Python.\\n\\nThe most popular hosting site', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' and Python.\\n\\nThe most popular hosting site for Python is the Python Package Index (PyPI). This is a place for finding, installing and publishing python libraries. Whenever you run pip install <package_name> (or easy_intall ) you are fetching a package from PyPI.\\n\\nWhile we won’t cover hosting our package on PyPI it’s still a good idea to see if our library name is unique. This will minimize confusion with other popular Python libraries and improve the odds our library name is distinctive, should we decide to someday host it on PyPI.\\n\\nFirst, we should follow a few naming conventions for Python libraries.\\n\\nPython Library Naming Conventions\\n\\nUse all', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='Python Library Naming Conventions\\n\\nUse all lowercase ;\\n\\n; Make the name unique on PyPI (search for name on PyPI)\\n\\non PyPI (search for name on PyPI) No hyphens (you can use underscore to separate words)\\n\\nOur library name is datapeek, so the first and third criteria are met; let’s check PyPI for uniqueness:\\n\\nAll good. 👍\\n\\nWe’re now ready to move through each step required to create a Python library.\\n\\nSTEP 1: Create Package Framework\\n\\nJupyterLab should be up-and-running as per the instructions in the setup section of this article.\\n\\nUse J', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' setup section of this article.\\n\\nUse JupyterLab’s New Folder and Text File options to create the following directory structure and files:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n└── utilities.py\\n\\n├── setup.py\\n\\nNOTE: Bold names are folders and light names are files. We will refer to the inner datapeek folder as the “module directory” and the outer datapeek directory as the “root directory.”\\n\\nThe following video shows me creating our datapeek directory in JupyterLab:\\n\\nThere will be files we do not want to', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='\\nThere will be files we do not want to commit to source control. These are files that are created by the Python build system. As such, let’s also add the following .gitignore file to our package framework:\\n\\nNOTE: At the time of this writing JupyterLab lacks a front-end setting to toggle hidden files in the browser. As such, we will simply name our file gitignore (no preceding dot); we will change it to a hidden file later prior to pushing to GitHub.\\n\\nAdd your gitignore file as a simple text file to the root directory:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n�', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='�── __init__.py\\n\\n└── utilities.py\\n\\n├── setup.py\\n\\n├── gitignore\\n\\nSTEP 2: Fill Out Description Details\\n\\nJust as we did for R, we should add metadata about our new library. We do this using Setuptools. Setuptools is a Python library designed to facilitate packaging Python projects.\\n\\nOpen setup.py and add the following details for our library:\\n\\nOf course you should change the authoring to your own. We will add more details to this file later. The keywords are fairly self-explanatory. url is the URL of our project on GitHub, which we will add later; unless you’ve already', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' will add later; unless you’ve already created your python repo, in which case add the URL now. We talked about licensing in the R section. zip_safe simply means our package can be run safely as a zip file which will usually be the case. You can learn more about what can be added to the setup.py file here.\\n\\nSTEP 3: Add Functions\\n\\nOur library obviously requires functions to be useful. For larger libraries we would organize our modules so as to balance cohesion/coupling, but since our library is small we will simply keep all functions inside a single file.\\n\\nWe will add the same functions we did for R, this time written in Python:\\n\\nAdd these functions to the', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\" in Python:\\n\\nAdd these functions to the utilities.py module, inside datapeek’s module directory.\\n\\nSTEP 4: List External Dependencies\\n\\nOur library will often require other packages as dependencies. Our user’s Python environment will need to be aware of these when installing our library (so these other packages can also be installed). Setuptools provides the install_requires keyword to list any packages our library depends on.\\n\\nOur datapeek library depends on the fuzzywuzzy package for fuzzy string matching, and the pandas package for high-performance manipulation of data structures. To specify our dependencies, add the following to your setup.py file:\\n\\ninstall_requires=[\\n\\n'\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\":\\n\\ninstall_requires=[\\n\\n'fuzzywuzzy',\\n\\n'pandas'\\n\\n]\\n\\nYour setup.py file should currently look as follows:\\n\\nWe can confirm all is in order by running the following in a JupyterLab terminal session:\\n\\npython setup.py develop\\n\\nNOTE: Run this in datapeek’s root directory.\\n\\nAfter running the command you should see something like this:\\n\\n…with an ending that reads:\\n\\nFinished processing dependencies for datapeek==0.1\\n\\nIf one or more of our dependencies is not available on PyPI, but is available on GitHub (e.g. a bleeding\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\" available on GitHub (e.g. a bleeding-edge machine learning package is only available on Github…or it’s another one of our team’s libraries hosted only on GitHub), we can use dependency_links inside our setup call:\\n\\nsetup(\\n\\n...\\n\\ndependency_links=['http://github.com/user/repo/tarball/master#egg=package-1.0'],\\n\\n...\\n\\n)\\n\\nIf you want to add additional metadata, such as status, licensing, language version, etc. we can use classifiers like this:\\n\\nsetup(\\n\\n...\\n\\nclassifiers=[\\n\\n'Development Status :: 3 - Alpha',\\n\\n\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"'Development Status :: 3 - Alpha',\\n\\n'License :: OSI Approved :: MIT License',\\n\\n'Programming Language :: Python :: 2.7',\\n\\n'Topic :: Text Processing :: Linguistic',\\n\\n],\\n\\n...\\n\\n)\\n\\nTo learn more about the different classifiers that can be added to our setup.py file see here.\\n\\nSTEP 5: Add Data\\n\\nJust as we did above in R we can add data to our Python library. In Python these are called Non-Code Files and can include things like images, data, documentation, etc.\\n\\nWe add data to our library’s module directory, so that any code that requires those data can use a\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' that any code that requires those data can use a relative path from the consuming module’s __file__ variable.\\n\\nLet’s add the Iris dataset to our library in order to provide users a quick way to test our functions. First, use the New Folder button in JupyterLab to create a new folder called data inside the module directory:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n└── utilities.py\\n\\n└── data\\n\\n├── setup.py\\n\\n├── gitignore\\n\\n…then make a new Text File inside the data folder called iris.csv, and paste the data', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' called iris.csv, and paste the data from here into the new file.\\n\\nIf you close and open the new csv file it will render inside JupyterLab as a proper table:\\n\\nCSV file rendered in JupyterLab as formatted table.\\n\\nWe specify Non-Code Files using a MANIFEST.in file. Create another Text File called MANIFEST.in placing it inside your root folder:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n└── utilities.py\\n\\n└── data\\n\\n├── MANIFEST.in\\n\\n├── setup.py\\n\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='\\n\\n├── setup.py\\n\\n├── gitignore\\n\\n…and add this line to the file:\\n\\ninclude datapeek /data/iris.csv\\n\\nNOTE: The MANIFEST.in is often not needed, but included in this tutorial for completeness. See here for more discussion.\\n\\nWe also need to include the following line in setup.py:\\n\\ninclude_package_data=True\\n\\nOur setup.py file should now look like this:\\n\\nSTEP 6: Add Tests\\n\\nAs with our R library we should add tests so others can extend our library and ensure their own functions do not conflict with existing code. Add a test folder to our library’', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='. Add a test folder to our library’s module directory:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n└── utilities.py\\n\\n└── data\\n\\n└── tests\\n\\n├── MANIFEST.in\\n\\n├── setup.py\\n\\n├── gitignore\\n\\nOur test folder should have its own __init__.py file as well as the test file itself. Create those now using JupyterLab’s Text File option:\\n\\ndatapeek\\n\\n├── datapeek\\n\\n└── __init__.py\\n\\n└── utilities', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='init__.py\\n\\n└── utilities.py\\n\\n└── data\\n\\n└── tests\\n\\n└── __init__.py\\n\\n└── datapeek_tests.py\\n\\n├── MANIFEST.in\\n\\n├── setup.py\\n\\n├── gitignore\\n\\nOur datapeek directory structure is now set to house test functions, which we will write now.\\n\\nWriting Tests\\n\\nWriting tests in Python is similar to doing so in R. Assertions are used to check the expected outputs produced by our library’s functions. We can use these “unit tests” to check a variety of expected outputs depending on what', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' to check a variety of expected outputs depending on what might be expected to fail. For example, we might want to ensure a data frame is returned, or perhaps the correct number of columns after some known transformation.\\n\\nI will add a simple test for each of our 4 functions. Feel free to add your own tests. Think about what should be checked, and keep in mind Martin Fowler’s quote shown in the R section of this article.\\n\\nWe will use unittest, a popular unit testing framework in Python.\\n\\nAdd unit tests to the datapeek_tests.py file, ensuring the unittest and datapeek libraries are imported:\\n\\nTo run these tests we can use Nose, which extends', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\" run these tests we can use Nose, which extends unittest to make testing easier. Install nose using a terminal session in JupyterLab:\\n\\n$ pip install nose\\n\\nWe also need to add the following lines to setup.py:\\n\\nsetup(\\n\\n...\\n\\ntest_suite='nose.collector',\\n\\ntests_require=['nose'],\\n\\n)\\n\\nOur setup.py should now look like this:\\n\\nRun the following from the root directory to run our tests:\\n\\npython setup.py test\\n\\nSetuptools will take care of installing nose if required and running the test suite. After running the above, you should see the following:\\n\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' the above, you should see the following:\\n\\nAll our tests have passed!\\n\\nIf any test should fail, the unittest framework will show which functions did not pass. At this point, check to ensure you are calling the function correctly and that the output is indeed what you expected. It can also be good practice to purposely write tests to fail first, then write your functions until they pass.\\n\\nSTEP 7: Create Documentation\\n\\nAs I mentioned in the R section, I use Julep to rapidly create sharable and searchable documentation. This avoids writing cryptic annotations and provides the ability to immediately host our documentation. Of course this doesn’t come with the IDE hooks that other documentation does, but for rapidly', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' IDE hooks that other documentation does, but for rapidly communicating it works.\\n\\nYou can find the documentation I create for this library here.\\n\\nSTEP 8: Share Your Python Library\\n\\nThe standard approach for sharing python libraries is through PyPI. Just as we didn’t cover CRAN with R, we will not cover hosting our library on PyPI. While the requirements are fewer than those associated with CRAN there are still a number of steps that must be taken to successfully host on PyPI. The steps required to host on sites other than GitHub can always be added later.\\n\\nGitHub\\n\\nWe covered the steps for adding a project to GitHub in the R section. The same steps apply here.\\n\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' section. The same steps apply here.\\n\\nI mentioned above the need to rename our gitignore file to make it a hidden file. You can do that by running the following in terminal:\\n\\nmv gitignore .gitignore\\n\\nYou’ll notice this file is no longer visible in our JupyterLab directory (it eventually disappears). Since JupyterLab still lacks a front-end setting to toggle hidden files simply run the following in terminal at anytime to see hidden files:\\n\\nls -a\\n\\nWe can make it visible again should we need to view/edit the file in JupyterLab, by running:\\n\\nmv .gitignore gitignore\\n\\nHere is a', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' .gitignore gitignore\\n\\nHere is a quick recap on pushing our library to GitHub (change git URL to your own):\\n\\nCreate a new repo on GitHub called datapeek_py\\n\\na new repo on GitHub called datapeek_py Initialize your library’s directory using git init\\n\\nyour library’s directory using Configure your local repo with your GitGub email and username (if using Docker) using:\\n\\ngit config --global user.email {emailaddress}\\n\\ngit config --global user.name {name}\\n\\nAdd your new remote origin using git remote add origin https://github.com/sean-mcclure/datapeek_py.', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='cclure/datapeek_py.git\\n\\nyour new remote origin using Stage your library using git add .\\n\\nyour library using Commit all files using git commit -m ‘initial commit’\\n\\nall files using Push your library to the remote repo using git push origin master (authenticate when prompted)\\n\\nNow, anyone can use our python library. 👍 Let’s see how.\\n\\nSTEP 9: Install your Python Library\\n\\nWhile we usually install Python libraries using the following command:\\n\\npip install <package_name>\\n\\n… this requires hosting our library on PyPI, which as explained above is beyond the scope of this article. Instead we will learn how to install', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' this article. Instead we will learn how to install our Python libraries from GitHub, as we did for R. This approach still requires the pip install command but uses the GitHub URL instead of the package name.\\n\\nInstalling our Python Library from GitHub\\n\\nWith our library hosted on GitHub, we simply use pip install git+ followed by the URL provided on our GitHub repo (available by clicking the Clone or Download button on the GitHub website):\\n\\npip install git+https://github.com/sean-mcclure/datapeek_py\\n\\nNow, we can import our library into our Python environment. For a single function:\\n\\nfrom datapeek.utilities import encode_and_bind\\n', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\".utilities import encode_and_bind\\n\\n…and for all functions:\\n\\nfrom datapeek.utilities import *\\n\\nLet’s do a quick check in a new Python environment to ensure our functions are available. Spinning up a new Docker container, I run the following:\\n\\nFetch a dataset:\\n\\nCheck functions:\\n\\nencode_and_bind(iris, 'species')\\n\\nremove_features(iris, ['petal_length', 'petal_width'])\\n\\napply_function_to_column(iris, ['sepal_length'], 'times_4', 'x*4')\\n\\nget_closest_string(['\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=\"\\nget_closest_string(['hey there','we we are','howdy doody'], 'doody')\\n\\nSuccess!\\n\\nSUMMARY\\n\\nIn this article we looked at how to create both R and Python libraries using JupyterLab running inside a Docker container. Docker allowed us to leverage Docker Stacks such that our environment was easily controlled and common packages available. This also made it easy to use the same high-level interface to create libraries through the browser for 2 different languages. All files were written to our local machine since we mounted a volume inside Docker.\\n\\nCreating libraries is a critical skill for any machine learning practitioner, and something I encourage others to do regularly. Libraries\", metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content=' and something I encourage others to do regularly. Libraries help isolate our work inside useful abstractions, improves reproducibility, makes our work shareable, and is the first step towards designing better software. Using a lightweight approach ensures we can prototype and share quickly, with the option to add more detailed practices and publishing criteria later as needed.\\n\\nAs always, please ask questions in the comments section should you run into issues. Happy coding.\\n\\nIf you enjoyed this article you might also enjoy:\\n\\nFURTHER READING AND RESOURCES', metadata={'Title': 'Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)'}),\n",
       " Document(page_content='Predictive Maintenance: detect Faults from Sensors with CNN\\n\\nPhoto by Bruce Warrington on Unsplash\\n\\nIn Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: it’s useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. For these reasons when a data scientist engages himself in this new field of battle has to follow a linear and rational approach, keeping in mind that the easiest solutions are always the better ones.\\n\\nIn this article, we will take a look at a classification problem. We will apply a simple but very powerful model made with CNN in Ker', metadata={'Title': 'Predictive Maintenance: detect Faults from Sensors with CNN'}),\n",
       " Document(page_content=' simple but very powerful model made with CNN in Keras and we will try to give a visual explanation of our results.\\n\\nTHE DATASET\\n\\nI decided to take a dataset from the evergreen UCI repository (Condition monitoring of hydraulic systems).\\n\\nThe data set was experimentally obtained with a hydraulic test rig. This test rig consists of a primary working and a secondary cooling-filtration circuit which are connected via the oil tank. The system cyclically repeats constant load cycles (duration 60 seconds) and measures process values such as pressures, volume flows and temperatures while the condition of four hydraulic components (cooler, valve, pump and accumulator) is quantitatively varied.\\n\\nWe can image to have a hydraulic pipe', metadata={'Title': 'Predictive Maintenance: detect Faults from Sensors with CNN'}),\n",
       " Document(page_content='\\n\\nWe can image to have a hydraulic pipe system which cyclically receives impulse due to e.g. the transition of particular type of liquid in the pipeline. This phenomenon lasts 60 seconds and was measured by different sensors (Sensor Physical quantity Unit Sampling rate, PS1 Pressure bar, PS2 Pressure bar, PS3 Pressure bar, PS4 Pressure bar, PS5 Pressure bar, PS6 Pressure bar, EPS1 Motor power, FS1 Volume flow, FS2 Volume flow, TS1 Temperature, TS2 Temperature, TS3 Temperature, TS4 Temperature, VS1 Vibration, CE Cooling efficiency, CP Cooling power, SE Efficiency factor) with different Hz frequencies.\\n\\nOur purpose is to predict the condition of four', metadata={'Title': 'Predictive Maintenance: detect Faults from Sensors with CNN'}),\n",
       " Document(page_content='\\nOur purpose is to predict the condition of four hydraulic components which compose the pipeline. These target condition values are annotated in the form of integer values (easy to encode) and say us if a particular component is…', metadata={'Title': 'Predictive Maintenance: detect Faults from Sensors with CNN'}),\n",
       " Document(page_content='Ethics in Machine Learning\\n\\nMost of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based on given attributes, classifying pictures into different categories, or teaching a computer the best way to play PAC-MAN — what do we do when we are asked to base predictions of protected attributes according to anti-discrimination laws?\\n\\nHow do we ensure that we do not embed racist, sexist, or other potential biases into our algorithms, be it explicitly or implicitly?\\n\\nIt may not surprise you that there have been several important lawsuits in the United States on this topic, possibly the most notably one involving Northpointe’s', metadata={'Title': 'Handling Discriminatory Biases in Data for Machine Learning'}),\n",
       " Document(page_content=' most notably one involving Northpointe’s controversial COMPAS — Correctional Offender Management Profiling for Alternative Sanctions — software, which predicts the risk that a defendant will commit another crime. The proprietary algorithm considers some of the answers from a 137-item questionnaire to predict this risk.\\n\\nIn February 2013, Eric Loomis was found driving a car that had been used in a shooting. He was arrested and pleaded guilty to eluding an officer. In determining his sentence, a judge looked not just to his criminal record, but also to a score assigned by a tool called COMPAS.\\n\\nCOMPAS is one of several risk-assessment algorithms now used around the United States to predict hot spots of violent crime, determine', metadata={'Title': 'Handling Discriminatory Biases in Data for Machine Learning'}),\n",
       " Document(page_content=' States to predict hot spots of violent crime, determine the types of supervision that inmates might need, or — as in Loomis’s case — provide information that might be useful in sentencing. COMPAS classified him as high-risk of re-offending, and Loomis was sentenced to six years.\\n\\nHe appealed the ruling on the grounds that the judge, in considering the outcome of an algorithm whose inner workings were secretive and could not be examined, violated due process. The appeal went up to the Wisconsin Supreme Court, who ruled against Loomis, noting that the sentence would have been the same had COMPAS never been consulted. Their ruling, however, urged caution and skepticism in the algorithm’s use.\\n', metadata={'Title': 'Handling Discriminatory Biases in Data for Machine Learning'}),\n",
       " Document(page_content=' skepticism in the algorithm’s use.\\n\\nThe case, understandably, caused quite a stir in the machine learning community — I doubt anyone would want to be judged by an algorithm, after all, you cannot blame an algorithm for being unethical, can you?', metadata={'Title': 'Handling Discriminatory Biases in Data for Machine Learning'}),\n",
       " Document(page_content='While still in Beta, BigQuery ML has been available since mid last year; however, I didn’t get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression — what’s not to like? After all, the ability to run ML models from the comfort of web-based SQL editor is a dream come-true for any analyst out there. Not only this platform eliminates the need to learn a programming language, be it R , Python or SAS; it also streamlines data engineering process by leveraging existing BigQuery data sources, instead of having to bring external data into your model. Effectively, this product removes a number of barriers to entry into this coveted', metadata={'Title': 'Getting Started with Google BigQuery’s Machine Learning — Titanic Dataset'}),\n",
       " Document(page_content=' removes a number of barriers to entry into this coveted data science specialty and democratizes the field of ML by allowing any analyst with adequate knowledge of SQL to run linear and logistic regression models without having to invest in pricey hardware, such as multi-core GPUs usually needed to support a scalable ML project. Below image does a great job showcasing platform’s capabilities:\\n\\nBigQuery ML demo from Google AI Blog\\n\\nMany aspiring data science students turn to the trusted Titanic: Machine Learning from Disaster data set from one of the most popular Kaggle competitions to practice working with binary classification models. In fact, for a beginner, a binary classification model is a fairly easy concept to grasp: your task is to simply predict whether a certain event will', metadata={'Title': 'Getting Started with Google BigQuery’s Machine Learning — Titanic Dataset'}),\n",
       " Document(page_content=' task is to simply predict whether a certain event will occur or will not happen; or whether a certain condition will evaluate to be true or false. For this problem, anyone is able to wrap their head around the concept of predicting whether a particular Titanic ship passenger survives one of the most monumental ship wrecks of all times: there are only two possibilities here. To follow along you simply need to log in to existing Kaggle account or create a new one and download all three files provides. The irony of using Kaggle website (purchased by Google back in 2017) and BigQuery platform (another Google product) is not lost on me. Let’s dive into this problem using sample data set and a working BigQuery instance.', metadata={'Title': 'Getting Started with Google BigQuery’s Machine Learning — Titanic Dataset'}),\n",
       " Document(page_content='Review: DeepPose — Cascade of CNN (Human Pose Estimation)\\n\\nIn this story, DeepPose, by Google, for Human Pose Estimation, is reviewed. It is formulated as a Deep Neural Network (DNN)-based regression problem towards body joints. With a cascade of DNN, high precision pose estimates are achieved. This is a 2014 CVPR paper with more than 900 citations. (Sik-Ho Tsang @ Medium)', metadata={'Title': 'Review: DeepPose — Cascade of CNN (Human Pose Estimation)'}),\n",
       " Document(page_content='Image used under licence from Getty Images.\\n\\nMicrosoft Introduction to AI — Part 1\\n\\nAre you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course and wrote course notes to help me retain the knowledge that I have learned. I have tried to write these notes in a basic way to make them easy to consume. I’ve recently become an aunt and have bought a few children’s books related to technology and space, I really love how the authors and illustrators have managed to simplify complicated topics. So, I’ve', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' simplify complicated topics. So, I’ve been inspired to treat these topics in a similar way by simplifying them to make them a lot more accessible.\\n\\n*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more here.*\\n\\nSummary\\n\\nThe Microsoft Introduction to AI course provides an overview of AI and explores machine learning principles that provide the foundation for AI. From the course you can discover the fundamental techniques that you can use to integrate AI capabilities into your apps. Learn how software can be used to process, analyse and extract meaning from natural language. Find out how software processes images and video to understand the world the way humans do. Learn about', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=\" understand the world the way humans do. Learn about how to build intelligent bots that enable conversations between humans and AI systems.\\n\\nImage created by the author. Microsoft Introduction to Artificial Intelligence Course\\n\\nThe course takes approximately 1 month to complete so 1 medium article I write contains 1 week's worth of content. This means that it would only take you approximately 18 minutes to read 1 week worth of content which is a fast way of learning. The course is free without a certificate however, if you’d like a certificate as proof of completion there is a fee. There are labs associated with this course that I won’t include in the notes as I believe the best way to learn is to actually do the labs. However, these notes\", metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' to actually do the labs. However, these notes are useful if you’d like to know about the fundamental theory behind AI and would like to learn it in a way that might be a lot simpler than other resources. I’ve tried to write it in layman terms and have included visuals to help illustrate the ideas. These notes are useful if you don’t have time to do the course, it’s a quick way to skim through the core concepts. Alternatively, if you have done the course like me you can use these notes to retain what you have learned.\\n\\nInstructor: Graeme Malcolm — Senior Content Developer at Microsoft Learning Experiences.\\n\\nSyllabus\\n\\nThe course is broken into', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='Syllabus\\n\\nThe course is broken into the four parts which include:\\n\\n1. Machine Learning (*this medium article will focus on just this section)\\n\\nLearn about the fundamentals about AI and machine learning.\\n\\nLearn how software can be used to process, analyse and extract meaning from natural language.\\n\\nLearn how software can be used to process images and video to understand the world the way that we do.\\n\\nFind out how to build intelligent bots that enable conversational communication between humans and AI systems.\\n\\nImage created by the author.\\n\\nMachine Learning\\n\\nThe ‘Machine Learning’ part of the course will tackle the following topics:\\n\\n· What is Artificial Intelligence? · What is Machine', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='· What is Artificial Intelligence? · What is Machine Learning? · Supervised · Unsupervised · Regression · Classification · Clustering\\n\\nIllustration by Michael Korfhage for HR Magazine SHRM.\\n\\nWhat is Artificial Intelligence?\\n\\nArtificial Intelligence (AI) is a way to enable people to accomplish more by collaborating with smart software. Think of it as putting a more human face on technology. AI is technology that can learn from vast amounts of data that is available in the modern world. Learning from this data it can understand our human kind of language and can respond in a similar kind of way. It’s technology that can see and interpret the world the way that we humans do.\\n\\nIllustration', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' way that we humans do.\\n\\nIllustration by Justin Middendorp.\\n\\nWhat is Machine Learning?\\n\\nMachine learning (ML) provides the foundation for artificial intelligence.\\n\\nSo what is it?\\n\\nMachine learning gives computers the ability to learn and make predictions or decisions based on data without explicitly programming that in. Well as the name suggests it’s a technique in which we train a software model using data. A model is a mathematical representation of a real-world process. The model learns from the training cases (training situations or examples) and then we can use the trained model to make predictions for new data cases. The key to this is to understand that fundamentally computers are very good at one thing and that', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' fundamentally computers are very good at one thing and that is performing calculations. To have a computer make intelligent predictions from the data, we just need a way to train it to perform the correct calculations.\\n\\nWe start with a dataset that contains historical records which we often call ‘cases’ or ‘observations’. Each observation includes numeric features. Numeric features are basically characteristics of the item we’re working with and they have a numeric value attached to each characteristic.\\n\\nIllustration by Vecteezy.\\n\\nLet’s call the numeric feature X.\\n\\nIn general, we also have some value that we’re trying to predict which we’ll call that Y. We', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' which we’ll call that Y. We use our training cases to train a machine learning model so that it can calculate a value for Y from the features in X. So in very simplistic terms, we’re creating a function that operates on a set of features ‘X’, to produce predictions ‘Y’. Don’t worry if this is confusing it will make more sense in the next sections where we start to apply real world examples.\\n\\nNow generally speaking, there are two broad types of machine learning and they are called supervised and unsupervised.\\n\\nSupervised\\n\\nIn supervised learning scenarios, we start with the observations that include known values for the variable that we want to predict.', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' values for the variable that we want to predict. We call these ‘labels’. Since we started with data that includes the label we’re trying to predict, we can train the model using only some of the data and withhold the rest of the data which we can use to evaluate the performance of the model. We then use a machine learning algorithm to train a model that fits the features to the known label.\\n\\nSince we started with the known label value we can validate the model by comparing the value predicted by the function to the actual label value that we knew. Then when we’re happy that the model works well, we can use it with new observations for which the label is unknown and generate new predicted values', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' which the label is unknown and generate new predicted values.\\n\\nIn this example we know the value of both X (numeric feature) and Y (variable we want to predict). Since we know X and Y we can use this algorithm to train our model. Once the model has been trained and we are happy that it works well we can use this model to calculate Y for when X is unknown. Illustration by Vecteezy.\\n\\nUnsupervised\\n\\nUnsupervised learning is different from supervised learning, in that this time we don’t have the known label values in the training dataset. We train the model by finding similarities between the observations. After the model is trained, each new observation is assigned to the cluster', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' trained, each new observation is assigned to the cluster of observations with the most similar characteristics.\\n\\nIn this example the value Y is unknown and so the way we train the model is through finding similarities between the observations. The observations are categorised in clusters that have similar characteristics. Once we train the model based on these clusters we can use it to predict the value of Y by assigning a new observation to a cluster. Illustration by Vecteezy.\\n\\nRegression\\n\\nOkay, let’s start with a supervised learning technique called ‘Regression’. Imagine we have some historic data about some health trials participants. We have information such as the exercise they have done, the number of calories they have spent and a', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=', the number of calories they have spent and a lot more stats and info about them. In this case we could use machine learning to predict how many calories any new participants might be expected to burn while engaging in some exercises. When we need to predict a numeric value, like for example an amount of money or a temperature or the number of calories then what we use is a supervised learning technique called regression.\\n\\nFor example, let’s suppose Rosy here is a participant in our health study. Here she is doing some weight exercises. We gather some data about Rosy when she first signed up for the study. We also gather data while she’s exercising and capture data using a fitness monitor smart watch.\\n\\nNow what', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' a fitness monitor smart watch.\\n\\nNow what we want do is model the calories burned using the features we have for Rosy’s exercise. These numeric features (X) are her age, weight, heart rate, duration, and so on. In this case we know all of the features and we know the label value (Y) of 231 calories. So we need an algorithm to learn the function that operates all of Rosy’s exercise features to give us a result of 231.\\n\\nIllustration by Vecteezy.\\n\\nNow of course a sample of only one person isn’t likely to give us a function that generalises well. So what we need to do is gather the same sort of', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' we need to do is gather the same sort of data from lots of diverse participants and train our model based on this larger set of data.\\n\\nIllustration by Vecteezy.\\n\\nAfter we’ve trained the model and we have a generalised function that can be used to calculate our label Y, we can then plot the values of Y calculated for specific features of X values on a chart like this.\\n\\nImage created by the author.\\n\\nWe can then interpolate any new values of X to predict an unknown Y.\\n\\nImage created by the author.\\n\\nNow because we started with data that includes the label we are trying to predict, we can train the model using only some of the data and', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' train the model using only some of the data and withhold the rest of the data for evaluating model performance.\\n\\nThen we can use the model to predict (F(X)) for evaluation data and compare the predictions or scored labels to the actual labels that we know to be true. The difference between the predicted and actual levels are what we call the ‘residuals’. Residuals can tell us something about the level of error in the model.\\n\\nImage created by the author.\\n\\nNow there are a few ways we can measure the error in the model and these include root-mean-square error (RMSE) and mean absolute (MAE). Now both of these are absolute measures of error in', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' Now both of these are absolute measures of error in the model.\\n\\nImage created by the author.\\n\\nFor example an RMSE value of 5 would mean that the standard deviation of error from our test error is 5 calories. Of course absolute values can vary wildly depending on what you are predicting. An error of 5 calories would seem to indicate a reasonably good model. But if we were predicting how long an exercise session takes an error of 5 hours would indicate a very bad model.\\n\\nSo you might want to evaluate the model using relative metrics to indicate a more general level of error as a relative value between 0 and 1. Relative absolute error (RAE) and relative squared error (RSE) produce a metric where the closer to', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='RSE) produce a metric where the closer to 0 the error, the better the model.\\n\\nImage created by the author.\\n\\nThe coefficient of determination (CoD(R2)) which we sometimes call R squared is another relative metric but this time a value closer to 1 indicates a good fit for the model.\\n\\nImage created by the author.\\n\\nClassification\\n\\nSo we’ve seen how to train a regression model to predict a numeric value. Now it’s time to look at another kind of supervised learning called classification. Classification is a technique that we can use to predict which class or category that something belongs to. The simplest variant of this is binary classification (ones and zeros) where we', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' binary classification (ones and zeros) where we predict whether an entity belongs to one of two classes. It’s often used to determine if something is true or false about the entity.\\n\\nFor example, suppose we take a number of patients in our health clinic and we gather some personal details. We run some tests and we can identify which patients are diabetic and which are not. We can learn a function that can be applied to these patient features and give the result 1 for patients that are diabetic and 0 for patients that aren’t.\\n\\nIllustration by Vecteezy.\\n\\nMore generally, a binary classifier is a function that can be applied to features X to produce a Y value of 1 or 0', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' X to produce a Y value of 1 or 0.\\n\\nIllustration by Vecteezy.\\n\\nNow the function won’t actually calculate an absolute value of 1 or 0, it will calculate a value between 1 and 0. We will use a threshold value (dotted line in diagram) to decide whether the result should be counted as a 1 or a 0.\\n\\nThe threshold is represented as the dotted line. Image created by the author.\\n\\nWhen you use the model to predict values, the resulting value is classed as a 1 or a 0 depending on which side of the threshold line it falls.\\n\\nImage created by the author.\\n\\nBecause classification is a supervised learning technique we withhold some of', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' classification is a supervised learning technique we withhold some of the test data to validate the model using known labels.\\n\\nImage created by the author.\\n\\nCases where the model predicts a 1 for a test observation that actually has a label value of 1 these are considered true positives (TP).\\n\\nImage created by the author.\\n\\nSimilarly cases where the model predicts 0 and the actual label is 0 these are true negatives (TN).\\n\\nImage created by the author.\\n\\nNow the choice of threshold determines how predictions are assigned to classes. In some cases a predicted value might be very close to the threshold but is still misclassified. You can move the threshold to control how the predicted values are classified. In the case of the', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' predicted values are classified. In the case of the diabetes model it might be better to have more false positives (FP) but reduce the number of false negatives (FN) so that more people who are at risk of diabetes get identified.\\n\\nImage created by the author.\\n\\nThe number of true positives (TP), false positives (FP), true negatives (TN), and false negative (FN) produced by your model is crucial in evaluating its effectiveness.\\n\\nImage created by the author.\\n\\nThe grouping of these are often shown in what’s called a confusion matrix shown below. This provides the basis for calculating performance metrics for the classifier. The simplest metric is accuracy which is just the number of correctly classified cases divided by', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' is just the number of correctly classified cases divided by the total number of cases.\\n\\nImage created by the author.\\n\\nIn this case there are 5 true positives (TP) and 4 true negatives (TN). There are also 2 false positives (FP) and no false negatives (FN). That gives us 9 correct predictions out of a total of 11 which is an accuracy of 0.82 or 82%.\\n\\nImage created by the author.\\n\\nNow that might seem like a really good result but perhaps surprisingly accuracy actually isn’t all that useful as a measure of a model’s performance. Suppose that only 3% of the population is diabetic. I can create a model that simply always predicts zero and it would be', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' model that simply always predicts zero and it would be 97% accurate but it’s completely useless for identifying potential diabetics.\\n\\nA more useful metric might be the fraction of cases classified as positive that are actually positive. This metric’s known as precision. In other words out of all the cases classified as positives which ones are true and not false alarms.\\n\\nImage created by the author.\\n\\nIn this case there are 5 true positives, and 2 false positives. So our precision is 5 / (7) which is 0.71 or 71% of our cases identified as positive are really diabetic and 29% are false alarms.\\n\\nImage created by the author.\\n\\nIn some situations we might want a', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='.\\n\\nIn some situations we might want a metric that’s sensitive to the fraction of positive cases we correctly identify. We call this metric recall. Recall is calculated as the number of true positives divided by the combined true positives and false negatives. In other words, what fraction of positive cases are correctly identified?\\n\\nImage created by the author.\\n\\nIn this case, there are 5true positives and no false negatives. So our recall is 5 out of 5 which is of course is 1 or 100%. So in this case we’re correctly identifying all patients with diabetes. Now recall actually has another name sometimes it’s known as the True Positive Rate.\\n\\nImage created by the author.\\n\\nThere�', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='Image created by the author.\\n\\nThere’s an equivalent rate for false positives compared to the actual number of negatives. In this case we have 2 false positives and 4 true negatives. So our false positive rate is 2/(6) which is 0.33.\\n\\nImage created by the author.\\n\\nNow you may remember that the metrics we got were based on a threshold (blue dotted line) of around 0.3 and we can plot the true positive rate against the false positive rate for that threshold like this.\\n\\nImage created by the author.\\n\\nIf we were to move the threshold back to 0.5 our true positive rate becomes 4 out of 5 or 0.8. Our false positive rate is 1', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' 0.8. Our false positive rate is 1 out of 6 or 0.16 which we can plot here.\\n\\nImage created by the author.\\n\\nMoving the threshold further to say 0.7 gives us a true positive rate of 2 out of 5 or a 0.4 and a false positive rate of 0 out of 6 or 0.\\n\\nImage created by the author.\\n\\nIf we plotted this for every possible threshold rate we would end up with a curved line as shown in the diagram below. Now this is known as a receiver operator characteristic, a ROC chart. Now the area under the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. A perfect classifier would go straight up the left and then along the top giving an AUC of one. Now, you can always compare with a diagonal line and that represents how well the model would perform if you simply made a 50–50 guess. It’s an AUC of 0.5. So you’re just simply random guessing 50% of the time true, 50% false. In this case, our model has an AUC of 0.9 which means that our model is definitely outperforming guessing.\\n\\nThe area under the curve (AUC) is an indication of', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. What is shown in the blue graph is a good example of a model that is outperforming a 50–50 guess. Image created by the author.\\n\\nClustering\\n\\nWell, we’ve seen some examples of supervised learning specifically regression and classification but what about unsupervised learning? Now with unsupervised learning techniques you don’t have a known label with which to train the model. But you can still use an algorithm that finds similarities in data observations in order to group them into clusters.\\n\\n', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' in order to group them into clusters.\\n\\nSuppose for example our health clinic has a website that contains links to articles and medical and healthy lifestyle publications. Now I might want to automatically group similar articles together.\\n\\nIllustration by Vecteezy.\\n\\nOr maybe I want to segment our study participants and we can categorise them based on similar characteristics.\\n\\nIllustration by Vecteezy.\\n\\nThere are a number of ways we can create a clustering model and we’re going to look at one of the most popular clustering techniques which is called k-means clustering.\\n\\nImage created by the author.\\n\\nNow the key to understanding k-means is to remember that', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' to understanding k-means is to remember that our data consists of rows of data and each row has multiple features. Now if we assume that each feature is a numeric value then we can plot them as coordinates. Now here we’re plotting two features on a two dimensional grid. But in reality, multiple features would be plotted in n-dimensional space.\\n\\nWe then decide how many clusters we want to create which we call k. We plot k points at random locations that represent the center points of our clusters.\\n\\nk points are represented as the stars in the diagram. Image created by the author.\\n\\nIn this case, k is 3 so we’re creating 3 clusters. Next, we identify which of the', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' 3 clusters. Next, we identify which of the three centroids each point is closest to and assign the points to clusters accordingly.\\n\\nImage created by the author.\\n\\nThen we move each centroid to the true center of the points and its cluster.\\n\\nImage created by the author.\\n\\nWe then reallocate the points in the cluster based on their nearest centroid.\\n\\nImage created by the author.\\n\\nWe just repeat that process until we have nicely separated clusters.\\n\\nImage created by the author.\\n\\nSo what do I mean by nicely separated? Well, we want a set of clusters that separate data by the greatest extent possible. To measure this we can compare the average distance between the', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' measure this we can compare the average distance between the cluster centers.\\n\\nImage created by the author.\\n\\nIn addition, the average distance between the points in the cluster and their centers.\\n\\nImage created by the author.\\n\\nClusters that maximise this ratio have the greatest separation. We can also use the ratio of the average distance between clusters and the maximum distance between the points and the centroid of the cluster.\\n\\nImage created by the author.\\n\\nNow another way we could evaluate the results of a clustering algorithm is to use a method called principal component analysis (PCA). In this method we decompose the points in a cluster into directions. We represent the first two components of the PCA decomposition', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' the first two components of the PCA decomposition as an ellipse.\\n\\nImage created by the author.\\n\\nThe first principal component is along the direction of the maximum variance or major axis of the ellipse and the second PCA is along the minor axis of the ellipse. A cluster that is perfectly separate from the first cluster shows up as an ellipse with the major axis of the ellipse perpendicular to the ellipse of the first cluster.\\n\\nImage created by the author.\\n\\nEvery second cluster is reasonably well separated but not perfectly separated.\\n\\nImage created by the author.\\n\\nThen it will have a major axis that is not quite perpendicular to the first ellipse.', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' not quite perpendicular to the first ellipse. If the second cluster is quite poorly separated from the first then the major axis of both ellipses will be nearly collinear.\\n\\nImage created by the author.\\n\\nSo the ellipse may be more like a circle because the second cluster is less well defined.\\n\\nFinal Word\\n\\nThanks for reading this article, Part 1 of the Microsoft Introduction to Artificial Intelligence course. If you found this helpful then check out all 4 parts on my Medium account or in Towards Data Science. If you had some trouble with some of the concepts in this article (don’t worry it took me awhile for the information to sink in) and you need a bit more info, then', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=') and you need a bit more info, then enrol for free in the Microsoft Introduction to AI course. It’s helpful to watch the course videos alongside with these notes.\\n\\n*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more through here.*\\n\\nA little background\\n\\nHi, I’m Christine :) I’m a product designer who’s been in the digital field for quite some time and have worked at many different companies; from large companies (as large as 84,000 employees), to mid size and to very small startups still making a name for themselves. Despite having a lot of experience I’m', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content=' Despite having a lot of experience I’m a product designer who has a fear of suffering from the dunning-kruger effect and so I’m continuously trying to educate myself and I’m always searching for more light. I believe to be a great designer you need to constantly hone your skills especially if you are working in the digital space which is constantly in motion.', metadata={'Title': 'Microsoft Introduction to AI — Part 1'}),\n",
       " Document(page_content='When we face computer vision project, first of all we need to load the images before any preprocessing.\\n\\nThere are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through:\\n\\nLibraries for loading image Colour channel Efficiency Cheatsheet!\\n\\nLibrary for loading image\\n\\nThere are four libraries that are usually used for loading images.\\n\\nMatplotlib — plt.imread()\\n\\nOpenCV — cv2.imread()\\n\\nPillow — Image.open()\\n\\nscikit-image — io.imread()\\n\\nimport matplotlib.pyplot as plt img =', metadata={'Title': 'What libraries can load image in Python and what are their difference?'}),\n",
       " Document(page_content='plotlib.pyplot as plt img = plt.imread(img_dir) import cv2 img = cv2.imread(img_dir)\\n\\nfrom PIL import Image img = Image.open(img_dir)\\n\\nfrom skimage import io img = io.imread(img_dir)\\n\\n\\n\\nColour channel\\n\\nAfter loading the image, usually plt.imshow(img) will be used to plot the images. Let’s plot some doge!\\n\\nYou may spot that the OpenCV image above looks odd. It is because matplotlib, PIL and skimage represent image in RGB (Red, Green, Blue) order,', metadata={'Title': 'What libraries can load image in Python and what are their difference?'}),\n",
       " Document(page_content=' RGB (Red, Green, Blue) order, while OpenCV is in reverse order! (BGR — Blue, Green, Red)\\n\\nEasy Fix', metadata={'Title': 'What libraries can load image in Python and what are their difference?'}),\n",
       " Document(page_content='Repetition in Songs: A Python Tutorial\\n\\nCredit: Unsplash\\n\\nEveryone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song …in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections.\\n\\nIn his journal article called “The complexity of Songs”, computer scientist Donald Knuth capitalized on the tendency of popular songs to devolve from long and content-rich ballads to highly repetitive texts. As some may', metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content=' ballads to highly repetitive texts. As some may waste no time agreeing with his notion, it does raise some questions like: Does repetitiveness really help songs become a hit? Is music really becoming more repetitive over time?\\n\\nIn an attempt to teach some basic python code in the form of a case study, I am going to test this hypothesis (Are popular songs really repetitive?)with one of my favorite songs. One way to test this hypothesis is to figure out the unique words and calculate the fraction of those words to the total number of words in a song.\\n\\nIn this tutorial, we’ll cover:\\n\\nVariables and data types\\n\\nLists and Dictionaries\\n\\nBasic Arithmetic operations\\n\\nBuilt', metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content='aries\\n\\nBasic Arithmetic operations\\n\\nBuilt-in Functions and Loops\\n\\nPrerequisite Knowledge\\n\\nTo get the most out of this tutorial, you can follow along by running the codes yourself.\\n\\nThe music we will be using is entitled ‘Perfect’ by Ed Sheeran. You can copy the lyrics here. However, the lyrics I am using in this analysis was cleaned out to get a conclusive result. For example, I changed words like “we’ll” to “we will” etc. You can get my version of the lyrics here The editor used was Jupiter NoteBook. Here is a quick tutorial on how to install and use it.\\n\\nFor the purpose of this', metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content=\" use it.\\n\\nFor the purpose of this case study, we will streamline our hypothesis by asking two major questions:\\n\\nHow many unique words were used compared to the whole lyrics of our case study song — Perfect by Ed Sheeran?\\n\\nWhat are the most repetitive words used and how many times were they used throughout the song?\\n\\nLet's get started analyzing already\\n\\nThe Basic\\n\\n1. A String is a list of characters. A character is anything you can type on the keyboard in one keystroke, like a letter, a number, or a backslash. However, Python recognizes strings as anything that is delimited by quotation marks either a double quote (“ “) or a single\", metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content=' quote (“ “) or a single quote (‘ ‘) at the beginning and end of a character or text. For example: ‘Hello world’\\n\\nFor this case study, a string is our lyrics as seen below\\n\\n2. Variables are typically descriptive names, words or symbols used to assign or store values. In other words, they are storage placeholders for any datatype. It is quite handy in order to refer to a value at any time. A variable is always assigned with an equal sign, followed by the value of the\\n\\nvariable. (A way to view your code output is to use a print function. As you may already know with Jupyter notebook,', metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content=\" may already know with Jupyter notebook, an output can be viewed without a print function)\\n\\nTo store the lyrics, we will assign it a variable named perfect_lyrics .\\n\\n3. Lists can be created simply by putting different comma-separated values between square brackets. It can have any number of items and they may be of different types (integer, float, string etc.). It can even have another list as an item. For example:\\n\\nlist1 = [1,'mouse', 3.5, [2,'is',5.0]]\\n\\n#3.5 is a float\\n\\nNow that we have gotten a sense of what a list looks like. Let go back to our data.\", metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content=' looks like. Let go back to our data.\\n\\nSince one of our aims is to figure out the number of unique words used, it means we will need to do a bit of counting i.e to count each word. In order to achieve these, we will not only have to put our string into a list but will have to separate each word using a .split() method. Therefore our dataset will look like this\\n\\nInput\\n\\nOutput', metadata={'Title': 'Repetition in Songs: A Python Tutorial'}),\n",
       " Document(page_content='Transfer Learning Intuition for Text Classification\\n\\nThis post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I delved deeper into Deep learning models and', metadata={'Title': 'Transfer Learning Intuition for Text Classification'}),\n",
       " Document(page_content=', I delved deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach for NLP.\\n\\nAs a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\\n\\nBefore introducing the notion of transfer learning to', metadata={'Title': 'Transfer Learning Intuition for Text Classification'}),\n",
       " Document(page_content='\\n\\nBefore introducing the notion of transfer learning to NLP applications, we will first need to understand a little bit about Language models.\\n\\nLanguage Models And NLP Transfer Learning Intuition:\\n\\nIn very basic terms, the objective of the language model is to predict the next word given a stream of input words. In the past, many different approaches have been used to solve this particular problem. Probabilistic models using Markov assumption is one example of this sort of models.\\n\\nIn the recent era, people have been using RNNs/LSTMs to create such language models. They take as input a word embedding and at each time state return the probability distribution of next word probability over the dictionary words. An', metadata={'Title': 'Transfer Learning Intuition for Text Classification'}),\n",
       " Document(page_content=' of next word probability over the dictionary words. An example of this is shown below in which the…', metadata={'Title': 'Transfer Learning Intuition for Text Classification'}),\n",
       " Document(page_content='Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldn’t you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability?\\n\\nBefore going any further we should try to clarify what makes a model interpretable. Often, interpretability is equated with simplicity. This definition is obviously ambiguous; what is simple for one person may not be so for another. More importantly, advanced machine learning is most useful for modeling complex systems. These systems are called complex for a reason — they are not simple! Demanding that useful models for such systems', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' simple! Demanding that useful models for such systems should be simple, by any measure, makes very little sense.\\n\\nPerhaps, we can go further if we consider how we think about interpretability in a traditional modeling setting. Consider the mathematical model of an internal combustion engine in a car. I doubt if anyone would consider that to be simple. But, on the other hand, I also doubt if anyone would consider the model of an internal combustion engine to be non-interpretable, either. This is primarily because we can derive this model in a deductive manner from well established physical theories such as thermodynamics and fluid dynamics.\\n\\nWhy not use this as our definition of interpretability? A model should be considered to be interpretable if', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' A model should be considered to be interpretable if it can be derived (or at least motivated) from a trustworthy theory. This definition of interpretability serves the dual purpose of understanding and trust. It helps us understand the model because we tend to understand things in a deductive manner — by going from the known to the unknown. Also, with such a definition, the trust in the model is derived from the trust that we place in the underlying theory.\\n\\nIndeed, there are situations where both understanding and trust are necessary — scenarios where we are interested in determining the causal factors behind the behavior of a system. In such scenarios, we must insist that the corresponding models must be interpretable according to the above definition. Most models in the realm of', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' the above definition. Most models in the realm of physical sciences belong to this category. One can argue that purely inductive blackbox models are not suitable for such scenarios.\\n\\nHowever, there are many other situations where understanding might be nice to have, but by no means is it a must have. In these situations what really matters is the ability to make trustworthy predictions. In these situations, if we could provide an alternate source of trust, then our models need not be bound by the definition of interpretability given above. This is a common argument, and there is merit to it. Remember, machine learning is a way of systematically building models from (preferably) large amounts of data using inductive reasoning. Constraining these models to', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' inductive reasoning. Constraining these models to be interpretable in a deductive manner can seriously limit their accuracy.\\n\\nSo then the question becomes how can we generate trust in a blackbox model where we have little to no insight into its inner workings. A credible basis for trust could be testing. After all, testing forms the basis of our trust in regular software. But to test a model we need be able to formalize our expectations about it. If we could formalize our expectations completely then that would correspond to a complete specification of the model itself. In that case, we would not really need machine learning or any other modeling methodology. What we really need to be able to do is to formalize our expectations about the aspects of', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' is to formalize our expectations about the aspects of the model that we consider important. This is not easy either, because many of the concepts that we care about, such as fairness, do not lend themselves to a convenient mathematical treatment.\\n\\nIt is worth pointing out that significant progress has been made in developing testing methodologies for testing machine learning models. I personally find the idea of using metamorphic relations for formalizing expectations to be particularly promising. But, we are still a long way from having concrete methodologies that will allow us to perform comprehensive testing of blackbox models, and this inability of ours contributes to a trust deficit in blackbox models.\\n\\nOne could question the efficacy of such expectation-based comprehensive testing. After all,', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' such expectation-based comprehensive testing. After all, the goal of machine learning is to find undiscovered patterns in data. By insisting that the models meet our expectations amounts to pre-defining the model, which defeats the whole purpose. Following this line of reasoning, one would argue that as long as the data is representative and our algorithms are powerful enough to capture the patterns, there is little reason not to trust the model — we should expect the model results to generalize to the overall population, and the extent to which we should expect them to generalize is encapsulated in the model’s performance (accuracy) scores.\\n\\nThus, in essence we are asked to delegate our trust to the trifecta of data, algorithms', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' to the trifecta of data, algorithms and performance scores. We first need to dissuade ourselves from the notion that a single performance (accuracy) score can form sufficient basis for trusting the model. A performance score is usually a point estimate of how a model is expected to generalize on an average over a population given the current data. Trust, on the other hand, is a nuanced multidimensional concept that cannot be encapsulated in such a single coarse grained score. One can imagine defining more granular performance scores— e.g. by population segments. But, that would require a certain level of understanding of the population and determining what we consider important — this is not very different from forming expectations.\\n\\nLet us examine', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' different from forming expectations.\\n\\nLet us examine the data aspect of this argument. It is, indeed, quite easy to convince oneself that if the data is representative of the population we are interested in, then it should contain all the relevant patterns and no spurious ones. Unfortunately, that is rarely the case. The degree to which the data can be non-representative depends quite acutely on the situation. Nonetheless, we can identify certain high level scenarios.\\n\\nIn the first scenario, we would have a good understanding of the population and complete control over the data collection mechanism. In this scenario, we can choose our data to be representative, and with a high degree of confidence we can expect our resulting model’s predictions to be applicable to', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' resulting model’s predictions to be applicable to the overall population. However, note that having a good enough understanding of the population to be able to draw a representative sample for the task at hand means that we already have some understanding of which features are important for the prediction. Hence, in this case it is debatable if blackbox models are terribly useful. Opinion polling for predicting election results is a good example of this scenario.\\n\\nIn the second scenario, we do not have complete control over the data collection, but our predictions do not affect the data collected. In this scenario, if we assume that the data collection mechanism is unbiased then were we to wait long enough, we would have a representative sample of the population. Of course, there', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' representative sample of the population. Of course, there are a lot of ifs and buts that go with this assumption. Firstly, one does not know how long is long enough. Thus one needs to assume that the time scale over which the data is collected is long enough to produce a representative sample. Furthermore, the population itself might change in the meantime. Thus, an additional assumption is that the time scale over which population changes is much longer than the time scale over which a representative sample is generated. As long as we can justify those assumptions, then the estimated performance will be reliable. A model for predicting the stock prices is an example of such a scenario — as long as we are not making investments that are large enough to tip the market as', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' investments that are large enough to tip the market as a whole, the decisions that we make as the result of the predictions should not affect the stock prices.\\n\\nThe third scenario is one where the data collection is impacted by the predictions, but we have a moderate to high risk appetite for wrong predictions. An example of this is a product recommender system. The model for a recommender systems will be trained on data consisting of ordered lists of products that different users have bought/clicked on. Based on this data the model will predict what a user is most likely to buy/click on and based on the model’s predictions the system will decide what the user gets to see, which limits what (s)he can buy/click', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' limits what (s)he can buy/click-on. Thus the prediction biases the data collection. In product recommender systems, one can circumvent this problem, somewhat, by keeping an exploration budget — for a fraction of the cases the system shows the user a random set of products regardless of the prediction of the model. The observations resulting from these randomized predictions can then be used to estimate the performance of the model. One still has to address the concerns of the aforementioned second scenario in order to access the reliability of these estimates.\\n\\nIn the fourth and final scenario, the data collection is impacted by the predictions, but we have little to no risk appetite for wrong predictions. For example, suppose we have to build a model to predict whether someone', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' we have to build a model to predict whether someone will default on their mortgage loan payments. The mortgage loan will be approved or not based on the prediction. If the prediction is that the person will default, then the loan will not be approved, and in that case there is no way of knowing whether this person would have actually defaulted or not. It is difficult to imagine a situation where an institution would randomly approve (or otherwise) a loan for the sake of data exploration. In these situations, it is very difficult to gauge the reliability of the estimated performance of the resulting model without additional information.\\n\\nThus, it is not such a great idea to blindly expect the data to be representative of the population. In most scenarios, given the constraints', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' the population. In most scenarios, given the constraints of the problem at hand, it simply might not be possible to get an unbiased representative sample. Understanding the limitations of one’s data collection mechanism, being able to deduce the implications of those limitations, and having the honesty to report those as a part of the model’s results goes a long way in building trust.\\n\\nLet us now consider the algorithm aspect of the argument. It is a widespread belief that the more flexible an algorithm is the better it is, because flexibility equips an algorithm to capture more complex patterns. But if the history of the actual successful applications of machine learning are anything to go by, then this belief would appear to be utterly misplaced. In computer vision', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' would appear to be utterly misplaced. In computer vision, success came when we were able to encode the symmetries in pictures into models in the form of convolutional neural networks. In natural language processing we are now able to build extremely accurate cross-purpose language models because we could encode our knowledge about languages, including structure and word context, into these models. In recommender systems — most collaborative filtering algorithms including matrix factorization methods, make strong assumptions about the affinity of a user towards an item.\\n\\nWhether we would like to slap the label of interpretability on these models or not, it is an objective fact that we build better models when we understand the domain and the context in which the model needs to operate. The best models do', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' the model needs to operate. The best models do not come from the most flexible algorithms, they come from algorithms that are well constrained by domain knowledge and have just the right amount of flexibility to capture the relevant patterns in the data.\\n\\nWe have seen the word understanding being used quite a few times in the above discussion. What we should have realized by now is that it is difficult to build trust without understanding. In the end, it boils down to how one perceives machine learning. Yes, machine learning is an incredibly powerful inductive modeling technique. When combined with big data and big compute, it allows us to model systems and solve problems that were previously out of our reach. But the entry of machine learning should not imply the exit of everything', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content=' of machine learning should not imply the exit of everything else, including common sense. Machine learning is one element in the wider modeling family that includes deductive modeling as well as domain knowledge. The better we understand and leverage the interconnections between these elements, the further we will go towards robust complex system modeling.\\n\\nTrust is contextual and trust can have multiple sources, but eventually it flows from knowledge and integrity; specifically in our trust in the knowledge and integrity of the individuals who are building the models. Trust as well as adoption of models will come, in my opinion, only when the wider audience is convinced that the modelers have the knowledge to understand the limitations of their models (machine learning or otherwise), and the integrity to report them.', metadata={'Title': 'Trust and interpretability in machine learning'}),\n",
       " Document(page_content='This paper by Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu from University of California, Los Angeles proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs.\\n\\nFigure 1: Comparison of a filter’s feature maps in an interpretable CNN and those in a traditional CNN\\n\\nProblem: without any additional human supervision, can we modify a CNN to obtain interpretable knowledge representations in its conv-layers?\\n\\nBau et al. [1] defined six kinds of semantics in CNNs, i.e. objects, parts, scenes, textures, materials, and', metadata={'Title': 'Interpretable Convolutional Neural Network'}),\n",
       " Document(page_content=', parts, scenes, textures, materials, and colors.\\n\\nIn fact, we can roughly consider the first two semantics as object-part patterns with specific shapes, and summarize the last four semantics as texture patterns without clear contours. Filters in low conv-layers usually describe simple textures, whereas filters in high conv-layers are more likely to represent object parts.\\n\\nTheir approach is to train each filter in a high conv-layer to represent an object part. In a traditional CNN, a high-layer filter may describe a mixture of patterns, i.e. the filter may be activated by both the head part and the leg part of a cat (Figure 1). Such complex representations in high conv-layers significantly', metadata={'Title': 'Interpretable Convolutional Neural Network'}),\n",
       " Document(page_content=' Such complex representations in high conv-layers significantly decrease the network interpretability. Therefore, their approach forces the filter in an interpretable CNN is activated by a certain part.\\n\\nLearning a better representation\\n\\nThis paper invented a generic loss to regularize the representation of a filter to improve its interpretability.\\n\\nThe loss encourages a low entropy of inter-category activations and a low entropy of spatial distributions of neural activations which means forcing feature map of a layer in a CNN not to be randomly activated by different region of an object and to have consistent distribution of activations.\\n\\nThe filter must be activated by a single part of the object, rather than repetitively…', metadata={'Title': 'Interpretable Convolutional Neural Network'}),\n",
       " Document(page_content='An Executive’s Guide to Implementing AI and Machine Learning\\n\\nAs a Chief Analytics Officer, I’ve had to bridge the gap between business needs and data scientists. How that gap is bridged is, in my experience, the difference between how well the value and promise of artificial intelligence (AI) and machine learning is realized. Here are a few things I’ve learned.\\n\\nAI = machine learning (at least in 2019)\\n\\nMachine learning is a path to get to AI. At least as of 2019, it is the only known viable path that I’m aware of. In the coming years, there may be other approaches. The two terms are not interchangeable, but for our purposes I will', metadata={'Title': 'An Executive’s Guide to Implementing AI and Machine Learning'}),\n",
       " Document(page_content=' are not interchangeable, but for our purposes I will focus on machine learning.\\n\\nMachine learning is a category of tools and approaches where a computer is given a large training set of data that includes an “answer key”. The machine then learns how to derive the answer key from combinations of the inputs. The model is then tested against a different testing data set to determine its accuracy.\\n\\nMachine learning as a category can include basic statistical tools (e.g. linear regression) that fit this approach. It also includes neural networks, decision trees, and several other tools.\\n\\nIs machine learning the right tool for the problem you’re trying to solve?\\n\\nThis one has tripped me up in the past.', metadata={'Title': 'An Executive’s Guide to Implementing AI and Machine Learning'}),\n",
       " Document(page_content=' one has tripped me up in the past.\\n\\nFor example, recently I had a data set with a lot of data collected from hospitals which had, for each employee, fifty measurements (for example, whether they showed up for work on time or whether they were consistently the only experienced person on their shift) and an indicator of whether they resigned in the weeks and months following. The question was: given this data set, could we create a model to predict employees who would resign before they did so, allowing hospitals to intervene early?\\n\\nWe spent months reviewing the data set and had used basic data visualization approaches to determine a set of rules. For example, employees who were just hired were twice as likely to resign than employees who had already', metadata={'Title': 'An Executive’s Guide to Implementing AI and Machine Learning'}),\n",
       " Document(page_content=' twice as likely to resign than employees who had already worked at the hospital for ten years. Employees in certain…', metadata={'Title': 'An Executive’s Guide to Implementing AI and Machine Learning'}),\n",
       " Document(page_content='Ocearch tracks whales, seals, sharks, and many other types of sea life as they make their way back and forth across the ocean. The tracking is available on their site and you can even sign up to follow certain sharks as they send in their telemetry data to the center.\\n\\nOh, does it bother you to be tagged?\\n\\nAll of this comes together as a ton of data including, location, speed, depth, heart rate and other metrics in the hope that we can better understand these animals in their own environment. By tracking and storing all of this data, environmentalists and biologists hope to analyze and improve the understanding of these magnificent creatures, without disturbing them.\\n\\nThe process of tagging an animal certainly has some moment', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content='The process of tagging an animal certainly has some momentary pain for the animal, and then they pretty much go about their lives and begin to transmit tons of data.\\n\\nFitness Trackers Are Telemetry Tags for Humans\\n\\nSo adorably simple.\\n\\nIn her book, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power, Shoshana Zuboff does an excellent write up on telemetry and its origins and similarities to human tracking. Telemetry in the wild requires us to capture and forcibly attach a device to the animal. In the human realm, this happens in retail stores and on Amazon when we buy our fitness trackers. Sure, it’s a bit painful to pay', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content=', it’s a bit painful to pay the $600 for a top-notch tracker, but heck, I want to improve my fitness, so let’s go for it.\\n\\nAbout ten years ago, this was my Timex watch. It was before fitness tracking really took off. There were a few Garmin and Polar trackers back then, but they were like strapping a microwave to your wrist, so only the most die-hard triathletes used them. Most of us just used this type of stopwatch to keep track of how long we worked out. This Timex was a higher-end model because it actually stored something like the last 20 workouts, which amounted to exactly 40 data points total. Start', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content=' which amounted to exactly 40 data points total. Start. Stop. Start. Stop. Start. Stop.\\n\\nTruly high-tech.\\n\\nSWOLF you.\\n\\nToday, I have this incredible Garmin Fenix watch. This watch tracks literally everything. Time, location, waypoints, direction, speed, cadence, pace, heart rate, elevation, VO2 Max (oxygen velocity), steps, weight, swim speed, and even your SWOLF. Never heard of SWOLF? It’s what you get if swimming and golf have a baby.\\n\\nTo think how far these devices have come in such a brief period is amazing. I imagine they learned many of their algorithmic tricks from the', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content=' they learned many of their algorithmic tricks from the classic use of telemetry in other fields of biology.\\n\\nTracking all of this data is at first amazing to a data geek such as myself. However, after a few months of inspecting and analyzing the data, I came to understand two painful realizations.\\n\\nFirst, I’m No Healthier\\n\\nGathering data is not the same as having an answer or an action plan. The reality is that data is helpful but it must be interpreted and turned into action by individuals. As it turns out, my findings personally are not that dissimilar to others, in that fitness tracking doesn’t have a drastic difference in the outcome.\\n\\nThe reams and re', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content=' the outcome.\\n\\nThe reams and reams of data available from Garmin are impressive but what I realized is that I would analyze that data for a pretty decent amount of time each week. At the end of each workout and at least a few times a week, I’d review the data for anywhere from 5 minutes to 10 minutes. Add that up, and I could have actually used that time to work out, resulting in roughly one additional 45-minute workout a week. Doesn’t sound like much, but to give credit to the Timex stopwatch, finding the most important data points and focusing on those is sometimes more important than tracking everything.\\n\\nSecond, I’m Producing a Ton of Personal Data', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content='’m Producing a Ton of Personal Data\\n\\nBeing the geek that I am, I took the output from Garmin from one 8-mile run while visiting the Jersey Shore and began to analyze it. Garmin provides these beautiful dashboard views of the data it gathers workout by workout. You can view some combination of screens on your phone or desktop through their App.', metadata={'Title': 'We are All Baby Shark (in Data Tracking)'}),\n",
       " Document(page_content='How a hackathon can help founders find their purpose\\n\\nThe inner path to becoming SensAI Arjan Haring · Follow 5 min read · Mar 30, 2019 -- Listen Share\\n\\nRecently I did a proof of concept hackathon with Yama Saraj for his startup SensAI. Such hackathon forces founders to express their vision on their business model and their technology stack. There is a big difference running a social entreprise on DIY technology or a startup with a stack in the cloud and a clear exit strategy. And in Yama’s case he might be going for both…\\n\\nYama characterizes himself as a “crazy development economist”. When he was young, his family had to leave Afghanistan and they', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' young, his family had to leave Afghanistan and they ended up in the Netherlands where he became a succesful student. Yama first studied electrical engineering then economics, but there was something missing. After his studies, in search of his ikigai, he drove all the way from the Netherlands to Afghanistan to give kids boxing lessons.\\n\\n“Technology is just a medium to get the message across.”\\n\\nHe then realized that through sports you can empower this generation to become more resilient and you can even inspire them to be true changemakers. With a touch of irony, martial arts could lead to a peaceful society with less violence and competition. Technology is just a medium to get the message across. But what is the', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' to get the message across. But what is the message?\\n\\nYama is one of a kind, with an energy that matches mine (almost), he is someone you can not not like. He sees connections everywhere and gets almost everyone excited about his ideas. And his ideas are great, if you ask me. Normally people would say someone like Yama needs to focus to be more effective. That could be true, but that most probably doesn’t make him happy.\\n\\nTeam “Sustainable SensAI” working on their prototype. Hardware by MadLab Eindhoven.\\n\\nTeam Sellout SensAI versus team Sustainable SensAI\\n\\nIn the process of our hackathon Yama’s ideas were', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' our hackathon Yama’s ideas were made tangible. One team wanted to work on his idea of a circular boxing bag that would help both the problem of used car tires and that of obesity in working class neighborhoods. And another team start working on the idea to gamify boxing, both for professional and private use.\\n\\nPart of the slidedeck of team “Sellout SensAI”\\n\\nTo run a sustainable, as in long term viable, company you need a robust busines model. That much is certain. So the tendency of millenials to work on something that is “good for the world” still needs to be matched by a revenue model. There are too many nice initiatives that', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' revenue model. There are too many nice initiatives that don’t last because there is no clear revenue model. Volunteering or bootstrapping is often not a sustainable model.\\n\\nTo run a sustainable, as in long term viable, company you need a robust business model.\\n\\nSoul-searching; 1 proof of concept at a time\\n\\nThinking by doing is a wellknown strategy in a lot of disciplines, for example in electrical engineering. But this is not yet a very common approach to the art of living. And in some cases, like finding a life partner, you want to take a more conservative approach and think more before you do.\\n\\n“a failed startup is even considered a positive addition to your', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' failed startup is even considered a positive addition to your resume”\\n\\nBut in your working life I would argue you have quite a lot of flexibility to test things out. Is it something you like doing? Is it something you see yourself doing in 10 years still? Job hopping is more and more accepted, and a failed startup is even considered a positive addition to your resume (lucky me 😜).\\n\\nA working prototype of a gamified boxing bag (Rogier Brussee demonstrating his makey makey contraption and a pretty mean Jab-Jab-cross combo)\\n\\nSo why not test with life a little?\\n\\nSo why not test with life a little? Enjoy the different things it has to offer and better', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' Enjoy the different things it has to offer and better prepare yourself for choices that have great impact on your life.\\n\\nDuring the hackathon in the proof of concept lab Yama could better imagine what it would be like to run a company that sold the products the teams came up with. But Yama could also better understand what people he needed on his team for all the different projects.\\n\\nFind your SensAI\\n\\nYama has a special relationship with 5 time World Champion Thai boxing Yucel Fidan. Yucel is one of the persons that sees the great things that SensAI could be part of. And Yama would like to learn how to be a champion like Yucel, albeit in a different arena.\\n\\nFor me', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' albeit in a different arena.\\n\\nFor me it was an incredible honor to be part of the follow-up day of the SensAI hackathon as well that was hosted at Fidan Gym. Yucel is an incredibly balanced champion. Having worked hard myself, I am always in awe of people that managed to metaphorically move mountains, made a small dent in the universe and stayed true to themselves no matter what.\\n\\n5 time World Champion Thai boxing Yucel Fidan & CEO of SensAI Yama Saraj\\n\\nI am not sure what choices Yama will take in the nearby future. But I hope, and I am pretty sure, he stays true to himself. I have a feeling that the proof of concepts have given him', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content=' a feeling that the proof of concepts have given him more grip on what the effects are of the choices he makes as an entrepreneur for his own life.\\n\\nI am very grateful to have worked with Yama for my first proof of concept session at JADS. And it might not surprise you we are already planning a new proof of concept session soon. Stay tuned, stay SensAI.', metadata={'Title': 'How a hackathon can help founders find their purpose'}),\n",
       " Document(page_content='Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article.\\n\\nPhoto by Franck V. on Unsplash\\n\\nExecutive Summary\\n\\nCandidate Problem\\n\\nMany people are interested in automating redundant processes within the organization using AI. Let’s start with a concrete problem, what I noticed is that lawyers typically gather facts from clients when something bad happens. These facts form the basis of causes of action (negligence, battery, assault, intentional infliction of emotional distress) that an individual can sue on. Once the causes of action have been determined based on legal', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' the causes of action have been determined based on legal justification and the facts, a complaint is written up and submitted to the court for commencement of the legal action. The complaint is a legal document which sets out the facts giving rise to a legal basis for taking action against another party. Manually creating this document can be time consuming and similar facts result in similar causes of action. For example, if someone hits another person there is usually a “battery”. If someone accidentally hurts someone else or someone slips and falls within a store there could be an action for negligence. Based in this problem we have a customer who would like to use AI to learn how to write a complaint from a fact paragraph describing what happened.\\n\\nUnderstanding the Problem', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' paragraph describing what happened.\\n\\nUnderstanding the Problem\\n\\nTrying to get AI/ML to read facts and figure out a way for AI/ML to write a whole complaint might be biting off more than the model can chew and may be an effort that would take years to solve. However, if you take the time to understand and think about the underlying problem, you can find existing techniques (with some slight modifications) that could be used to solve different pieces of the puzzle. For example, when you look at a complaint it starts with a description of the parties and their positions (plaintiff vs defendant) as well as counsel representing them. There may be a class action section, a justification of jurisdiction (does court have power over parties', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' justification of jurisdiction (does court have power over parties), description of the parties, a justification of venue (are we in the proper court location), a listing of the causes of action, and description of the facts. When you look at the sections you have to think about where the data that is going to build the individual sections is going to come from. In certain cases you will not have an answer but if you look carefully you will see patterns and correlations between different sections of the complaint. This will allow you to think about what your inputs to the neural network will be and the candidate outputs.\\n\\nGetting Inputs for the Neural Network\\n\\nWe don’t have any data per se but there may be a way to parse the facts', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' but there may be a way to parse the facts out of all existing complaints and use them as the input for our neural network. Every complaint that is submitted to the court becomes public information so there will be plenty of data. This solution will require attorneys to write their facts as if they were inserting them directly into the complaint, but this is a minor inconvenience to be able to have machine learning provide generated complaints. Generating a complete complaint may be difficult. So let’s break the problem down.\\n\\nBreaking the Problem Down\\n\\nLogically how would you break the generation of a document down into smaller pieces? Well you need to look at one so here is an example: https://www.heise.de/downloads/', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content='www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf. To make it interesting I picked a maker of adult toys so it might peak your curiosity. Basically, we want to eventually generate a complaint (above pdf) from the facts provided by a lawyer. So if you look at the document and at other complaints you will find similar patterns as to structure.\\n\\nSo what do you think would be the best way to break things down… don’t scroll down until you have had time to think about it.\\n\\n….Really think about it…..\\n\\nWell if you said to break things', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content='…..\\n\\nWell if you said to break things down by section using templating, then this would be the route that would probably be best.\\n\\nWhen you break down a complaint there are causes of action listed in the complaint. Each cause of action (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.) has supporting rules and justification based on the facts. So now there are two problems. How do you come up with the causes of action from the facts text and how do you generate the supporting text under each cause of action?\\n\\nFinding the Causes of Action\\n\\nWhen we look at the', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' Causes of Action\\n\\nWhen we look at the facts of the case we need to find all of the causes of action (laws that were broken) that we could sue on. There are no direct solutions for finding causes of action from text so we will have to think more fundamentally.\\n\\nWhat existing techniques do you think we can use to look at text and infer meaning or a description of the text. If you said multi-label text classification or multi-label sentiment analysis, then you are ahead of the game (https://paperswithcode.com/task/text-classification, https://paperswithcode.com/task/sentiment-analysis). Analyzing text to determine its associated causes of action is a similar process to', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' its associated causes of action is a similar process to classifying text or finding the sentiment of related text. There are associated problems like the fact that causes of action will need to be updated as laws are introduced. There may be an alternate way to create an embedding for the facts and then tie the causes of action to the facts based on triplet (https://arxiv.org/pdf/1503.03832.pdf) or quadruplet loss (https://arxiv.org/pdf/1704.01719.pdf) to push causes of action sharing similar words together in the embedding space and unrelated causes of action further apart. Then use a clustering technique to find causes of action close to determinative', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' technique to find causes of action close to determinative word embeddings used in the supporting argument associated with the words in the individual cause of action sections of the complaint.\\n\\nGenerating the Text in the Supporting Arguments Section of Individual Causes of Action\\n\\nNow that you have figured out how to get the high level causes of action from the text, how can you generate the supporting argument text for each of the individual cause of action sections (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.)?\\n\\nThis one is not so straight forward. Think about a what neural network architectures which generate text (', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' about a what neural network architectures which generate text (Don’t scroll down until you have some ideas)….\\n\\n….Open your mind….Use the Force….\\n\\nText generation algorithms (https://paperswithcode.com/task/data-to-text-generation, https://paperswithcode.com/area/nlp/text-generation) might be an option but even the best ones create gibberish often. The better alternative might be to use an architecture like neural networks involved in translation (https://paperswithcode.com/task/machine-translation, https://paperswithcode.com/task/unsupervised-machine-translation, https://paperswithcode.com/paper/unsuper', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content='paperswithcode.com/paper/unsupervised-clinical-language-translation). In addition, it might be a good idea to have a separate “translation” neural network for each cause of action to help each neural network focus on identifying the key facts used in generating a supporting argument for each cause of action.\\n\\nClean Up\\n\\nIt is probably going to be a good idea to run the candidate text for the supporting argument text for each cause of action through a grammar checker/fixer (https://paperswithcode.com/task/grammatical-error-correction). This way any blatant mess ups are fixed.\\n\\nConclusion\\n\\nI hope you learned how to apply the machine learning solutions more broadly', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content=' learned how to apply the machine learning solutions more broadly. Let me know if you get stuck as I would definitely be interested in hearing about problems that people are trying to solve with machine learning.', metadata={'Title': 'Applied AI: Going From Concept to ML Components'}),\n",
       " Document(page_content='Wild Wide AI: responsible data science\\n\\nData Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused:\\n\\nIn 2012 a team of investigative journalists from The Wall Street Journal found out that Staples - a multinational supply retailing corporation — offered lower prices to buyers who live in more affluent neighborhoods. Staples’ intention was to offer discounts to customers who lived closer to their competitors’ stores. However their competitors tended to build stores in richer neighborhoods. Based on the correlation between location and social status, this resulted in', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' correlation between location and social status, this resulted in price discrimination based on race. Neither Staples nor customers did not know about this side effect until a team of investigative journalists brought it to light (source).\\n\\nIn 2015, the AdFischer project demonstrated via simulation that synthetic men online profiles were being shown ads for high paying jobs significantly more frequently than female profiles. The result was a clear employment discrimination based on gender (source). The study started surfacing the problem due to lack of responsibility intentionally or not in data-driven algorithmic pipelines.\\n\\nIn 2016, investigators from ProPublica discovered that the software used by judges in court to predict future crimes was often incorrect, and it was racist: blacks were almost twice as likely as whites to', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=': blacks were almost twice as likely as whites to be labeled a higher risk but less likely to re-offend. The tool made the opposite mistake among whites: they were much more likely than blacks to be labeled lower risk but went on to commit other crimes. ProPublica’s study was very influential. They published the dataset, the data methodology, as well as the data processing code in the form of a Jupyter Notebook on GitHub. This striking result really speaks about the opacity and the lack of fairness in these types of tools, especially when they were used in the public sector, in governments, in the juridical system (source).\\n\\nIs Data Science impartial?\\n\\nIt is often claimed that data', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' impartial?\\n\\nIt is often claimed that data science is algorithmic and therefore cannot be biased. And yet, we saw examples above where all traditional evils of discrimination exhibit themselves in the data science ecosystem. Bias is inherited both in the data and in the process, is propelled and amplified.\\n\\nTransparency is an idea, a mindset, a set of mechanisms that can help prevent discrimination, enable public debate and establish trust. When we make data science, we interact with society. The way we do decisions has to be in an environment where we have trust from the participants, from the public. Technology alone won’t solve the issue. User engagement, policy efforts are important.\\n\\nData responsibility\\n\\nAspects of responsibility in', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content='\\nData responsibility\\n\\nAspects of responsibility in the data science ecosystem include: fairness, transparency, diversity and data protection. The area of responsible data science is very new but is already at the edge of all the top machine learning conferences because these are difficult but interesting and relevant problems.\\n\\nMoritz Hardt\\n\\nWhat is Fairness?\\n\\nPhilosophers, lawyers, sociologists have been asking this question for many years. In the data science context we usually solve the task of predictive analytics, predicting future performance or behavior based on some past or present observation (dataset). Statistical bias occurs when models used to solve such tasks do not fit the data very well. A biased model is somehow imprecise and does not', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' biased model is somehow imprecise and does not summarize the data correctly. Societal bias happens when the data or the model does not represent the world correctly. An example occurs when the data is not representative. This is the case if we only used the data for police going to the SAME neighborhood over and over, and we use this information only about crime from those particular neighborhoods. Societal bias can also be caused by how we define world. Is it the world as it is that we are trying to impact with predictive analytics or the world as it should be? Who should determine what the world should be like?\\n\\nWhat is discrimination?\\n\\nIn most legal systems, there are two concepts defining discrimination:\\n\\nDisparate treatment', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' concepts defining discrimination:\\n\\nDisparate treatment is the illegal practice of treating an entity, such as a creditor or employee, differently based on a protected characteristic such as race, gender, age, religion, sexual orientation, or national origin. Disparate treatment comes in a context where there is some benefit to begin or some harm to be brought to the individual being treated, for example sentencing them or admitting them to college or granting them credit. It is something where there is actually tangible positive or negative impact.\\n\\nDisparate impact is the result of systematic disparate treatment, where disproportionate adverse impact is observed on members of\\n\\na protected class. Different countries protect different classes or sub-populations.\\n\\nWhen we talk about discrimination', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content='populations.\\n\\nWhen we talk about discrimination, we are using terms which could be uncomfortable, such as racism, gender, sexual orientation. Political correctness in the extreme sense has no place in these debates about responsible data science. We have to be able to name concepts to be able to talk about them. Once we can talk about those concepts, we can take corrective action.\\n\\nTechnical definition of fairness\\n\\nLet’s consider vendors who are assigning outcomes to members of a population. This is the most basic case, a binary classification. Positive outcomes may be: offered employment, accepted to school, offered a loan, offered a discount. Negative outcomes may be: denied employment, rejected from school, denied a loan, not offered a', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' from school, denied a loan, not offered a discount. What we worry about in fairness is how outcome is assigned to members of a population. Let’s assume that 40% got the positive outcome. Some sub-population however may be treated differently by this process. Let’s assume that we know ahead of time what the sub-population is, for example red haired people. Thus we can divide our population into two groups: people with red hair, and people without red hair. In our example we observe that while 40% of the population got the positive outcome, only 20% of red haired received the positive outcome. 60% of the other received the positive outcome. Here, according to some definition, we observe disparate', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' Here, according to some definition, we observe disparate impact on the group of red haired individuals. Another way to denote this situation is that statistical parity fails. This is a baseline definition of fairness without conditioning. There is quite some sophistication about using such assessment in real-life, for example in courts. This basic definition of fairness, written into many laws around the globe, dictates that demographics of the individuals receiving any outcome are the same as demographics of the underlying population.\\n\\nAssessing disparate impact\\n\\nThe vendor could say that he actually did not intend or did not look at all at hair color, which happens to be the sensitive attribute in the dataset. Instead the vendor would say that he decided to give the positive outcome to people whose hair', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' decided to give the positive outcome to people whose hair is long. The vendor is denying the accusation and saying that he is not discriminating based on hair color. The thing is that the vendor has adversely impacted red haired people. It is not the intention that we care about, but the effect on the sub-population. In other words, blinding is not a legal or ethical excuse. Removing hair color from vendor’s process on outcome assignment does not prevent discrimination from occurring. Disparate impact is legally assessed on the impact, not on the intention.\\n\\nMitigating disparate impact\\n\\nIf we detect a violation of statistical parity, we may want to mitigate. In an environment in which we have a number of positive outcomes which we', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' which we have a number of positive outcomes which we can assign, we have to swap some outcomes. We have to take a positive outcome from somebody in the not-red haired group and give it to someone else in the red haired group. Not everyone will agree with swapping outcomes. An individual who used to get the positive outcome would stop getting it any more. This would lead to individual fairness. It stipulates that any two individuals who are similar within a particular task should receive similar outcomes. There is a tension between group and individual fairness that is not easy to resolve.\\n\\nIndividual vs group fairness\\n\\nAn example in which individual fairness and group fairness was taken to the supreme court appears in the Ricci v. DeStefano', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' in the Ricci v. DeStefano case in 2009. Firefighters took a test for promotion, and the department threw out the test results because none of the black firefighters scored high enough to be promoted. The fire department was afraid that they could be sued for discrimination and disparate impact if they were to admit results and not promote any black firefighter. But then the lawsuit was brought by the firefighters who would have been eligible for promotion but who weren’t promoted as a result of this. There was an individual fairness argument, a disparate treatment argument. They argued that race was used to negatively impact them. This case was ruled in favor of white firefighters, in favor of individual fairness.\\n\\nIndividual fairness is equality, everybody gets the same', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content='\\nIndividual fairness is equality, everybody gets the same box to reach the tree. Group fairness is the equity view, everybody gets as many boxes as they need to be able to reach the tree. Equity costs more because society has to invest more. These are two intrinsically different world views that we cannot logically decide which one is better. These are just two different points of view, there isn’t a better one. They go back to what we believe a world as it is, is a world as it should be. The truth is going to be somewhere in the middle. It is important to understand which kinds of mitigation are consistent with which kinds of belief systems.\\n\\nFormal definition of fairness\\n\\nFriedler et. al.', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' fairness\\n\\nFriedler et. al. tease out the difference between beliefs about fairness and\\n\\nmechanisms that logically follow from those beliefs in their paper from 2016. The construct space is intrinsically the state of the world. It is made of things we cannot directly measure such as intelligence, grit, propensity to commit crime and risk-adverseness. We however want to measure intelligence and grit when we decide who to admit to college. We want to know the propensity of a person to recommit crime and his risk-adverseness in justice. These are raw properties which are exhibited and not directly accessible. Instead we look at the observed space where there are proxies, which are to a greater or lesser degree aligned with the properties that', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content=' a greater or lesser degree aligned with the properties that we want to measure. For intelligence the proxy would be SAT score, grit would be measured by high-school GPA, propensity to commit crime by family history and risk-adverseness by age. The decision space is then made of what we would like to decide: performance in college and recidivism.\\n\\nFairness is defined here as a mapping from the construct space to the decision space, via the observe space. Individual fairness (equality) believes that the observed space faithfully represents the construct space. For example high-school GPA is a good measure of grit. Therefore the mapping from construct to decision space has low distortion. Group fairness (equity) however says that there is a systematic', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content='equity) however says that there is a systematic distortion caused by structural bias, society bias when going from the construct space to observed space. Furthermore this distortion aligns with groups structure, with membership in protected groups in our society. In other words the society systematically discriminates.\\n\\nto be continued …\\n\\nReferences: lecture on responsible data science at Harvard University by Prof. Julia Stoyanovich (New York University) — selected chapters from “ The Age of Surveillance Capitalism” book by Shoshana Zuboff — thoughts from “What worries me about AI” post by François Chollet.', metadata={'Title': 'Wild Wide AI: responsible data science'}),\n",
       " Document(page_content='#InsideRL\\n\\nIn a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question for this blog is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.\\n\\nTypical Reinforcement Learning cycle\\n\\nBefore we answer our root question i.e. How we formulate RL problems mathematically (', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='e. How we formulate RL problems mathematically (using MDP), we need to develop our intuition about :\\n\\nThe Agent-Environment relationship\\n\\nMarkov Property\\n\\nMarkov Process and Markov chains\\n\\nMarkov Reward Process (MRP)\\n\\nBellman Equation\\n\\nMarkov Reward Process\\n\\nGrab your coffee and don’t stop until you are proud!🧐\\n\\nThe Agent-Environment Relationship\\n\\nFirst let’s look at some formal definitions :\\n\\nAgent : Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions. Environment :It is the demonstration of the problem to be solved', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='It is the demonstration of the problem to be solved.Now, we can have a real-world environment or a simulated environment with which our agent will interact.\\n\\nDemonstrating an environment with which agents are interacting.\\n\\nState : This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.\\n\\nAnything that the agent cannot change arbitrarily is considered to be part of the environment. In simple terms, actions can be any decision we want the agent to learn and state can be anything which can be useful in choosing actions. We do not assume that everything in the environment is', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent. Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.\\n\\nThe Markov Property\\n\\nTransition : Moving', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='The Markov Property\\n\\nTransition : Moving from one state to another is called Transition. Transition Probability: The probability that the agent will move from one state to another is called transition probability.\\n\\nThe Markov Property state that :\\n\\n“Future is Independent of the past given the present”\\n\\nMathematically we can express this statement as :\\n\\nMarkov Property\\n\\nS[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that the transition from state S[t] to S[t+1] is entirely independent of the past. So, the RHS of the Equation means the same as LHS if the', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.\\n\\nState Transition Probability :\\n\\nAs we now know about transition probability we can define state Transition Probability as follows :\\n\\nFor Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by\\n\\nState Transition Probability\\n\\nWe can formulate the State Transition probability into a State Transition probability matrix by :\\n\\nState Transition Probability Matrix\\n\\nEach row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' starting state to any successor state.Sum of each row is equal to 1.\\n\\nMarkov Process or Markov Chains\\n\\nMarkov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property.So, it’s basically a sequence of states with the Markov Property.It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).\\n\\nBut what random process means ?\\n\\nTo answer this question let’s look at a example:\\n\\nMark', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='�s look at a example:\\n\\nMarkov chain\\n\\nThe edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.\\n\\nSome samples from the chain :\\n\\nSleep — Run — Ice-cream — Sleep\\n\\nSleep — Ice-cream — Ice-cream — Run\\n\\nIn the above two sequences what we see is we get random set of States(S) (i', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain.Hope, it’s now clear why Markov process is called random set of sequences.\\n\\nBefore going to Markov Reward process let’s look at some important concepts that will help us in understand MRPs.\\n\\nReward and Returns\\n\\nRewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent. In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' the environment) instead of, the reward agent receives from the current state(also called immediate reward). This total sum of reward the agent receives from the environment is called returns.\\n\\nWe can define Returns as :\\n\\nReturns (Total rewards from the environment)\\n\\nr[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' step by performing an action to move to another state.\\n\\nEpisodic and Continuous Tasks\\n\\nEpisodic Tasks: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent. Continuous Tasks : These are the tasks that have no ends i.e. they don’t have any terminal state.These types of tasks will never end.For example, Learning how to code!\\n\\nNow, it', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' Learning how to code!\\n\\nNow, it’s easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?\\n\\nThis is where we need Discount factor(ɤ).\\n\\nDiscount Factor (ɤ): It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' of 1 means that more importance is given to future rewards. In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.\\n\\nSo, we can define returns using discount factor as follows :(Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)\\n\\nReturns using discount factor\\n\\nLet’s understand it with an example,suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (ɤ).Let’s look at two possibilities : (Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)\\n\\nOne with discount factor (ɤ) 0.8 :\\n\\nDiscount Factor (0.8)\\n\\nThis means that we should wait till 15th hour because the decrease is not very significant , so it’s still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.\\n\\nSecond, with discount factor (ɤ) 0.2 :\\n\\nDiscount Factor (0.2)\\n\\nThis means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.\\n\\nSo which value of discount factor to use ?\\n\\nIt depends on the task that we want to train an agent for. Suppose', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent’s king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.\\n\\nMarkov Reward Process\\n\\nTill now we have seen how Markov chain defined the dynamics of a environment using set of states(S) and Transition Probability Matrix(P).But, we know that Reinforcement Learning is all about goal to maximize the reward', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='forcement Learning is all about goal to maximize the reward.So, let’s add reward to our Markov Chain.This gives us Markov Reward Process.\\n\\nMarkov Reward Process : As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.\\n\\nMathematically, we define Markov Reward Process as :\\n\\nMarkov Reward Process\\n\\nWhat this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.\\n\\nWe define MRP as (S,P, R,ɤ) , where :\\n\\nS is a set of states,\\n\\nP is the Transition Probability Matrix,\\n\\nR is the Reward function, we saw earlier,\\n\\nɤ is the discount factor\\n\\nMarkov Decision Process\\n\\nNow, let’s develop our intuition for Bellman Equation and Markov Decision Process.\\n\\nPolicy Function and Value Function\\n\\nValue Function determines how good it is for the agent to be in a particular state. Of course, to determine how good it will be to be in a', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. A policy defines what actions to perform in a particular state s.\\n\\nA policy is a simple function, that defines a probability distribution over Actions (a∈ A) for each state (s ∈ S). If an agent at time t follows a policy π then π(a|s) is the probability that the agent with taking action (a ) at a particular time step (t).In Reinforcement Learning the experience of the agent determines the change in policy. Mathematically, a policy is defined as follows :\\n\\nPolicy Function\\n\\nNow, how do we find', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='Policy Function\\n\\nNow, how do we find a value of a state. The value of state s, when the agent is following a policy π which is denoted by vπ(s) is the expected return starting from s and following a policy π for the next states until we reach the terminal state. We can formulate this as :(This function is also called State-value Function)\\n\\nValue Function\\n\\nThis equation gives us the expected returns starting from the state(s) and going to successor states thereafter, with the policy π. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic. It is the expectation of returns from start state s and', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let’s look at an example :\\n\\nExample\\n\\nSuppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.\\n\\nOur expected return is with a discount factor of 0.5:\\n\\nCalculating the Value of Class 2\\n\\nNote:It’s -2 + (-2 * 0.5) + 10 * 0.25 + 0 instead of -2 * -2 * 0.5 + 10 * 0.', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='2 * 0.5 + 10 * 0.25 + 0.Then the value of Class 2 is -0.5 .\\n\\nBellman Equation for Value Function\\n\\nBellman Equation helps us to find optimal policies and value functions. We know that our policy changes with experience so we will have different value functions according to different policies. The optimal value function is one that gives maximum value compared to all other value functions.\\n\\nBellman Equation states that value function can be decomposed into two parts:\\n\\nImmediate Reward, R[t+1]\\n\\nDiscounted value of successor states,\\n\\nMathematically, we can define Bellman Equation as :\\n\\nBellman Equation for Value', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' as :\\n\\nBellman Equation for Value Function\\n\\nLet’s understand what this equation says with a help of an example :\\n\\nSuppose, there is a robot in some state (s) and then he moves from this state to some other state (s’). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s’) he moved to.\\n\\nLet’s look at another example :\\n\\nBackup Diagram\\n\\nWe want to know the value of state s. The value', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' to know the value of state s. The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.\\n\\nValue Calculation\\n\\nThe above equation can be expressed in matrix form as follows :\\n\\nBellman Linear Equation\\n\\nWhere v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.\\n\\nThe running time complexity for this computation is O(n³). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs).In later', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='Ps (same for MDPs).In later Blogs, we will look at more efficient methods like Dynamic Programming (Value iteration and Policy iteration), Monte-Claro methods, and TD-Learning.\\n\\nWe are going to talk about the Bellman Equation in much more detail in the next story.\\n\\nWhat is Markov Decision Process ? Markov Decision Process : It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.\\n\\nIt is a tuple of (S, A, P, R, 𝛾) where:\\n\\nS is a set of states,\\n\\nA is the set of actions agent can choose to', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='A is the set of actions agent can choose to take,\\n\\nP is the transition Probability Matrix,\\n\\nR is the Reward accumulated by the actions of the agent,\\n\\n𝛾 is the discount factor.\\n\\nP and R will have slight change w.r.t actions as follows :\\n\\nTransition Probability Matrix\\n\\nTransition Probability Matrix w.r.t action\\n\\nReward Function\\n\\nReward Function w.r.t action\\n\\nNow, our reward function is dependent on the action.\\n\\nTill now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy π. Actually, in Markov Decision', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=' policy π. Actually, in Markov Decision Process(MDP) the policy is the mechanism to take decisions. So now we have a mechanism that will choose to take an action.\\n\\nPolicies in an MDP depend on the current state. They do not depend on history. That’s the Markov Property. So, the current state we are in characterizes history.\\n\\nWe have already seen how good it is for the agent to be in a particular state(State-value function). Now, let’s see how good it is to take a particular action following a policy π from state s (Action-Value Function).\\n\\nState-action value function or Q-Function\\n\\nThis', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='action value function or Q-Function\\n\\nThis function specifies how good it is for the agent to take action (a) in a state (s) with a policy π.\\n\\nMathematically, we can define the State-action value function as :\\n\\nState-action value function\\n\\nBasically, it tells us the value of performing a certain action(a) in a state(s) with a policy π.\\n\\nLet’s look at an example of the Markov Decision Process :\\n\\nExample of MDP\\n\\nNow, we can see that there are no more probabilities. In fact, now our agent has choices to make like after waking up, we can choose to watch Netflix or code and', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content=', we can choose to watch Netflix or code and debug. Of course, the actions of the agent are defined w.r.t some policy π and will get the reward accordingly.', metadata={'Title': 'Reinforcement Learning : Markov-Decision Process (Part 1)'}),\n",
       " Document(page_content='When I first took science class in elementary school, we learned about the five senses. Maybe not far into the future, that information might be as outdated as the idea of nine planets in our solar system (sorry, Pluto). This might be thanks to Neuralink, a company that’s been making headlines for its controversial brain-chip interface. On July 17, 2019, the company finally unveiled its hard work in a YouTube live stream.\\n\\nWhen you first hear it, Neuralink’s pursuit sounds like it comes straight out of a mad genius’s diary; certainly, with Elon Musk at the head of the company, that image might not be a far stretch. But if you look beyond what seems to be a Sci', metadata={'Title': 'What Is Neuralink: A Look At What It Is, What It Wants to Be, and What It Could Become'}),\n",
       " Document(page_content=' if you look beyond what seems to be a Sci-Fi horror movie, you can get a glimpse of quite a different future for humanity — one that has a new, sixth sense: Neuralink.', metadata={'Title': 'What Is Neuralink: A Look At What It Is, What It Wants to Be, and What It Could Become'}),\n",
       " Document(page_content='Amount of information available today on the web is astounding and it is ever-expanding. For example, there are more than 1.94 billion websites that are linked with the World Wide Web and search engines (e.g., Google, Bing, etc.) can go through those links and serve useful information with great precision and speed. In most of those successful search engines, the most important denominator is the use of Knowledge Graphs. Not only search engines, social network sites (e.g., Facebook, etc.), e-commerce sites (e.g., Amazon, etc.) are also using Knowledge Graphs to store and retrieve useful information.\\n\\nA Brief History\\n\\nIn 1960, Semantic Networks were invented to address the growing', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=', Semantic Networks were invented to address the growing need for a knowledge representation framework that can capture a wide range of entities — real-world objects, events, situations or abstract concepts and relations and in the end can be applied to extended English Dialogue tasks. The main idea behind Semantic Networks was to capture a wide range of issues which includes the representation of plans, actions, time, individuals’ beliefs and intentions, and be general enough to accommodate each issue.\\n\\nAccording to Wikipedia, in late 1980, two Netherlands universities started a project called Knowledge Graph which was kind of a semantic network, but with some added restrictions to facilitate algebraic operations on the graph.\\n\\nIn 2001, Tim Berners-Lee coined the term Semantic Web', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' Berners-Lee coined the term Semantic Web which is an application of Semantic Network combined with the Web.\\n\\nTim Berners-Lee stated that “The Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation.”\\n\\nIn 2012, Google named its Knowledge Graph as Knowledge Graph.\\n\\nAn Undefined Definition\\n\\nEvery Company/Group/Individual creates their own version of the Knowledge Graph to limit complexity and organize information into data and knowledge. For example, Google’s Knowledge Graph, Knowledge Vault, Microsoft’s Satori, Facebook’s Entities Graph, etc.\\n\\nSo, there', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content='ities Graph, etc.\\n\\nSo, there is no formal definition of Knowledge Graph. In a broader perspective, a Knowledge Graph is a variant of semantic network with added constraints whose scope, structure, characteristics and even uses are not fully realized and in the process of development.\\n\\nAn Example of Knowledge Graph\\n\\nSource: Maximilian Nickel et al. A Review of Relational Machine Learning for Knowledge Graphs: From Multi-Relational Link Prediction to Automated Knowledge Graph Construction\\n\\nWhy Should You Get Excited?\\n\\nWith every passing year, Machine Learning and Knowledge Representation Learning on Knowledge Graphs are advancing rapidly, both in scale and depth, but in different directions. On one hand, Machine Learning techniques are getting better', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' On one hand, Machine Learning techniques are getting better at performing various tasks (e.g., Classification, Generation, etc.) on a variety of datasets with great precision and recall. On the other hand, Knowledge Representation brings the ability to represent entities and relations with high reliability, explainability, and reusability. Recent advances in Knowledge Representation Learning include mining logical rules from the graph.\\n\\nSource: Bishan Yang et al. Embedding Entities and Relations for Learning and Inference in Knowledge Bases.\\n\\nHowever, bringing knowledge graphs and machine learning together will systematically improve the accuracy of the systems and extend the range of machine learning capabilities. For example, results inferred from Machine Learning models will have better explainability and', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' from Machine Learning models will have better explainability and trustworthiness.\\n\\nBelow are some of the opportunities that can be availed by bringing Knowledge Graph to Machine Learning:\\n\\nData Insufficiency\\n\\nHaving a sufficient amount of data to train a machine learning model is very important. In the case of sparse data, Knowledge Graph can be used to augment the training data, e.g., replacing the entity name from original training data with an entity name of a similar type. This way a huge number of both positive and negative examples can be created using Knowledge Graph.\\n\\nZero-Shot Learning\\n\\nToday, the main challenge with a Machine Learning model is that without a properly trained data it can not distinguish between two data points. In', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' it can not distinguish between two data points. In Machine Learning, this is considered as Zero-Shot Learning problem. This is where knowledge graphs can play a very big role. The induction from the Machine Learning model can be complemented with a deduction from the Knowledge Graph, e.g., with pictures where the type of situation did not appear in the training data.\\n\\nExplainability\\n\\nOne of the major problems in machine learning industry is explaining the predictions made by machine learning systems. One issue is the implicit representations causing the predictions from the machine learning models. Knowledge Graph can alleviate this problem by mapping the explanations to some proper nodes in the graph and summarizing the decision-taking process.\\n\\nNote: The above opportunities are explained in', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content='\\n\\nNote: The above opportunities are explained in more detail in the seminar report of Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web (Dagstuhl Seminar 18371)\\n\\nSome Use Cases\\n\\nQuestion — Answering is one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs.\\n\\nis one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs. Storing Information', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' retrieve information from Knowledge Graphs. Storing Information of Research is another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc.\\n\\nis another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc. Netflix uses a Knowledge Graph to store a vast amount of varied information for its Recommendation System which helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next.', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' to predict what customers might like to watch next.\\n\\nwhich helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next. Supply Chain Management is also being benefited from the use of Knowledge Graph. Companies can easily keep track of inventories of different components, personnel involved, time, etc which allows them to move items more swiftly and cost-effectively.\\n\\nand many more…\\n\\nOpen Challenges\\n\\nA coherent set of best practices, that can be applied during the creation of knowledge graphs, will be helpful in understanding and reuse of Knowledge Graphs amongst engineers, developer, and researchers. Given a set of unstructured data and Knowledge Graph', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content=' a set of unstructured data and Knowledge Graph, the problem of knowledge integration is to identify whether the entities mentioned in the data match with the real world entities present in Knowledge Graph. Although this problem can be solved using machine learning algorithms, the outcome of those algorithms directly depends on the quality of the training data. Given a wide variety of dataset, knowledge integration becomes quite difficult. Knowledge is not static, but constantly evolving. For example, if a Knowledge Graph keeps track of patients’ health, the data stored at a particular moment could be false for some later moment. So, how do we capture this evolving nature of knowledge? How to evaluate a Knowledge Graph? Which quality improvement (e.g., completeness, correctness, linkage, etc', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content='g., completeness, correctness, linkage, etc.) is more important?\\n\\nAcknowledgments', metadata={'Title': 'Knowledge Graph: The Perfect Complement to Machine Learning'}),\n",
       " Document(page_content='Should you buy the book?\\n\\nYes. But you don’t have to. You can read it first. But you should buy a copy, hold it, read it, sit it on your coffee table. Then when your friends ask, ‘What is machine learning?’, you’ll be able to tell them.\\n\\nWho is the book for?\\n\\nMaybe you’re studying data science. Or you’ve heard of machine learning being everywhere and you want to understand what it can do. Or you’re familiar with applying the tools of machine learning but you want to make sure you’re not missing any.\\n\\nI’ve been studying and practising machine', metadata={'Title': 'The Hundred-Page Machine Learning Book Book Review'}),\n",
       " Document(page_content='I’ve been studying and practising machine learning for the past two-years. I built my own AI Masters Degree, it led to being a machine learning engineer. This book is part of my curriculum now but if it was out when I started, it would’ve been on there from the beginning.\\n\\nWhat previous knowledge do I need for the Hundred-Page Machine Learning book?\\n\\nHaving a little knowledge about math, probability and statistics would be helpful but The Hundred-Page Machine Learning Book has been written a way that you’ll get most of these as you go.\\n\\nSo the answer to this question remains open. I read it from the perspective of a machine learning engineer, I knew some things but', metadata={'Title': 'The Hundred-Page Machine Learning Book Book Review'}),\n",
       " Document(page_content=' a machine learning engineer, I knew some things but learned many more.', metadata={'Title': 'The Hundred-Page Machine Learning Book Book Review'}),\n",
       " Document(page_content='How to tune hyperparameters of tSNE\\n\\nThis is the second post of the column Mathematical Statistics and Machine Learning for Life Sciences. In the first post we discussed whether and where in Life Sciences we have Big Data suitable for Machine / Deep Learning, and emphasized that Single Cell is one of the most promising Big Data resources. t-distributed stochastic neighbor embedding (tSNE) is a Machine Learning non-linear dimensionality reduction technique which is absolutely central for Single Cell data analysis. However, the choice of hyperparameters for the tSNE might be confusing for beginners.\\n\\nIn this post, I will share my recommendations on selecting optimal values of hyperparameters such as perplexity, number of principal', metadata={'Title': 'How to tune hyperparameters of tSNE'}),\n",
       " Document(page_content='parameters such as perplexity, number of principal components to keep, and number of iterations for running tSNE.\\n\\nHow to Use tSNE Effectively\\n\\nWhen teaching single cell RNA sequencing (scRNAseq) course I keep getting questions about sensitivity of tSNE with respect to hyperparameters such as perplexity. The questions are usually inspired by this fantastic post about challenges with interpreting tSNE plots.\\n\\nA popular tutorial on developing intuition behind tSNE\\n\\nDespite my great respect for the main message of the post, I think scRNAseq community should not worry too much about perplexity and other tSNE hyperparameters based on what they learn from that post because: a) many examples in', metadata={'Title': 'How to tune hyperparameters of tSNE'}),\n",
       " Document(page_content=' from that post because: a) many examples in the post come from abstract mathematical topologies which do not really resemble scRNAseq data, b) the post concentrates on extreme tSNE hyperparameters which are rarely used in the real world scRNAseq analysis.\\n\\nIf you do scRNAseq analysis you will not avoid the popular Rtsne function and R package which is based on Barnes-Hut C++ implementation of the original tSNE algorithm. The Rtsne function has three main hyperparameters:', metadata={'Title': 'How to tune hyperparameters of tSNE'}),\n",
       " Document(page_content='Photo credit: Annie Spratt, UK\\n\\nI’ve become something of a MOOC connoisseur; at this point in my journey to employment, I’ve tried every major MOOC and I still dabble to keep things fresh on the brain.\\n\\nThe great thing is, I learned a ton without spending much money. So far Udacity was the most expensive, and I got it on an interest-free loan. Often they offer discounts and many of them run on a monthly subscription for $30–50. For brevity, I don’t want to focus on the syllabus of each program here, but explain their strengths and weaknesses.\\n\\nPart of my motivation for doing so many courses', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content='\\nPart of my motivation for doing so many courses was my interest in education; I think the MOOC format’s made a lot of bright people rethink learning, and Coursera and SharpestMinds offer an important format that could compete with some aspects of academia. If you remember nothing else, though: MOOCs are a great way to build an industry skill and augment your education; they supply you with the skill-building academia can not. They are not a substitute for academic education and you will still need to\\n\\nbuild your own projects, and answer your own questions\\n\\nto become a data scientist. The closest one to academia would be Coursera. Ultimately I became a data scientist by using a combination of MO', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' became a data scientist by using a combination of MOOCs, self-studying books, and leveraging my formal education.\\n\\nCodecademy:\\n\\nI highly recommend Codecademy Pro. Price is worth it and exercises really give the right amount of brainhurt, along with good explanations. I recommend clearing your progress on certain lessons and redoing them if you had trouble (reworking missed problems is good practice for anything technical).\\n\\nI did their essential Python 3 course to make me a better Python user, but they have a Computer Science career track which is more involved and covers topics like recursion, algorithms and data structures.\\n\\nThe strength of Codecademy lies in its combination of code-alongs', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content='emy lies in its combination of code-alongs, guided projects, quizzes and videos: they are really pushing for a holistic understanding of the material. You need all those formats to truly understand something technical.\\n\\nDataquest:\\n\\nImagine Codecademy, but harder. These lessons are incredibly detailed. It’s a great place to learn, but it will take a lot of time to do even a normal mission. I love their ethos of “quality education at a low price” ($30/month currently, $50 for the Premium plan, much cheaper if you buy a year) which makes it accessible to anyone willing to roll up their sleeves and work. For me, since I already covered a lot of these', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' me, since I already covered a lot of these concepts, I didn’t find it worthwhile to complete any of the major certificates, but struggling through missions made me a better coder and practitioner of data science.\\n\\nOn the Premium plan you get a monthly call with a mentor and he’ll review your resume or give technical advice; take advantage of this opportunity. The most valuable thing they can give you is criticism.\\n\\nDataquest offers a nice curriculum selection that will take you on up to advanced Python topics and, as somewhat of a statistics pro, I found their explanation of stats very good. I recommend going from Codecademy to Dataquest to move beyond foundational skills and get strong.\\n\\nMy one complaint is', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' and get strong.\\n\\nMy one complaint is they could turn down the difficulty a hair; there’s something to be said for repetition and plug-and-chug when learning an applied skill. However, they cover an enormous amount of material and seem to have the best SQL courses of any MOOC. They go into Redshift and really advanced data engineering stuff. I’d consider a few DQ missions essential for any aspiring data scientist and it almost makes other MOOCs obsolete.\\n\\nDataCamp:\\n\\nMy opinion is: DataCamp’s an earnest shot at education. It’s a great place to get your feet wet and the video format is noobie-friendly. I enjoyed it', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' is noobie-friendly. I enjoyed it at the time, but it‘s only step 1 of a long process. The code-along format is like Codecademy, but you don’t write code, just fill in blanks. There is some nice brainhurt here, but it’s not the best bang for your buck.\\n\\nVideos (and lectures in general) aren’t a great way to teach a technical concept. If you’re totally new to data science, this is a nice place to start and get motivated, but I’d still recommend DataQuest.\\n\\nMany DataCamp learners (myself included) report doing a DC course then going over to', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=') report doing a DC course then going over to DataQuest to strengthen their skills, which is something I recommend (studying the same thing from different sources is always good practice).\\n\\nI will say DataCamp has the best selection of R courses and if you are looking to familiarize yourself with a new R package, you can’t go wrong with DataCamp; their lecturers are world-class. It also exposes you to what good code looks like. At $30/month I’d say a few months is a good way to augment your education.\\n\\nUdacity (Machine Learning Engineer):\\n\\nThis program covers the major ML algorithms and I thought the videos were helpful for things I’d already', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' videos were helpful for things I’d already studied; they expanded my understanding of things like SVMs, but I don’t think they’d teach it to you alone.\\n\\nThe projects here are also good and the mini-labs (not required) are nice skill-builders. I thought the staff grading my projects were great and very thorough. They really pushed me to write good explanations of every tool I used, and justify my decisions.\\n\\n$2000 is the current full list price for this program, and I think they’ve actually dismantled it and changed it fundamentally. The Machine Learning Engineer program I see on the site now seems smaller and focused on deployment and data engineering, whereas what I took discussed', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' deployment and data engineering, whereas what I took discussed all the major ML and Deep Learning algorithms (with no coverage of deployment).\\n\\nUltimately I’d say $2000 is way too much for what I got out of this, and the “Career Services” from Udacity are a joke (I got a good resume review and that was it). It will, however, look good on your resume and has probably helped me land interviews.\\n\\nSpringboard (Intermediate Data Science w/ Python):\\n\\nSpringboard does not create much content of their own, instead linking you to free or low-cost education resources around the web. For machine learning fundamentals I think this is a good idea; there’s plenty', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' is a good idea; there’s plenty of quality content out there so why reinvent the wheel at students’ expense? It’s oriented for students making data science projects in Jupyter for the first time, and does its job well (previous MOOCs I listed are all in-browser code-alongs without pulling up your own Jupyter notebook).\\n\\nThe really valuable asset of SpringBoard is their mentors. This was my first time seeing a lot of Machine Learning concepts and my mentor really grilled me to make sure I know my stuff. Brilliant guy.\\n\\nAt $500/month I’d consider this program a great way to build your chops. Get ready to be challenged,', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' build your chops. Get ready to be challenged, earn a nice certificate, and interact with an experienced mentor. Student support from the staff and community is quite cool and staff quickly responds to emails. I wouldn’t be anywhere in data science without them.\\n\\nSharpestMinds:\\n\\nSharpestMinds offered what I’d been looking for a long time, being a guy in a smallish city with a smaller tech scene: a chance to interact with real data scientists. You need to apply to get in, and to acquire a mentor you should be like 80% of the way there on education/skillset, but once you’re in you’re golden.\\n\\nThe other mentees', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content='�re golden.\\n\\nThe other mentees are bright and about as far along in data science as I, so the community Slack has interesting projects and posts. Often in the Slack, folks will say “My company’s hiring, shoot me a message if you’re interested in X topic.”\\n\\nAfter walking across the graduation stage with my MS in Pure Math, I felt like someone pushed me out a plane and said “Quick, get a job before you hit the ground.” SharpestMinds was like the 101st Airborne Division appearing out of the sky and handing me a parachute.\\n\\nThis is probably the best ‘career services’ of any MOOC or Boot', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content='er services’ of any MOOC or Bootcamp, ever, since they don’t get paid until you find a job. SM now averages one new hire every 3 days (an impressive feat considering how difficult landing an entry-level data science role is in 2019).\\n\\nPart of its success stems from its specificity and selectivity: this NOT a program for people wondering “Is data science for me?” It’s for people with significant knowledge under their belt trying to land the best entry-level DS role. You will need to work hard to with SM and build an ambitious project with your mentor, and once you’re hired you owe them a small portion of your first year’s salary (', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' portion of your first year’s salary (they’re upfront about this arrangement). They also put a lot of focus on your resume and online presence, and the mentors coach you with mock interviews.\\n\\nEssentially, this is not a MOOC or a bootcamp, but an education and career service for those trying to turn pro.\\n\\nCoursera:\\n\\nA great format for learning. It’s fundamentally different from code-along MOOCs but instead focuses on conceptual understanding. The quizzes here genuinely make you think and I never felt the videos were wasting my time.\\n\\nI used Coursera for an introductory R course and introductory SQL, which helped me snag a lot of interviews because employers', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' which helped me snag a lot of interviews because employers are always looking for someone who knows SQL. I probably don’t need to tell you about Coursera, but if you’re applying for jobs, make sure you have a few of their certificates on your resume and LinkedIn; the old guard don’t know all the new MOOCs but everyone knows (and respects) Coursera.\\n\\nBe the Real McCoy:\\n\\nDon’t forget that, to build coding skills, you’ll need to stop by Edabit or Hackerrank and see if you can level up. Start easy and don’t get ahead of yourself, make those fundamentals really strong (you’ll need', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content=' those fundamentals really strong (you’ll need them when you’re slicing strings backwards and building time series graphs). People become programmers by coding hours a day for months. You will inevitably need to find interesting datasets and test your knowledge yourself on them; no MOOC can recreate this. If you want to become a data scientist, don’t just knock out MOOCs to make your resume look good; actually build the skills employers are looking for.', metadata={'Title': 'The Path to Data Science: MOOC Reviews'}),\n",
       " Document(page_content='Photo by Sebastian Pichler on Unsplash\\n\\nWeb crawling, also known as web scraping, data scraping or spider, is a computer program technique used to scrape a huge amount of data from websites where regular-format data can be extracted and processed into easy-to-read structured formats.\\n\\nWeb crawling is commonly used:\\n\\nWeb crawling basically is how the internet functions. For example, SEO needs to create sitemaps and gives their permissions to let Google crawl their sites in order to make higher ranks in the search results. Many consultant companies would hire companies to specialize in web scraping to enrich their database so as to provide professional service to their clients.\\n\\nIt is really hard to determine the legality of web scraping in the', metadata={'Title': 'Is web crawling legal?'}),\n",
       " Document(page_content=' hard to determine the legality of web scraping in the era of the digitized era.\\n\\nWhy does web crawling have a negative connotation:\\n\\nWeb crawling can be used in the malicious purpose for example:\\n\\nScraping private or classified information. Disregard of the website’s terms and service, scrape without owners’ permission. An abusive manner of data requests would lead web server crashes under additionally heavy load.\\n\\nIt is important to note that a responsible data service provider would refuse your request if:\\n\\nThe data is private which would need a username and passcodes The TOS (Terms of Service) explicitly prohibits the action of web scraping The data is copyrighted\\n\\nWhat reasons can be used to', metadata={'Title': 'Is web crawling legal?'}),\n",
       " Document(page_content=' is copyrighted\\n\\nWhat reasons can be used to sue people?\\n\\nYour “just scraped a website” may cause unexpected consequences if you used it inappropriately.\\n\\nHiQ vs LinkedIn\\n\\nYou probably heard of the HiQ vs Linkedin case in 2017. HiQ is a data science company that provides scraped data to corporate HR departments. Linkedin then sent desist letter to stop HiQ scraping behavior. HiQ then filed…', metadata={'Title': 'Is web crawling legal?'}),\n",
       " Document(page_content='Neural Style Transfer\\n\\nNST was first published in the paper “A Neural Algorithm of Artistic Style” by Gatys et al, originally released to ArXiv 2015 [7].\\n\\nSeveral mobile apps use NST techniques, including DeepArt and Prisma.\\n\\nHere are some more examples of stylizations being used to transform the same image of the riverbank town that we used earlier.\\n\\nNeural style transfer combines content and style reconstruction. We need to do several things to get NST to work:\\n\\nchoose a layer (or set of layers) to represent content — the middle layers are recommended (not too shall, not too deep) for best results.\\n\\nMinimize the', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=') for best results.\\n\\nMinimize the total cost by using backpropagation.\\n\\nInitialize the input with random noise (necessary for generating gradients).\\n\\nReplacing max-pooling layers with average pooling to improve the gradient flow and to produce more appealing pictures.\\n\\nCode Implementation\\n\\nNow for the moment you’ve all been waiting for, the code to be able to make these images yourself. For clearer relationship between the code and the mathematical notation, please see the Jupyter notebook located in the GitHub repository.\\n\\nPart 1: Import Necessary Functions\\n\\nPart 2: Content Loss\\n\\nWe can generate an image that combines the content and style of a pair with a', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' combines the content and style of a pair with a loss function that incorporates this information. This is achieved with two terms, one that mimics the specific activations of a certain layer for the content image, and a second term that mimics the style. The variable to optimize in the loss function will be a generated image that aims to minimize the proposed cost. Note that to optimize this function, we will perform gradient descent on the pixel values, rather than on the neural network weights.\\n\\nWe will load a trained neural network called VGG-16 proposed in 1, who secured the first and second place in the localization and classification tracks of ImageNet Challenge in 2014, respectively. This network has been trained to discriminate over 1000 classes over more than', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' been trained to discriminate over 1000 classes over more than a million images. We will use the activation values obtained for an image of interest to represent the content and styles. In order to do so, we will feed-forward the image of interest and observe it’s activation values at the indicated layer.\\n\\nThe content loss function measures how much the feature map of the generated image differs from the feature map of the source image. We will only consider a single layer to represent the contents of an image.\\n\\nPart 3: Style Loss\\n\\nThe style measures the similarity among filters in a set of layers. In order to compute that similarity, we will compute the Gram matrix of the activation values for the style layers. The Gram matrix is', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' values for the style layers. The Gram matrix is related to the empirical covariance matrix, and therefore, reflects the statistics of the activation values.\\n\\nThe output is a 2-D matrix which approximately measures the cross-correlation among different filters for a given layer. This, in essence, constitutes the style of a layer.\\n\\nPart 4: Style Loss — Layer’s Loss\\n\\nIn practice we compute the style loss at a set of layers rather than just a single layer; then the total style loss is the sum of style losses at each layer:\\n\\nPart 5: Total-Variation Regularizer\\n\\nWe will also encourage smoothness in the image using a total-variation regularizer. This penalty term', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' total-variation regularizer. This penalty term will reduce variation among the neighboring pixel values.\\n\\nPart 6: Style Transfer\\n\\nWe now put it all together and generate some images! The style_transfer function below combines all the losses you coded up above and optimizes for an image that minimizes the total loss. Read the code and comments to understand the procedure.\\n\\nPart 6: Generate Pictures\\n\\nNow we are ready to make some images, run your own compositions and test out variations of hyperparameters and see what you can come up with, I will give you an example below. The list of hyperparameters to vary is as follows:\\n\\nThe base_img_path is the filename of content image', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content='_img_path is the filename of content image.\\n\\nis the filename of content image. The style_img_path is the filename of style image.\\n\\nis the filename of style image. The output_img_path is the filename of the generated image.\\n\\nis the filename of the generated image. The convnet is for the neural network weights, VGG-16 or VGG-19.\\n\\nis for the neural network weights, VGG-16 or VGG-19. The content_layer specifies which layer to use for content loss.\\n\\nspecifies which layer to use for content loss. The content_weight weights the content loss in the overall composite loss function. Increasing the value of this', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).\\n\\nweights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content). style_layers specifies a list of which layers to use for the style loss.\\n\\nspecifies a list of which layers to use for the style loss. style_weights specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.\\n\\nspecifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image. tv_weight specifies the', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' of the style image. tv_weight specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content.\\n\\nThe following code will generate the front image of this article if run for 50 iterations.\\n\\nHere are a couple of rough examples from my own implementation after 50 iterations:\\n\\nStyle of ‘Escher Sphere’ used to transform an image of the Goldengate Bridge.\\n\\nStyle of ‘Seated Nude’ used to transform an image of the riverbank town image.\\n\\nI recommend taking some of the images in the GitHub repository (or your own) and', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content=' in the GitHub repository (or your own) and playing around with the hyperparameters and seeing what images you can make. However, to warn you, the training times are quite high unless you have access to a GPU, possibly taking several hours for one image.', metadata={'Title': 'Neural Style Transfer and Visualization of Convolutional Networks'}),\n",
       " Document(page_content='The Easiest Python Numpy Tutorial Ever\\n\\nPie\\n\\nWant to be inspired? Come join my Super Quotes newsletter. 😎\\n\\nPython is by far one of the easiest programming languages to use. Writing programs is intuitive and so is reading the code itself — it’s almost like plain English!\\n\\nOne of Python’s greatest strengths is its endless supply of powerful libraries. Many of these libraries are written at least partially in C / C++ for speed and a Python wrapper on top for easy usage!\\n\\nNumpy is one such Python library.\\n\\nNumpy is mainly used for data manipulation and processing in the form of arrays. It’s high speed coupled with easy to use functions make it', metadata={'Title': 'The Easiest Python Numpy Tutorial Ever'}),\n",
       " Document(page_content=' high speed coupled with easy to use functions make it a favourite among Data Science and Machine Learning practitioners.\\n\\nThis article will be a code tutorial — the easiest one ever — for learning how to use Numpy!\\n\\nCreating arrays', metadata={'Title': 'The Easiest Python Numpy Tutorial Ever'}),\n",
       " Document(page_content='Introduction: Fast R-CNN (Object Detection)\\n\\nA beginners guide to one of the most fundamental concepts in object detection. shafu.eth · Follow 3 min read · Jul 18, 2019 -- 2 Listen Share\\n\\nThis is the second part of a three-part series, covering systems that combine region proposals with Convolutional Neural Networks (CNN). In the first part, we covered the R-CNN system. You will need to read that first to fully understand this article. You can find it here.\\n\\nIn this part, we will talk about the fast R-CNN system. This paper was published one year after the original R-CNN paper and directly builds on top of it. The R-CNN paper was', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content=' top of it. The R-CNN paper was a major breakthrough in 2014, combining region proposals with a CNN. But it had some problems:\\n\\nIt was slow : You had to calculate a feature map (one CNN forward pass) for each region proposal.\\n\\n: You had to calculate a feature map (one CNN forward pass) for each region proposal. Hard to train : Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately. This makes training very difficult.\\n\\n: Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content=' Box Regressor) that we had to train separately. This makes training very difficult. Large memory requirement: You had to save every feature map of each region proposal. This needs a lot of memory.\\n\\nFast R-CNN System\\n\\nSo, how did the author try to solve those problems. The major thing introduced in this paper is the following:\\n\\nWe only have one system, that we can train end-to-end.\\n\\nIf you only take one thing with you from this article it’s this: We combine the three different parts that we had in the R-CNN system (CNN, SVM, Bounding Box Regressor) into one architecture.\\n\\nArchitecture\\n\\nFast R-', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content='\\n\\nArchitecture\\n\\nFast R-CNN architecture. First image in the paper.\\n\\nThis architecture looks more complicated than it actually is. It works like this:\\n\\n1. Process the whole image with the CNN. The result is a feature map of the image.\\n\\nThe CNN input and output\\n\\n2. For each region proposal extract the corresponding part from the feature map. We will call this the region proposal feature map. We take the region proposal feature map from the feature map and resize it to a fixed size with the help of a pooling layer.\\n\\nThis pooling layer is called the Region of interest (RoI) pooling layer. If you want to find out more about it, here', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content=' you want to find out more about it, here you can find a greate explanation.\\n\\nRoI pooling layer\\n\\n3. Then we flatten this fixed sized region proposal feature map. This is now a feature vector, that always has the same size.\\n\\n4. This feature vector is now the input to the last part. These are fully connected layers that have 2 outputs. The first is the softmax classification layer, where we decide which object class we found. The second it the Bounding Box Regressor, where we output the bounding box coordinates for each object class.\\n\\nResults\\n\\nThe fast R-CNN trains the VGG16 network 9 times faster than R-CNN. But the amazing thing', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content=' faster than R-CNN. But the amazing thing about this system is this:\\n\\nThe inference is 213 times faster and achieves a higher mAP. Wow!\\n\\nConclustion\\n\\nIn the second part of this series, we talked about the fast R-CNN system and how it tried to improve the R-CNN system. Rember that only one year passed between the two papers and the system improved dramatically, especially in inference time. This illustrates how fast the whole deep learning space moves.\\n\\nAnd again, the most important thing summarized in one sentence:\\n\\nWe now have one end-to-end system that we can train with back-propagation.\\n\\nThank you for reading and keep up the', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content='\\n\\nThank you for reading and keep up the learning!\\n\\nIf you want more and stay up to date you can find me here:', metadata={'Title': 'Introduction: Fast R-CNN (Object Detection)'}),\n",
       " Document(page_content='Getting an internship in Machine Learning as an undergraduate student is tough. Really tough… Most of the well-known companies are looking for Ph.D. students with publications in prestigious journals. How to increase the chances of getting an internship in ML if you can’t satisfy the above?\\n\\nPhoto by Hunters Race on Unsplash\\n\\nLast year, I have spent hours sending applications to apply for Machine Learning/AI internships. As an Electronic Engineering student, I’ve found it particularly difficult to get even an interview, even though I had some relevant experience in Machine Learning. During my search for internships, I have noticed a few common things which companies are looking for. Based on these facts, I have decided to change', metadata={'Title': 'How to land an internship in Machine Learning'}),\n",
       " Document(page_content=' Based on these facts, I have decided to change my strategy for sending CVs, which finally resulted in me getting a job as a Machine Learning Research Intern. If you are planning to apply for Machine Learning internships, or you are struggling to get one, I hope that this article will help you land your dream job!\\n\\nI also wrote another article that helps you prepare for the interview. Link is here.\\n\\nBig companies = Big competition\\n\\nPhoto by Paweł Czerwiński on Unsplash\\n\\nThe first mistake I made, was to apply to big, well-known companies. Companies such as Google, Amazon or Apple are getting hundreds of applications per day and it is very difficult to get even', metadata={'Title': 'How to land an internship in Machine Learning'}),\n",
       " Document(page_content=' per day and it is very difficult to get even through the first recruitment stage for an internship. If you feel that you have goods skills in Machine Learning, with substantial experience in this field then go for it. Otherwise, it might be better to focus on targeting smaller, less known companies to maximize your chances of getting hired. For example, have a look at the required basic qualifications for Machine Learning Intern at Amazon:\\n\\nInternship requirements for Machine Learning Internship at Amazon, found on Linkedin\\n\\nWoah! You see, that is what I meant by saying tough requirements. Preferably, you should be a P.h.D. student with several publications in Machine Learning. If you are undergraduate, like me, you are', metadata={'Title': 'How to land an internship in Machine Learning'}),\n",
       " Document(page_content=' If you are undergraduate, like me, you are clearly at the disadvantage here.', metadata={'Title': 'How to land an internship in Machine Learning'}),\n",
       " Document(page_content='Introduction\\n\\nStarbucks is a global coffee company selling coffee, tea, espresso drinks, bakery, and grab-and-go offerings in 75 countries. One of the company’s values is “Creating a culture of warmth and belonging, where everyone is welcome.” Therefore, it utilizes many channels to market its products from social media to TV spots and ads. Starbucks executes its extraordinary marketing strategy by deploying a combination of marketing media channels, where it creates brand recognition. Starbucks does not only understand its products and customers, but also keeps up with how its customers use technology. Starbucks App enables customers to keep track of the available offers and happy hour deals at participating stores. It allows customers to earn and collect stars (collect two stars per', metadata={'Title': 'Starbucks: Analyze-a-Coffee'}),\n",
       " Document(page_content=' to earn and collect stars (collect two stars per $1) that can be redeemed in-store or via the app.\\n\\n“With nearly 100 million customers in our stores every week, we’re looking for more opportunities to engage directly and personally, providing them with special benefits and offers that are meaningful” — Matt Ryan, executive vice president and chief strategy officer for Starbucks.\\n\\nHere, we are going to investigate and analysis three files that simulate how people make purchasing decisions and how promotional offers influence those decisions. A sneak of the final data after being cleaned and analyzed: the data contains information about 8 offers sent to 14,825 customers who made 26,226 transactions while completing at least one offer. Below are two examples', metadata={'Title': 'Starbucks: Analyze-a-Coffee'}),\n",
       " Document(page_content=' completing at least one offer. Below are two examples of the types of offers Starbucks sends to its customers through the app to encourage them to purchase products and collect stars.', metadata={'Title': 'Starbucks: Analyze-a-Coffee'}),\n",
       " Document(page_content='Fun fact: Scikit-Learn doesn’t have any distance metrics which can handle both categorical and continuous data! How can we then use clustering algorithms, e.g. k-NN, if we have a dataset with mixed-type variables?\\n\\nPhoto by Fredy Jacob on Unsplash\\n\\nUpdate (27/07/19) — The package has been released at PyPI as Distython. I have published an article to explain how it works.\\n\\nThe big problem that I have faced during my summer internship in the IT Innovation Centre was the lack of existing implementations of distance metrics which could handle both mixed-type data and missing values. It has started my long search for algorithms which can satisfy those requirements', metadata={'Title': 'The proper way of handling mixed-type data. State-of-the-art distance metrics.'}),\n",
       " Document(page_content=' my long search for algorithms which can satisfy those requirements. Several research papers later, I have discovered quite interesting distance metrics which can help to improve the accuracy of your machine learning model when dealing with mixed-type data, missing values, or both. I have implemented them in my spare time and published their code implementation on the Github so you can use it easily with Scikit-Learn. But how? I will explain that in this tutorial!\\n\\nWhat I love about Towards Data Science, is that it attracts many like-minded people who are passionate about AI and Data Science. That’s why I would like to connect with you on Linkedin! You can also leave any feedback and questions via my personal website.\\n\\nOverview of H', metadata={'Title': 'The proper way of handling mixed-type data. State-of-the-art distance metrics.'}),\n",
       " Document(page_content=' via my personal website.\\n\\nOverview of Heterogenous Distance Metrics\\n\\nPhoto by Annie Spratt on Unsplash\\n\\nBefore we start, I would like to recommend to look at this paper if you want to get a more in-depth understanding of the algorithms I will talk about. My main goal here is to provide you with an intuitive understanding of those algorithms so you can use my article as a quick reference sheet. You can find the practical part with a code at the end of the article. Let’s get started!\\n\\nDistance Metrics\\n\\nBut wait… What are actually the distance metrics? The distance metrics measure the distance between two instances in the dataset. They measure the similarity between instances based on their', metadata={'Title': 'The proper way of handling mixed-type data. State-of-the-art distance metrics.'}),\n",
       " Document(page_content='. They measure the similarity between instances based on their features. For example, imagine patients of a…', metadata={'Title': 'The proper way of handling mixed-type data. State-of-the-art distance metrics.'}),\n",
       " Document(page_content='Understanding the types of data in a business/organization\\n\\nTry to google it, and you will guarantee to find various sources each with their own versions (some said 3 types of data, some said 5 types, some even said 13 types). We make it easy for you by summarizing them and take your comprehending into the next level Rendy Dalimunthe · Follow Published in Towards Data Science · 4 min read · Jul 18, 2019 -- Listen Share\\n\\nBefore you start to rolling out your data management initiatives, be it Master Data Management, Enterprise Data Warehouse, Big Data Analytics or whatever it is, you need to start by understanding the very basic ingredient: the data. Only by thoroughly recognizing their characteristics, you will know the', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content=' by thoroughly recognizing their characteristics, you will know the right way on how to treat each of them.\\n\\n“Data is a precious thing and will last longer than the systems themselves” Tim Berners-Lee\\n\\nSo let’s get started!\\n\\nTransactional Data\\n\\nThis type of data describes your core business activities. If you are a trading company, this may includes the data of your purchasing and selling activities. If you are a manufacturing company, this will be your production activities data. If you are a ride-hailing or cab company, this will the trip data. In a very basic organizational operations, the data related to the activities of hiring and firing employees can also be classified as transactional data', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content=' firing employees can also be classified as transactional data. As a result, this kind of data has a very huge volume in comparison with the other types and usually created, stored, and maintained within the operational application such as ERP system.\\n\\nMaster Data\\n\\nIt consists of key information that make up the transactional data. For example, the trip data in a cab company may contain driver, passenger, route, and fare data. The driver, passenger, locations, and basic fare data are the master data. The driver data may consists the name of the driver and all of the associated information. So does the passenger data. Together, they make up the transactional data.\\n\\nMaster data usually contains places (addresses, postal', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content='Master data usually contains places (addresses, postal-coded, cities, countries), parties (customers, suppliers, employees) and things (products, assets, items, etc.). It is application-specific, meaning that its uses are specific for the application with business process related to it, e.g: the employees master data is created, stored, and maintained within the HR application.\\n\\nBy now, you should get some grasp of understandings that master data is relatively constant. While the transaction data is created at a lightning speed, the master data is somehow constant. The trip data is created in any second but the list of the driver will remain the same unless there’s a new driver on-board or get kicked out', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content=' a new driver on-board or get kicked out.\\n\\nNowadays, processes within the organization are usually so inter-dependable, which means that one process conducted in one system is related to the process conducted in other system. They may use the same master data. If each system manage their own master data, potential duplication and inconsistencies may arise. For instance, a customer may be stored as Rendy in system A, but listed as Randy in system B, although Rendy and Randy is actually the same entity. But no need to worries, there’s a discipline to manage this kind of situation. It’s called Master Data Management.\\n\\nReference Data\\n\\nReference data is a subset of master data. It is', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content=' data is a subset of master data. It is usually a standardized data that governed by certain codification (e.g. list of Countries is governed by ISO 3166–1. There’s an easy way to differentiate reference data from master data. Always remember that reference data is way less volatile than master data. Let’s back again to our cab company. Tomorrow, the day after tomorrow, or next week, the list of driver may change whenever there’s a new person onboard or kicked out. But I can guarantee you that the list of countries will remain the same even 2 decades from now, unless there’s a little land that declare its independence.\\n\\nReporting Data\\n\\nIt’s an', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content='\\nReporting Data\\n\\nIt’s an aggregated data compile for the purpose of analytic and reporting. This data consist of transactional, master, and reference data. For example: Trip data (transaction + master) on the 13th day of July in Greater London region (reference). Reporting data is very strategic and usually being produced as ingredient of decision making process.\\n\\nMetadata\\n\\nIt’s a data about data. Sounds confusing? Indeed. It’s the type of data that got me dizzy in the first time I enter the data management field. Thankfully, this beautiful picture make it easy for me to comprehend what metadata actually is.\\n\\nData & its metadata\\n\\nIf I ask you a', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content=' & its metadata\\n\\nIf I ask you a question: what is the color of the cat? Immediately by just looking at the data you can confidently answer my question. It’s grey. But what if I come up with another questions: when and where this picture be taken? Chances are high that you will not be able to give me the right answer by only looking at the data. And here is where the metadata come to rescue. It gives you the complete information about the data including when and where it was taken.\\n\\nSo metadata is giving you the answer to any question that you cannot answer by just looking at the data. That’s why it said: data about data.', metadata={'Title': 'Understanding the types of data in a business/organization'}),\n",
       " Document(page_content='Data Scientist’s toolkit — How to gather data from different sources\\n\\nPhoto by Jakob Owens on Unsplash\\n\\nNot so long ago!\\n\\nDo you remember the time when data was sent to you in an external hard drive for your analysis or model building.\\n\\nNow — as a data scientist, you are not limited to those means. There are several ways of storing data, sharing data as well as different sources to acquire data, augment data.\\n\\nBelow, I’m listing down several ways of gathering data for your analysis\\n\\nTable of contents:\\n\\nCSV file Flat File (tab, space, or any other separator) Text File (In a single file — reading data all', metadata={'Title': 'Data Scientist’s toolkit — How to gather data from different sources'}),\n",
       " Document(page_content=' File (In a single file — reading data all at once) ZIP file Multiple Text Files (Data is split over multiple text files) Download File from Internet (File hosted on a server) Webpage (scraping) APIs (JSON) Text File (Reading data line by line) RDBMS (SQL Tables)\\n\\nIn Python, a file is characterized as either text or binary, and the difference between the two is important\\n\\nText files are structured as sequence of lines. Each line is terminated with a special character called EOL or End of line character. There are several types but most common are\\n\\nor ,\\n\\nA Binary file type is basically any type of file that’s not a text file. Because', metadata={'Title': 'Data Scientist’s toolkit — How to gather data from different sources'}),\n",
       " Document(page_content=\" that’s not a text file. Because of their nature, binary file can only be processed by an application that know or understand the file’s structure\\n\\n1. CSV File\\n\\nMost common format for storing and sharing dataset is comma separated format or a csv file. pandas.read_csv() is THE most useful and powerful method and I strongly recommend you to read its documentation . By using appropriate kind of sep you can load several types of data in dataframe\\n\\nimport pandas df = pd.read_csv('data.csv', sep =',')\\n\\n2. Flat File\\n\\nbut at times you might receive file that’s tab separated or a fixed width format or…\", metadata={'Title': 'Data Scientist’s toolkit — How to gather data from different sources'}),\n",
       " Document(page_content='Introduction\\n\\nWhy?\\n\\nThere are many articles and courses dedicated to the latest ML/AI research aimed at training bigger models and achieving higher classification accuracy. This is great for research and academia and pushing the limits of what AI can do. However, these are not really tailored for poor student practitioners starting off with their first major AI projects or penny conscious entrepreneurs looking to build an MVP of their cool revolutionary idea.\\n\\nWhat?\\n\\nIn this work I take a budgeted approach to model training and try to answer the question:\\n\\nWhat is the minimum, practical cost to complete a real world AI project?\\n\\nThe problem that I chose for this was an Image Classification problem.This article captures the process I followed and key budget', metadata={'Title': 'Deep Learning on a Budget'}),\n",
       " Document(page_content='This article captures the process I followed and key budgeting lessons learned from each step.\\n\\nSummary\\n\\nThe answer is roughly $300 → This is the amount it takes to train a well performing Computer Vision model using cloud computing. Incidentally (or not) this is also the amount of credit that Google gives as incentive to get started on the Google Cloud Platform (GCP) [1].\\n\\nThe breakdown of the budget is given below. The two rightmost columns list the instances from AWS and GCP that are most amenable to this task. The cost is an average of the instances listed in these columns. At present there is not a lot separating the two cloud providers in terms of cost.', metadata={'Title': 'Deep Learning on a Budget'}),\n",
       " Document(page_content='Generating Startup names with Markov Chains\\n\\nThe most interesting applications of Machine Learning are, without a doubt, the generative models.\\n\\nThe idea of finding patterns in data and generating new content that at the same time is similar to your data, but unique in its own way, has always fascinated me.\\n\\nSo I have decided to develop a simple text generator to create Startup names using Markov Chains.\\n\\nBut first of all, a short introduction to Markov Chains 🔗\\n\\nMarkov Chains\\n\\nA Markov chain is a model of some random process that happens over time.\\n\\nMarkov chains are called this way because they follow a rule called the Markov property. The Markov property says', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=' the Markov property. The Markov property says that whatever happens next in a process only depends on how it is right now (the state).\\n\\nFor instance, consider the example of predicting the weather for the next day, using only the information about the current weather. By analysing some real data, we may find these conditions:\\n\\nGiven that today is sunny, tomorrow will also be sunny 90% of the time\\n\\nGiven that today is sunny, tomorrow will be rainy 10% of the time\\n\\nGiven that today is rainy, tomorrow will also be rainy 50% of the time\\n\\nGiven that today is rainy, tomorrow will be sunny 50% of the time\\n\\nThis is modelled via the Markov Chain below', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"This is modelled via the Markov Chain below, where each circle is a state and the numbers in the arrows represent the probabilities of changing from the current state.\\n\\nSo if we want to make a prediction for tomorrow, we just need to verify the current state we are in (sunny or rainy) and use the transition probabilities to calculate which next state is more likely.\\n\\nAn in-depth explanation of Markov Chains, with some cool animations, can be found here: http://setosa.io/ev/markov-chains/\\n\\nApplying Markov Chains to text generation\\n\\nSo, how can we apply this idea to generate text? Well, it's quite simple actually.\\n\\nNames or sentences\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"'s quite simple actually.\\n\\nNames or sentences are basically a sequence of characters and those sequences follow some patterns.\\n\\nFor instance, if I asked you to give words that started with whe__, you would quickly come up with the words when, where, whenever, etc. While wheeziness is a perfectly valid word, it's less frequent considering the initial whe_. In other words, given the state whe, we will most likely change to the states when, where or whenever than to the state wheeziness.\\n\\nSo how can we build a model that capture these probabilities given our data?\\n\\nFor this, I'll show you how to build a simple Markov Chain using the words startup, statistic and artist. First\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\" using the words startup, statistic and artist. First of all, we will list all the states transitions for each tuple of 3 characters:\\n\\nStartup\\n\\nsta -> tar\\n\\ntar -> art\\n\\nart -> rtu\\n\\nrtu -> tup Statistic\\n\\nsta -> tat\\n\\ntat -> ati\\n\\nati -> tis\\n\\ntis -> ist\\n\\nist -> sti\\n\\nsti -> tic Artist\\n\\nart -> rti\\n\\nrti -> tis\\n\\nist -> ist\\n\\nNow, if you pay close attention to the states, you will notice that some of them are shared among different tuples. To better visualize that, let's create a\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content='. To better visualize that, let\\'s create a dictionary where each entry is a state, and the values are the next states and its weights.\\n\\n{\\n\\n\"sta\": {\\n\\n\"tar\": 1,\\n\\n\"tat\": 1\\n\\n},\\n\\n\"tar\": {\\n\\n\"art\": 1\\n\\n},\\n\\n\"art\": {\\n\\n\"rtu\": 1,\\n\\n\"rti\": 1\\n\\n},\\n\\n\"rtu\": {\\n\\n\"tup\": 1\\n\\n},\\n\\n\"tat\": {\\n\\n\"ati\": 1\\n\\n},\\n\\n\"ati\": {\\n\\n\"tis\": 1\\n\\n},\\n\\n\"tis\": {\\n\\n', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content='\\n},\\n\\n\"tis\": {\\n\\n\"ist\": 2\\n\\n},\\n\\n\"ist\": {\\n\\n\"sti\": 1\\n\\n},\\n\\n\"sti\": {\\n\\n\"tic\": 1\\n\\n},\\n\\n\"rti\": {\\n\\n\"tis\": 1\\n\\n}\\n\\n}\\n\\nTa-da! That\\'s our Markov Chain. Simple, isn\\'t it? (PS: Technically, Markov Chains are defined with a transition matrix with probabilities, not weights. We could easily transform into a transition matrix, but for the problem we have in mind, this is a better way of visualizing it).\\n\\nNow, how can we generate new data with this?\\n', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\" how can we generate new data with this?\\n\\nThere are basically four steps, let's go through each one of them:\\n\\nStep 1: Start with some initial random state\\n\\nYou could select any of the states as a starting position, however, you will most likely generate text that doesn't make any sense. For instance, rtu is a valid initial state, but you won't find a word in real life that starts with those letters (none that I can't think of it, at least)\\n\\nA better approach is to keep track of the starting states in another dictionary, and selecting the first state from there. In our case, the possible initial states are sta(as both startup and statistic start with st\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content='a(as both startup and statistic start with sta) and art. For our example, let\\'s select sta.\\n\\nStep 2: Select randomly one of its transition states, considering its weights\\n\\nFor the tuple sta, you can go to tar or tat, both of them with the same probability (same weight). In a real case scenario, they would have different weights considering the distribution found in your dataset, but as we have just used three words, they have equal weights. Let\\'s \"randomly\" select the tuple tar.\\n\\nStep 3: Append the new state to your generated text\\n\\nSo far, we have started with the state sta and transitioned to the state tar. So our current', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\" and transitioned to the state tar. So our current generated word is star.\\n\\nStep 4: Repeat step one using the new state, until a stop character is found, or until you are happy with your result\\n\\nNow, for our current state tar, the only possible state is art, so our generated word become start.\\n\\nNow let's continue the algorithm in a faster way. From art, we can go to rti or rtu. Let's select rti. If you continue to apply the algorithm, you will quickly generate our new word: Startist, which is a mix of startup and artist.\\n\\nEven though the example is quite simple, it demonstrates the potential of Markov Chains.\\n\\nNow that\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=' potential of Markov Chains.\\n\\nNow that we have \"implemented\" by hand a Markov Chain, let\\'s do in Python using real data where you can get actual useful results.\\n\\nLet\\'s code!\\n\\nLet\\'s start by importing some modules. We will only need two: pandas, to read CSV data, and random to (unsurprisingly) generate random numbers.\\n\\nimport pandas as pd\\n\\nimport random\\n\\nAs a dataset for our Startup name generator, we are going to use a 2015 dump from CrunchBase with around 18k companies.\\n\\nThe dataset is not that big, but you will see that Markov Chains work pretty well even with a database much smaller than this.', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\" well even with a database much smaller than this.\\n\\nReading our companies data is pretty straightforward: pandas read_csv function accepts a URL as a parameter and returns a data frame. We have also removed symbols and transformed the names to lower case.\\n\\nAs we have discussed previously, the simplest way to model the data structure for a Markov Chain is a dictionary containing the states and transitions weights.\\n\\nchain = build_markov_chain(companies['name'].tolist(), 3)\\n\\nprint(chain['sta'])\\n\\nIf you run the above code, you will get this result:\\n\\n{\\n\\n'tar':290,\\n\\n'tat':151,\\n\\n'ta\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"\\n'tat':151,\\n\\n'ta.':52,\\n\\n'ta ':35,\\n\\n'tac':55,\\n\\n'tag':43,\\n\\n'tal':46,\\n\\n'tay':34,\\n\\n'tau':22,\\n\\n'tad':14,\\n\\n'tam':19,\\n\\n'tas':19,\\n\\n'taq':5,\\n\\n'tan':92,\\n\\n'tab':23,\\n\\n'tap':6,\\n\\n'tak':8,\\n\\n'tai':22,\\n\\n'taf':16,\\n\\n'tax':5,\\n\\n'ta™':1,\\n\\n'tah':2,\\n\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"1,\\n\\n'tah':2,\\n\\n'tav':5,\\n\\n'tae':1,\\n\\n'taj':1,\\n\\n'taw':1,\\n\\n'taa':2,\\n\\n'taz':1\\n\\n}\\n\\nWhat does this mean? These are the list of next states and weights on the Markov Chain, considering that the current state is the tuple sta. The higher the next state weight, more likely its transition to it.\\n\\nFor instance, if you take the first state tar, it has the largest weight on this state list. Intuitively, it makes sense, as it is probably capturing the occurrence of the word startup.\\n\\nNow we need to build\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\" word startup.\\n\\nNow we need to build a function that returns a random tuple from the chain considering its weights.\\n\\nFinally, here is where the magic happens: let's generate some new words.\\n\\nLet's go step by step in our generate function.\\n\\ntuple = select_random_item(chain['_initial'])\\n\\nresult = [tuple]\\n\\nRemember that we mentioned that is better to keep track of the initial tuples and selecting one of those as the initial state? That's exactly what we are doing here.\\n\\nwhile True:\\n\\ntuple = select_random_item(chain[tuple])\\n\\nlast_character = tuple[-1]\\n\\n\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"_character = tuple[-1]\\n\\nif last_character == '.':\\n\\nbreak\\n\\nresult.append(last_character)\\n\\nThis is where we are navigating through our Markov Chain, considering its probabilities. We are selecting a random weighted next state and appending the last character of this state to our result string. However, if the last character is a period, we stop our generation, as this is the ending of our chain.\\n\\nWe could add additional rules such as generating words given a minimum or maximum length, but let's keep it simple for now.\\n\\ngenerated = ''.join(result)\\n\\nif generated not in chain['_names']:\\n\\nreturn generated\\n\\n\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"_names']:\\n\\nreturn generated\\n\\nelse:\\n\\nreturn generate(chain)\\n\\nFinally, we join all of the generated characters together, and we do the last verification. As nothing prevents the Markov Chain from generating an already existing name, and as we are interested in creating new names, we will simply generate a new one if the generated name is already in our database.\\n\\nResults\\n\\nHere is a couple of examples from our Startup Name Generator™®.\\n\\nDomos\\n\\nHup Online\\n\\nVubanky\\n\\nAcara\\n\\nIgnaly\\n\\niFly\\n\\nPretty cool, huh? 😎\\n\\nMore ideas\\n\\nOne final idea, what if we generated Startup names\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content=\"One final idea, what if we generated Startup names specific for each industry? It would be awesome, don't you think?\\n\\nLet's do it, it will be extremely easy 🙃\\n\\nThe only thing we have to do is to build our Markov Chain considering only examples from the industry we are interested in.\\n\\nAnd here are our results:\\n\\nTravel Startups\\n\\nprint(generate_amount_by_category('Travel',5))\\n\\nPango\\n\\nMovology\\n\\nNextrive\\n\\nTriptel\\n\\nStingi\\n\\nTechnology Startups\\n\\nprint(generate_amount_by_category('Technology',5))\\n\\nNaco Innovation\\n\\nK\", metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content='5))\\n\\nNaco Innovation\\n\\nKicksense\\n\\nNetWatch\\n\\nChony\\n\\nDatars\\n\\nTry yourself\\n\\nYou can try yourself using this Google Colab link or downloading the source code directly from my GitHub.\\n\\nWhat are your thoughts? Any suggestion for new content? Feedbacks? Let me know in the comments.\\n\\nHope you have enjoyed it :)', metadata={'Title': 'Generating Startup names with Markov Chains'}),\n",
       " Document(page_content='A Recipe for using Open Source Machine Learning models\\n\\nPhoto by Luca Bravo on Unsplash\\n\\nMachine learning continues to produce state of the art (SOTA) results for an increasing variety of tasks and more companies are looking to ML to solve their problems. With the incredibly rapid pace of machine learning research, many of these SOTA models come from academic and research institutions which open source these models. Often, using one of these open source models to bootstrap your machine learning efforts within your company can be much more effective than building a model from scratch.\\n\\nHowever, these models are often released by researchers whose focus isn’t necessarily to enable easy use and modification of their models (though there are many exceptions). Using these open', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' (though there are many exceptions). Using these open source models for your tasks can be quite difficult.\\n\\nIn this post, my goal is to provide a recipe you can follow to evaluate and use open source ML models to solve your own tasks. These are the steps I’ve used over and over again in my own work (as of this writing I have anaconda environments set up over 15 open source models). As my work is mostly using deep learning for vision and NLP, my focus here is specifically on using neural network-based models.\\n\\nWhether you’re trying to use machine learning to solve real problems within your company or experiment with some fun SOTA results at home, my hope is that after this post', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' at home, my hope is that after this post, you’ll have a path to take an open source model and modify and use it to address your own task with your own dataset.\\n\\nStep 1: Naming your task\\n\\nThe first step is figuring out what your particular task is called in the research literature so you can successfully search for it. This can initially be quite frustrating. For example, finding all the instances of a dog in a picture would be an “object detection” task. But if you want to know exactly which pixels in the picture correspond to dogs that’s called “image segmentation.”\\n\\nThere are a few ways you can try to figure this out. First,', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' you can try to figure this out. First, if you happen to know any ML researchers or practitioners, definitely start there. Another option is to ask in r/machinelearning or r/learnmachinelearning. If none of these pan out, the next step is to google to the best of your ability. As you land on research papers you’ll often see the name commonly associated with the task in the literature.\\n\\nStep 2: Finding papers and code\\n\\nOnce you know what to search for, the next step is to find those open source models that best suit your task. There are a few resources that are helpful here:\\n\\npaperswithcode: A repository of papers and associated code, organized by task. This is', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' and associated code, organized by task. This is a really good starting point, especially if it’s a well-known task.\\n\\narxiv-sanity: Many open source models are associated with research papers. Most papers in machine learning are (fortunately!) openly published on arxiv. Searching arxiv for recent papers that solve for your task is another good place to start. Not all published papers here have code associated with them. If you find a paper you like, try searching for “<paper name> github” to see if the code has been released.\\n\\nKaggle: If there happens to be a Kaggle competition with a task similar to yours, this can be a', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' a task similar to yours, this can be a great way to get high quality, state of the art models. Pay particularly close attention to winner blogs for past competitions, these often have great explanations and code. The little tricks that were used to win the competition can often be really valuable for your task as well.\\n\\nDataset benchmarks: If there’s a benchmark dataset that’s similar to the task you’re working on, the leaderboard for that benchmark is a quick way to find papers with demonstrably SOTA results.\\n\\nGoogle: For standard/common tasks like image segmentation, searching for “image segmentation github”, “image segmentation pytorch” or �', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content='image segmentation pytorch” or “image segmentation tensorflow” will give you a lot of results.\\n\\nStep 3: Read the papers\\n\\nThis can be intimidating because academic papers can be pretty inaccessible, even to experienced software engineers. But if you focus on the abstract, introduction, related work, results, and delay a lot of the deep details/math for later readings, you’ll find you can get a lot out of the paper and a deeper understanding of the problem.\\n\\nPay particularly close attention to the dataset(s) they use and the constraints of those datasets or their model. Often you’ll find that the constraints may not be applicable to you and are fundamental to the', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' not be applicable to you and are fundamental to the model design. For example, classification models for imagenet expect there to be one and only one salient object in an image. If your images have zero, one or more objects to identify, those models are probably not directly applicable. This isn’t something you want to find out after investing the time needed to bring up the model.\\n\\nAlso, follow some of the references, especially those you see in multiple papers! You’ll regularly find that at least one of the references provides a very clear description of the problem and dramatically increases your understanding. Referenced papers may also end up being more useful and may have nicer code associated with them, so it’s worth', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' associated with them, so it’s worth doing a little digging here.\\n\\nStep 4: Make sure the code is usable\\n\\nOnce you’ve found a paper with open source code, make sure it’s usable. Specifically:\\n\\nCheck the license: While a lot of code is released under liberal open source licenses (MIT, BSD, Apache etc), some of it isn’t. You may find the model has a non-commercial use only license, or no license at all. Depending on your use case and company, the code may or may not be usable for you.\\n\\nCheck the framework: If you’re working with a particular framework (eg. Tensorflow, Py', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' particular framework (eg. Tensorflow, Pytorch) check the framework the model is built in. Most of the time you’re stuck with what you get, but sometimes there’s a reimplementation of the model in your preferred framework. A quick Google to check for this (eg. “<paper name> pytorch”) can save you a lot of trouble.\\n\\nCheck the language: Similarly, if the model is in Lua and you’re not a Lua developer, this can be really painful. See if there’s a reimplementation in the language of your choice (often Python, since in deep learning Python should be part of your repertoire), and if not you might be', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' of your repertoire), and if not you might be better off finding another model.\\n\\nCheck the coding style: Researchers aren’t all software engineers so you can’t have as high a bar as for other open source projects, but if the code is a total mess you may want to look for another model.\\n\\nStep 5: Get the model running\\n\\nResults from NVIDIA’s StyleGAN trained on a custom furniture dataset\\n\\nOnce you’ve found a model you think is a good fit, try to get the model running. The goal here is to run the training and inference loop for the model as-is, not to get it running on your specific dataset or to make any significant modifications. All', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' specific dataset or to make any significant modifications. All you want to do is make sure that you have the right dependencies and that the model trains and runs as advertised. To that end:\\n\\nCreate a conda environment for the model: You may be trying out multiple models, so create a conda environment (assuming Python) for each model (nvidia-docker is another option here, but personally I find it to be overkill).\\n\\nI’ll often set up my environments like so: conda create -n <name of the github repo> python=<same version of python used by the repo>\\n\\nA quick way to figure out which version of python the repo is using is to look at the print statements', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' repo is using is to look at the print statements. If there are no parens, it’s python 2.7, otherwise 3.6 should work.\\n\\nInstall the libraries: I highly recommend starting off by installing the exact same version of the framework that the original code used to start. If the model says it works with pytorch>0.4.0 , don’t assume it’ll work with pytorch 1.0. At this stage, you don’t want to be fixing those kinds of bugs, so start with pytorch=0.4.0 . You can install a particular version of a framework (eg. pytorch) with the command conda', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content='. pytorch) with the command conda install pytorch=0.4.0 -c pytorch . A lot of code won’t have a requirements.txt file, so it may take some sleuthing and iterating to figure out all the libraries you need to install.\\n\\nGet the original dataset and run the scripts: At this point, you should be able to download the original dataset and run the testing and training script. You’ll probably have to fix some paths here and there and use the README and source to figure out the correct parameters. If a pre-trained model is available, start with the testing script and see if you’re getting similar results to the paper.', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content='’re getting similar results to the paper.\\n\\nOnce you have the testing script running, try to get the training script up. You’ll probably have to work through various exceptions and make slight modifications to get it to work. Ultimately your goal with the training script is to see the loss decreasing with each epoch.\\n\\nIf it’s straightforward (ie. only requires changing some command-line flags), at this point you might try running training script on your own dataset. Otherwise, we’ll do this in step 7.\\n\\nStep 6: Create your own testing notebook\\n\\nAt this point, you’ve confirmed that the model works and you have the right environment set up to be able to use', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' the right environment set up to be able to use it. Now you can dig in and start really playing with it. At this point, I recommend creating a Jupyter notebook, copy-pasting in the testing script, and then modifying till you can use it with a single item of data. For example, if you’re using an object detection model that finds dogs in an image, you want a notebook where you can pass it a picture and have it output the bounding boxes of the dogs.\\n\\nThe goal here is to get a feel for the inputs and outputs, how they must be formatted and how exactly the model works, without having to deal with the additional complexity of training or munging your own data', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' complexity of training or munging your own data into the right formats. I recommend doing this in a Jupyter notebook because I find that being able to see the outputs along each step is really helpful in figuring it out.\\n\\nStep 7: Create your own training notebook with your dataset\\n\\nNow that you have some familiarity with the model and data, it’s time to try to create a training notebook. Similar to step 6, I start by copying and pasting in the training script, separating it into multiple cells, and then modifying it to fit my needs.\\n\\nIf you’re already feeling comfortable with the model, you may want to go directly to modifying the training notebook so it works with your dataset', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' modifying the training notebook so it works with your dataset. This may involve writing dataloaders that output the same format as the existing dataloaders in the model (or simply modifying those dataloaders). If you’re not yet comfortable enough to do that, start by just getting the training script to work as-is in the notebook and removing code that you don’t think is useful. Then work on getting it to work with your dataset.\\n\\nKeep in mind the goal here isn’t to modify the model, even if it’s not quite solving the exact task you want yet. It’s just to get the model working with your dataset.\\n\\nStep 8: Start modifying', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' your dataset.\\n\\nStep 8: Start modifying the model to suit your task!\\n\\nBy this point, you should have a notebook that can train the model (including outputting appropriate metrics/visualizations) and a notebook where you can test new models you create. Now is a good time to start to dig in and make modifications to the model (adding features, additional outputs, variations etc) that make it work for your task and/or dataset. Hopefully having the starting point of an existing state of the art model saved you a lot of time and provides better results than what you might get starting from scratch.\\n\\nThere’s obviously a lot happening in this step and you’d use all your existing model building strategies', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content='’d use all your existing model building strategies. However, below are some pointers that may be helpful specifically when building off of an existing model.\\n\\nModify the dataset before modifying the model: It’s often easier to munge your data into the format the model expects rather than modifying the model. It’s easier to isolate problems and you’re likely to introduce fewer bugs. It’s surprising how far you can sometimes push a model just by changing the data.\\n\\nReuse the pre-trained model as much as possible: If your model changes aren’t drastic, try to reuse the pre-trained model parameters. You may get faster results and the benefits of transfer learning. Even if', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' results and the benefits of transfer learning. Even if you expand the model, you can often load the pre-trained parameters into the rest of the model (eg. use strict=False when loading the model in pytorch).\\n\\nMake incremental changes and regularly check the performance: One benefit of using an existing model is you have an idea of the performance you started with. By making incremental changes and checking the performance after each one, you’ll immediately identify when you’ve made a mistake or are going down a bad path.\\n\\nAsk for help: If you’re totally stuck, try reaching out to the author and asking for some pointers. I’ve found they’re often willing to help,', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' found they’re often willing to help, but remember they’re doing you a favor and please act accordingly.\\n\\nAutomatically texturing a 3D model using neural renderer\\n\\nStep 9: Attribute and Contribute\\n\\nDepending on the license and how you’re distributing your model, you may be required to provide attribution to the developer of the original code. Even if it’s not required, it’s nice to do it anyway.\\n\\nAnd please contribute back if you can! If you come across bugs and fix them during your own development, submit a pull request. I’m sure well-written bug reports are welcome. Finally, if nothing else, sending a quick thank', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content=' Finally, if nothing else, sending a quick thank you to the author for their hard work is always appreciated.', metadata={'Title': 'A Recipe for using Open Source Machine Learning models'}),\n",
       " Document(page_content='How to Choose Between Multiple Models\\n\\nIn a previous article we discussed the concepts of underfitting and overfitting, how they can lead to models that don’t match the available data, how to identify each issue, and how to identify models that do fit the data well. These concepts can help you avoid major blunders and generate models that fit the data reasonably accurately; however, there are an incredible number of models that meet that description. This means that the next step, beyond generating a model that fits decently, is identifying which of the possible models fits best.\\n\\nWhen determining how well a model fits the data set it’s important to calculate statistical values comparing the model predictions to the data set. This is beyond the', metadata={'Title': 'How to Choose Between Multiple Models'}),\n",
       " Document(page_content=' predictions to the data set. This is beyond the scope of this conceptual article, but more information can be found in Data Science from Scratch or in Practical Statistics for Data Scientists. In this article we’ll discuss the process of developing, validating, and testing models.\\n\\nWhat are the model development, validation, and testing phases and why are they necessary?\\n\\nThe fundamental issue to be aware of here is that you cannot trust a model that you’ve developed simply because it fits the training data well. This is for a simple reason: You forced the model to fit the training data well. If after creating a model the statistical calculations show that it matches the data well, this means that it’s possible', metadata={'Title': 'How to Choose Between Multiple Models'}),\n",
       " Document(page_content=' well, this means that it’s possible to use mathematical methods to force a model to match the data well. What it doesn’t mean is that the model is capturing the trends that are really occurring, or that the model is able to predict other circumstances. The example of the overfit model in my previous article is a great way to highlight this.\\n\\nThe solution to this is model validation. Validation is the practice of using the model to predict the output in other situations for which you have data, and calculating those same statistical measures of fit on those results. Note that this means you need to divide your dataset into two different data files. The first is a training data set, which you use to generate your models.', metadata={'Title': 'How to Choose Between Multiple Models'}),\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
